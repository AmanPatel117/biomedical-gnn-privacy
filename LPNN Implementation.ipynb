{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " #conda install -c conda-forge pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install 'pip<24.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pyodbc==4.0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.datasets\n",
    "\n",
    "# Download and load the MUTAG dataset\n",
    "mutag = torch_geometric.datasets.TUDataset(root='./data/TUDataset', name='MUTAG')\n",
    "#ogb_molhiv = torch_geometric.datasets.OGBDataset(root='./data/OBG', name='ogbg-molhiv')\n",
    "# To inspect the data\n",
    "print(mutag[0])\n",
    "#print(ogb_molhiv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/numba/core/types/__init__.py:108: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.\n",
      "  long_ = _make_signed(np.long)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3577c91c23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSELU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessagePassing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAGEConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGATConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#import matmul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             f'matches your PyTorch install.')\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseStorage\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/torch_sparse/storage.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_scatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_csr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_sort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#print(torch.__version__)\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "#from torch_geometric.utils import accuracy as accuracy_1d\n",
    "from torchmetrics import Accuracy as accuracy_1d\n",
    "from torch.nn import Dropout, SELU\n",
    "from torch_geometric.nn import MessagePassing, SAGEConv, GCNConv, GATConv\n",
    "from torch_sparse import matmul\n",
    "#import matmul\n",
    "\n",
    "class KProp(MessagePassing):\n",
    "    def __init__(self, steps, aggregator, add_self_loops, normalize, cached, transform=lambda x: x):\n",
    "        super().__init__(aggr=aggregator)\n",
    "        self.transform = transform\n",
    "        self.K = steps\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.normalize = normalize\n",
    "        self.cached = cached\n",
    "        self._cached_x = None\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        if self._cached_x is None or not self.cached:\n",
    "            self._cached_x = self.neighborhood_aggregation(x, adj_t)\n",
    "\n",
    "        return self._cached_x\n",
    "\n",
    "    def neighborhood_aggregation(self, x, adj_t):\n",
    "        if self.K <= 0:\n",
    "            return x\n",
    "\n",
    "        if self.normalize:\n",
    "            adj_t = gcn_norm(adj_t, add_self_loops=False)\n",
    "\n",
    "        if self.add_self_loops:\n",
    "            adj_t = adj_t.set_diag()\n",
    "\n",
    "        for k in range(self.K):\n",
    "            x = self.propagate(adj_t, x=x)\n",
    "\n",
    "        x = self.transform(x)\n",
    "        return x\n",
    "\n",
    "    def message_and_aggregate(self, adj_t, x):  # noqa\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = None\n",
    "        self.conv2 = None\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        self.activation = SELU(inplace=True)\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "        x = self.conv1(x, adj_t)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, adj_t)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCN(GNN):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, dropout):\n",
    "        super().__init__(dropout)\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "class GAT(GNN):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, dropout):\n",
    "        super().__init__(dropout)\n",
    "        heads = 4\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, concat=True)\n",
    "        self.conv2 = GATConv(heads * hidden_dim, output_dim, heads=1, concat=False)\n",
    "\n",
    "\n",
    "class GraphSAGE(GNN):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, dropout):\n",
    "        super().__init__(dropout)\n",
    "        self.conv1 = SAGEConv(in_channels=input_dim, out_channels=hidden_dim, normalize=False, root_weight=True)\n",
    "        self.conv2 = SAGEConv(in_channels=hidden_dim, out_channels=output_dim, normalize=False, root_weight=True)\n",
    "\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 num_classes,\n",
    "                 model,\n",
    "                 hidden_dim,\n",
    "                 dropout,\n",
    "                 x_steps,\n",
    "                 y_steps,\n",
    "                 forward_correction\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x_prop = KProp(steps=x_steps, aggregator='add', add_self_loops=False, normalize=True, cached=True)\n",
    "        self.y_prop = KProp(steps=y_steps, aggregator='add', add_self_loops=False, normalize=True, cached=False,\n",
    "                            transform=torch.nn.Softmax(dim=1))\n",
    "\n",
    "        self.gnn = {'gcn': GCN, 'sage': GraphSAGE, 'gat': GAT}[model](\n",
    "            input_dim=input_dim,\n",
    "            output_dim=num_classes,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.cached_yt = None\n",
    "        self.forward_correction = forward_correction\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, adj_t = data.x, data.adj_t\n",
    "        x = self.x_prop(x, adj_t)\n",
    "        x = self.gnn(x, adj_t)\n",
    "\n",
    "        p_y_x = F.softmax(x, dim=1)                                                         # P(y|x')\n",
    "        p_yp_x = torch.matmul(p_y_x, data.T) if self.forward_correction else p_y_x          # P(y'|x')\n",
    "        p_yt_x = self.y_prop(p_yp_x, data.adj_t)                                            # P(y~|x')\n",
    "\n",
    "        return p_y_x, p_yp_x, p_yt_x\n",
    "\n",
    "    def training_step(self, data):\n",
    "        p_y_x, p_yp_x, p_yt_x = self(data)\n",
    "\n",
    "        if self.cached_yt is None:\n",
    "            yp = data.y.float()\n",
    "            yp[data.test_mask] = 0  # to avoid using test labels\n",
    "            self.cached_yt = self.y_prop(yp, data.adj_t)  # y~\n",
    "\n",
    "        loss = self.cross_entropy_loss(p_y=p_yt_x[data.train_mask], y=self.cached_yt[data.train_mask], weighted=False)\n",
    "\n",
    "        metrics = {\n",
    "            'train/loss': loss.item(),\n",
    "            'train/acc': self.accuracy(pred=p_y_x[data.train_mask], target=data.y[data.train_mask]) * 100,\n",
    "            'train/maxacc': data.T[0, 0].item() * 100,\n",
    "        }\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def validation_step(self, data):\n",
    "        p_y_x, p_yp_x, p_yt_x = self(data)\n",
    "\n",
    "        metrics = {\n",
    "            'val/loss': self.cross_entropy_loss(p_yp_x[data.val_mask], data.y[data.val_mask]).item(),\n",
    "            'val/acc': self.accuracy(pred=p_y_x[data.val_mask], target=data.y[data.val_mask]) * 100,\n",
    "            'test/acc': self.accuracy(pred=p_y_x[data.test_mask], target=data.y[data.test_mask]) * 100,\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(pred, target):\n",
    "        pred = pred.argmax(dim=1) if len(pred.size()) > 1 else pred\n",
    "        target = target.argmax(dim=1) if len(target.size()) > 1 else target\n",
    "        return accuracy_1d(pred=pred, target=target)\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(p_y, y, weighted=False):\n",
    "        y_onehot = F.one_hot(y.argmax(dim=1))\n",
    "        loss = -torch.log(p_y + 1e-20) * y_onehot\n",
    "        loss *= y if weighted else 1\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import ToSparseTensor, AddTrainValTestMask\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "from transforms import Normalize, FilterTopClass\n",
    "\n",
    "def load_dataset(dataset, val_ratio  = .25, test_ratio = .25 ):\n",
    "    \n",
    "    if dataset == 'mutag'\n",
    "        \n",
    "        #Get the dataset of interest\n",
    "        mutag = torch_geometric.datasets.TUDataset(root='./data/TUDataset', name='MUTAG')\n",
    "        \n",
    "        #For the time being get one graph\n",
    "        mutag_graph = mutag[5]\n",
    "        \n",
    "        #Get the number of nodes in the graph\n",
    "        numNode = mutag_graph.num_nodes\n",
    "        \n",
    "        #Randomize indices \n",
    "        index = torch.randperm(numNode)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Masks \n",
    "        train_mask = torch.zeros(mutag_graph.num_nodes, dtype=torch.bool)\n",
    "        val_mask   = torch.zeros(mutag_graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask  = torch.zeros(mutag_graph.num_nodes, dtype=torch.bool)\n",
    "        \n",
    "        #Get the splits for training, test, and val data \n",
    "        val_data = int(numNode*val_ratio)\n",
    "        test_data = int(numNode*test_ratio)\n",
    "        train_data = numNode - val_data - test_data\n",
    "        \n",
    "        #Get random nodes from the fractions calculated above \n",
    "        train_index = index[:train_data]\n",
    "        valid_index = index[train_data: train_data+val_data]\n",
    "        test_index = index[train_data+val_data:]\n",
    "        \n",
    "        #Assign masks - used in original code\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "        val_mask[valid_index] = True\n",
    "        \n",
    "        #Set masks in data \n",
    "        mutag_graph.train_mask = train_mask\n",
    "        mutag_graph.test_mask = test_mask\n",
    "        mutag_graph.val_mask = val_mask\n",
    "    \n",
    "        return mutag_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import traceback\n",
    "# import uuid\n",
    "# from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from tqdm.auto import tqdm\n",
    "# from torch_geometric.transforms import Compose\n",
    "# from datasets import load_dataset\n",
    "# from models import NodeClassifier\n",
    "# from models import GraphConvModel\n",
    "# from trainer import Trainer\n",
    "# from transforms import FeatureTransform, FeaturePerturbation, LabelPerturbation\n",
    "# from utils import print_args, WandbLogger, add_parameters_as_argument, \\\n",
    "#     measure_runtime, from_args, str2bool, Enum, EnumAction, colored_text, bootstrap\n",
    "\n",
    "\n",
    "# class LogMode(Enum):\n",
    "#     INDIVIDUAL = 'individual'\n",
    "#     COLLECTIVE = 'collective'\n",
    "\n",
    "\n",
    "# def seed_everything(seed):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# def confidence_interval(data, func=np.mean, size=1000, ci=95, seed=12345):\n",
    "#     bs_replicates = bootstrap(data, func=func, n_boot=size, seed=seed)\n",
    "#     p = 50 - ci / 2, 50 + ci / 2\n",
    "#     bounds = np.nanpercentile(bs_replicates, p)\n",
    "#     return (bounds[1] - bounds[0]) / 2\n",
    "\n",
    "\n",
    "# @measure_runtime\n",
    "# def run(args):\n",
    "#     dataset = from_args(load_dataset, args)\n",
    "\n",
    "#     test_acc = []\n",
    "#     run_metrics = {}\n",
    "#     run_id = str(uuid.uuid1())\n",
    "\n",
    "#     logger = None\n",
    "#     if args.log and args.log_mode == LogMode.COLLECTIVE:\n",
    "#         logger = WandbLogger(project=args.project_name, config=args, enabled=args.log, reinit=False, group=run_id)\n",
    "\n",
    "#     progbar = tqdm(range(args.repeats), file=sys.stdout)\n",
    "#     for version in progbar:\n",
    "\n",
    "#         if args.log and args.log_mode == LogMode.INDIVIDUAL:\n",
    "#             args.version = version\n",
    "#             logger = WandbLogger(project=args.project_name, config=args, enabled=args.log, group=run_id)\n",
    "\n",
    "#         try:\n",
    "#             data = dataset.clone().to(args.device)\n",
    "\n",
    "#             # preprocess data\n",
    "#             data = Compose([\n",
    "#                 from_args(FeatureTransform, args),\n",
    "#                 from_args(FeaturePerturbation, args),\n",
    "#                 from_args(LabelPerturbation, args)\n",
    "#             ])(data)\n",
    "\n",
    "#             # define model\n",
    "#             model = from_args(GraphConvModel, args, input_dim=data.num_features, num_classes=data.num_classes)\n",
    "\n",
    "#             # train the model\n",
    "#             trainer = from_args(Trainer, args, logger=logger if args.log_mode == LogMode.INDIVIDUAL else None)\n",
    "#             best_metrics = trainer.fit(model, data)\n",
    "\n",
    "#             # process results\n",
    "#             for metric, value in best_metrics.items():\n",
    "#                 run_metrics[metric] = run_metrics.get(metric, []) + [value]\n",
    "\n",
    "#             test_acc.append(best_metrics['test/acc'])\n",
    "#             progbar.set_postfix({'last_test_acc': test_acc[-1], 'avg_test_acc': np.mean(test_acc)})\n",
    "\n",
    "#         except Exception as e:\n",
    "#             error = ''.join(traceback.format_exception(Exception, e, e.__traceback__))\n",
    "#             logger.log_summary({'error': error})\n",
    "#             raise e\n",
    "#         finally:\n",
    "#             if args.log and args.log_mode == LogMode.INDIVIDUAL:\n",
    "#                 logger.finish()\n",
    "\n",
    "#     if args.log and args.log_mode == LogMode.COLLECTIVE:\n",
    "#         summary = {}\n",
    "#         for metric, values in run_metrics.items():\n",
    "#             summary[metric + '_mean'] = np.mean(values)\n",
    "#             summary[metric + '_ci'] = confidence_interval(values, size=1000, ci=95, seed=args.seed)\n",
    "\n",
    "#         logger.log_summary(summary)\n",
    "\n",
    "#     if not args.log:\n",
    "#         os.makedirs(args.output_dir, exist_ok=True)\n",
    "#         df_results = pd.DataFrame(test_acc, columns=['test/acc']).rename_axis('version').reset_index()\n",
    "#         df_results['Name'] = run_id\n",
    "#         for arg_name, arg_val in vars(args).items():\n",
    "#             df_results[arg_name] = [arg_val] * len(test_acc)\n",
    "#         df_results.to_csv(os.path.join(args.output_dir, f'{run_id}.csv'), index=False)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     init_parser = ArgumentParser(add_help=False, conflict_handler='resolve')\n",
    "\n",
    "#     # dataset args\n",
    "#     group_dataset = init_parser.add_argument_group('dataset arguments')\n",
    "#     add_parameters_as_argument(load_dataset, group_dataset)\n",
    "\n",
    "#     # data transformation args\n",
    "#     group_perturb = init_parser.add_argument_group(f'data transformation arguments')\n",
    "#     add_parameters_as_argument(FeatureTransform, group_perturb)\n",
    "#     add_parameters_as_argument(FeaturePerturbation, group_perturb)\n",
    "#     add_parameters_as_argument(LabelPerturbation, group_perturb)\n",
    "\n",
    "#     # model args\n",
    "#     group_model = init_parser.add_argument_group(f'model arguments')\n",
    "#     add_parameters_as_argument(GraphConvModel, group_model)\n",
    "\n",
    "#     # trainer arguments (depends on perturbation)\n",
    "#     group_trainer = init_parser.add_argument_group(f'trainer arguments')\n",
    "#     add_parameters_as_argument(Trainer, group_trainer)\n",
    "#     group_trainer.add_argument('--device', help='desired device for training', choices=['cpu', 'cuda'], default='cuda')\n",
    "\n",
    "#     # experiment args\n",
    "#     group_expr = init_parser.add_argument_group('experiment arguments')\n",
    "#     group_expr.add_argument('-s', '--seed', type=int, default=None, help='initial random seed')\n",
    "#     group_expr.add_argument('-r', '--repeats', type=int, default=1, help=\"number of times the experiment is repeated\")\n",
    "#     group_expr.add_argument('-o', '--output-dir', type=str, default='./output', help=\"directory to store the results\")\n",
    "#     group_expr.add_argument('--log', type=str2bool, nargs='?', const=True, default=False, help='enable wandb logging')\n",
    "#     group_expr.add_argument('--log-mode', type=LogMode, action=EnumAction, default=LogMode.INDIVIDUAL,\n",
    "#                             help='wandb logging mode')\n",
    "#     group_expr.add_argument('--project-name', type=str, default='LPGNN', help='wandb project name')\n",
    "\n",
    "#     parser = ArgumentParser(parents=[init_parser], formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "#     args = parser.parse_args()\n",
    "#     print_args(args)\n",
    "#     args.cmd = ' '.join(sys.argv)  # store calling command\n",
    "\n",
    "#     if args.seed:\n",
    "#         seed_everything(args.seed)\n",
    "\n",
    "#     if args.device == 'cuda' and not torch.cuda.is_available():\n",
    "#         print(colored_text('CUDA is not available, falling back to CPU', color='red'))\n",
    "#         args.device = 'cpu'\n",
    "\n",
    "#     try:\n",
    "#         run(args)\n",
    "#     except KeyboardInterrupt:\n",
    "#         print('Graceful Shutdown...')\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
