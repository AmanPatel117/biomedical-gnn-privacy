{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ogb\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv \\\n",
        "  -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
        "\n",
        "!pip install -U torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-m9TIxvNup-",
        "outputId": "9d1d5163-33b5-421f-a024-a4a0135adb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ogb in /usr/local/lib/python3.12/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.5.0)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.12/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.12/dist-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from outdated>=0.2.0->ogb) (2.32.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.0->ogb) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.0->ogb) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.8.3)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
            "Requirement already satisfied: pyg_lib in /usr/local/lib/python3.12/dist-packages (0.4.0+pt24cu124)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.12/dist-packages (2.1.2+pt24cu124)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.12/dist-packages (0.6.18+pt24cu124)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.12/dist-packages (1.6.3+pt24cu124)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.12/dist-packages (1.2.2+pt24cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch_sparse) (1.16.2)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaxUXKOAMfG3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch_geometric.datasets import TUDataset, PPI\n",
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "from torch_geometric.data import DataLoader, HeteroData\n",
        "import torch\n",
        "import bz2, json\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading our datasets"
      ],
      "metadata": {
        "id": "E1xzcrXYODZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MUTAG, PROTEINS, and PPI"
      ],
      "metadata": {
        "id": "cMGh17UAObV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "dataset = TUDataset(root='data/TUDataset', name='PROTEINS')\n",
        "dataset = PPI(root='data/PPI')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJGAgx2kNwU0",
        "outputId": "49dcc46f-af26-4b84-be97-8494820f51f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting data/PPI/ppi.zip\n",
            "Processing...\n",
            "/usr/local/lib/python3.12/dist-packages/networkx/readwrite/json_graph/node_link.py:290: FutureWarning: \n",
            "The default value will be changed to `edges=\"edges\" in NetworkX 3.6.\n",
            "\n",
            "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
            "\n",
            "  nx.node_link_graph(data, edges=\"links\") to preserve current behavior, or\n",
            "  nx.node_link_graph(data, edges=\"edges\") for forward compatibility.\n",
            "  warnings.warn(\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torch_geometric.datasets import TUDataset, PPI\n",
        "\n",
        "def _undirected_unique_edge_count(edge_index: torch.Tensor) -> int:\n",
        "    if edge_index is None or edge_index.numel() == 0:\n",
        "        return 0\n",
        "    u, v = edge_index\n",
        "    lo = torch.minimum(u, v)\n",
        "    hi = torch.maximum(u, v)\n",
        "    pairs = torch.stack([lo, hi], dim=1)           # [E, 2]\n",
        "    uniq = torch.unique(pairs, dim=0)              # unique undirected edges\n",
        "    return int(uniq.size(0))\n",
        "\n",
        "def summarize_dataset(dataset, name: str):\n",
        "    # Works for TUDataset, PPI splits, and ConcatDataset of PyG datasets\n",
        "    if isinstance(dataset, ConcatDataset):\n",
        "        it = (dataset[i] for i in range(len(dataset)))\n",
        "        num_graphs = len(dataset)\n",
        "    else:\n",
        "        it = iter(dataset)\n",
        "        num_graphs = len(dataset)\n",
        "\n",
        "    num_nodes_list, num_edges_list, densities, avg_degrees = [], [], [], []\n",
        "    for g in it:\n",
        "        n = int(g.num_nodes)\n",
        "        e = _undirected_unique_edge_count(g.edge_index.long())\n",
        "        num_nodes_list.append(n)\n",
        "        num_edges_list.append(e)\n",
        "        avg_degrees.append(0.0 if n == 0 else (2.0 * e) / n)\n",
        "        densities.append(0.0 if n <= 1 else (2.0 * e) / (n * (n - 1)))\n",
        "\n",
        "    print(f\"==== {name} Summary ====\")\n",
        "    print(f\"Graphs: {num_graphs}\")\n",
        "    print(f\"Total nodes: {sum(num_nodes_list):,}\")\n",
        "    print(f\"Total edges: {sum(num_edges_list):,}\")\n",
        "    print(f\"Avg nodes per graph: {np.mean(num_nodes_list):.2f}\")\n",
        "    print(f\"Avg edges per graph: {np.mean(num_edges_list):.2f}\")\n",
        "    print(f\"Avg degree (mean over graphs): {np.mean(avg_degrees):.2f}\")\n",
        "    print(f\"Avg density (mean over graphs): {np.mean(densities):.6f}\")\n",
        "    print(f\"Max nodes in a graph: {max(num_nodes_list)}\")\n",
        "    print(f\"Max edges in a graph: {max(num_edges_list)}\\n\")\n",
        "\n",
        "# ---- Load datasets ----\n",
        "mutag = TUDataset(root='data/TUDataset', name='MUTAG')           # 188 graphs\n",
        "proteins = TUDataset(root='data/TUDataset', name='PROTEINS')     # 1,113 graphs\n",
        "\n",
        "# PPI: load ALL splits, not just train\n",
        "ppi_train = PPI(root='data/PPI', split='train')   # 20 graphs\n",
        "ppi_val   = PPI(root='data/PPI', split='val')     # 2 graphs\n",
        "ppi_test  = PPI(root='data/PPI', split='test')    # 2 graphs\n",
        "ppi_all   = ConcatDataset([ppi_train, ppi_val, ppi_test])  # 24 graphs total\n",
        "\n",
        "# ---- Summaries ----\n",
        "summarize_dataset(mutag, \"MUTAG\")\n",
        "summarize_dataset(proteins, \"PROTEINS\")\n",
        "summarize_dataset(ppi_all, \"PPI (all splits)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8Wu6HwIhg2S",
        "outputId": "6cd70375-932d-4488-b73a-bf3e5f9e724b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== MUTAG Summary ====\n",
            "Graphs: 188\n",
            "Total nodes: 3,371\n",
            "Total edges: 3,721\n",
            "Avg nodes per graph: 17.93\n",
            "Avg edges per graph: 19.79\n",
            "Avg degree (mean over graphs): 2.19\n",
            "Avg density (mean over graphs): 0.138454\n",
            "Max nodes in a graph: 28\n",
            "Max edges in a graph: 33\n",
            "\n",
            "==== PROTEINS Summary ====\n",
            "Graphs: 1113\n",
            "Total nodes: 43,471\n",
            "Total edges: 81,044\n",
            "Avg nodes per graph: 39.06\n",
            "Avg edges per graph: 72.82\n",
            "Avg degree (mean over graphs): 3.73\n",
            "Avg density (mean over graphs): 0.212176\n",
            "Max nodes in a graph: 620\n",
            "Max edges in a graph: 1049\n",
            "\n",
            "==== PPI (all splits) Summary ====\n",
            "Graphs: 24\n",
            "Total nodes: 56,944\n",
            "Total edges: 793,632\n",
            "Avg nodes per graph: 2372.67\n",
            "Avg edges per graph: 33068.00\n",
            "Avg degree (mean over graphs): 26.44\n",
            "Avg density (mean over graphs): 0.012015\n",
            "Max nodes in a graph: 3480\n",
            "Max edges in a graph: 53377\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ogb-molhiv"
      ],
      "metadata": {
        "id": "Gg-ioolDOiA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Summary stats from archive_extracted/raw with 2-col edge.csv (no headers, no graph_id) ===\n",
        "\n",
        "import os, zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "zip_path = \"archive.zip\"\n",
        "extract_dir = \"archive_extracted\"\n",
        "raw_dir = os.path.join(extract_dir, \"raw\")\n",
        "\n",
        "# 0) Ensure extracted\n",
        "if not os.path.exists(raw_dir):\n",
        "    assert os.path.exists(zip_path), f\"Couldn't find {zip_path} in CWD\"\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(extract_dir)\n",
        "    assert os.path.exists(raw_dir), f\"Couldn't find {raw_dir} after unzip\"\n",
        "\n",
        "edges_csv = os.path.join(raw_dir, \"edge.csv\")\n",
        "num_nodes_csv = os.path.join(raw_dir, \"num-node-list.csv\")\n",
        "num_edges_csv = os.path.join(raw_dir, \"num-edge-list.csv\")\n",
        "\n",
        "# 1) Load files\n",
        "assert os.path.exists(edges_csv), f\"Missing {edges_csv}\"\n",
        "assert os.path.exists(num_nodes_csv), f\"Missing {num_nodes_csv}\"\n",
        "assert os.path.exists(num_edges_csv), f\"Missing {num_edges_csv}\"\n",
        "\n",
        "# edge.csv has NO header and only two columns -> read as src,dst\n",
        "edges = pd.read_csv(edges_csv, header=None, names=[\"src\", \"dst\"])\n",
        "e_counts = pd.read_csv(num_edges_csv, header=None)[0].astype(int).tolist()\n",
        "n_counts = pd.read_csv(num_nodes_csv, header=None)[0].astype(int).tolist()\n",
        "\n",
        "# Sanity checks\n",
        "total_edges_listed = len(edges)\n",
        "if sum(e_counts) != total_edges_listed:\n",
        "    # Some dumps store each undirected edge twice (u,v) + (v,u). Try halving if that matches.\n",
        "    if sum(e_counts) * 2 == total_edges_listed:\n",
        "        print(\"Note: edge.csv appears to store both directions; will unique within each graph slice.\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"num-edge-list.csv sums to {sum(e_counts)} but edge.csv has {total_edges_listed} rows.\"\n",
        "        )\n",
        "\n",
        "if len(e_counts) != len(n_counts):\n",
        "    raise ValueError(\n",
        "        f\"num-edge-list.csv has {len(e_counts)} rows but num-node-list.csv has {len(n_counts)} rows.\"\n",
        "    )\n",
        "\n",
        "# 2) Per-graph computation by slicing edge.csv according to e_counts\n",
        "num_graphs = len(e_counts)\n",
        "num_nodes_list, num_edges_list, avg_degrees, densities = [], [], [], []\n",
        "\n",
        "start = 0\n",
        "for i in range(num_graphs):\n",
        "    n = int(n_counts[i])\n",
        "    cnt_e = int(e_counts[i])\n",
        "\n",
        "    # slice this graph's edges\n",
        "    end = start + cnt_e\n",
        "    sub = edges.iloc[start:end].to_numpy(dtype=np.int64)\n",
        "    start = end\n",
        "\n",
        "    if sub.size == 0:\n",
        "        e_undir = 0\n",
        "    else:\n",
        "        u = sub[:, 0]\n",
        "        v = sub[:, 1]\n",
        "        lo = np.minimum(u, v)\n",
        "        hi = np.maximum(u, v)\n",
        "        pairs = np.stack([lo, hi], axis=1)\n",
        "        # unique undirected edges\n",
        "        pairs_unique = np.unique(pairs, axis=0)\n",
        "        e_undir = int(pairs_unique.shape[0])\n",
        "\n",
        "    num_nodes_list.append(n)\n",
        "    num_edges_list.append(e_undir)\n",
        "    avg_degrees.append(0.0 if n == 0 else (2.0 * e_undir) / n)\n",
        "    densities.append(0.0 if n <= 1 else (2.0 * e_undir) / (n * (n - 1)))\n",
        "\n",
        "# 3) Print in your format\n",
        "title = os.path.basename(os.path.abspath(extract_dir))\n",
        "print(f\"==== {title} Summary ====\")\n",
        "print(f\"Graphs: {num_graphs}\")\n",
        "print(f\"Total nodes: {sum(num_nodes_list):,}\")\n",
        "print(f\"Total edges: {sum(num_edges_list):,}\")\n",
        "print(f\"Avg nodes per graph: {np.mean(num_nodes_list):.2f}\")\n",
        "print(f\"Avg edges per graph: {np.mean(num_edges_list):.2f}\")\n",
        "print(f\"Avg degree (mean over graphs): {np.mean(avg_degrees):.2f}\")\n",
        "print(f\"Avg density (mean over graphs): {np.mean(densities):.6f}\")\n",
        "print(f\"Max nodes in a graph: {max(num_nodes_list)}\")\n",
        "print(f\"Max edges in a graph: {max(num_edges_list)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "56r3iWPYO2xu",
        "outputId": "66786080-41b0-49fe-dd73-af38c30dfede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Couldn't find archive.zip in CWD",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-463879827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 0) Ensure extracted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Couldn't find {zip_path} in CWD\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Couldn't find archive.zip in CWD"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hetionet"
      ],
      "metadata": {
        "id": "dNnudBjOSmQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_id(x):\n",
        "    # Normalize IDs: e.g., [\"Gene\", 1234] -> \"Gene::1234\"\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        return \"::\".join(map(str, x))\n",
        "    return str(x)\n",
        "\n",
        "def first_key(d, candidates, *, required=False, context=\"\"):\n",
        "    for k in candidates:\n",
        "        if k in d:\n",
        "            return k\n",
        "    if required:\n",
        "        raise KeyError(f\"None of {candidates} found in {context or 'object'}; available keys: {list(d.keys())[:10]}\")\n",
        "    return None\n",
        "\n",
        "with open(\"hetionet-v1.0-metagraph.json\", \"r\") as f:\n",
        "    metagraph = json.load(f)\n",
        "print(\"Metanode kinds (sample):\", metagraph.get(\"metanode_kinds\", [])[:8])\n",
        "\n",
        "with bz2.open(\"hetionet-v1.0.json.bz2\", \"rt\") as f:\n",
        "    het = json.load(f)\n",
        "\n",
        "nodes_container = het.get(\"nodes\")\n",
        "edges_container = het.get(\"edges\")\n",
        "if nodes_container is None or edges_container is None:\n",
        "    raise KeyError(\"Top-level keys 'nodes' and/or 'edges' not found in the JSON.\")\n",
        "\n",
        "# --- Detect node storage style ---\n",
        "node_is_dict_map = isinstance(nodes_container, dict)\n",
        "if node_is_dict_map:\n",
        "    nodes_iter = nodes_container.items()\n",
        "    sample_id, sample_node = next(iter(nodes_iter))\n",
        "    nodes_iter = nodes_container.items()  # rewind\n",
        "else:\n",
        "    nodes_iter = enumerate(nodes_container)\n",
        "    _, sample_node = next(iter(nodes_iter))\n",
        "    nodes_iter = enumerate(nodes_container)  # rewind\n",
        "\n",
        "node_id_key_candidates   = [\"id\", \"identifier\", \"node_id\"]\n",
        "node_type_key_candidates = [\"kind\", \"type\", \"label\", \"node_type\"]\n",
        "\n",
        "if not node_is_dict_map:\n",
        "    # ensure node dict has an id-like key\n",
        "    _ = first_key(sample_node, node_id_key_candidates, required=True, context=\"node\")\n",
        "node_type_key = first_key(sample_node, node_type_key_candidates, required=True, context=\"node\")\n",
        "\n",
        "# --- Pass 1: collect ids per type (and build aliasable identity strings) ---\n",
        "type_to_ids = defaultdict(list)     # canonical IDs per type (base id without type prefix)\n",
        "canon_to_type = {}                  # canonical id -> type (e.g., \"UBERON:0000178\" -> \"Anatomy\")\n",
        "\n",
        "print(\"Indexing nodes (with canonical IDs)...\")\n",
        "if node_is_dict_map:\n",
        "    for raw_id, n in tqdm(nodes_iter):\n",
        "        ntype = n.get(node_type_key)\n",
        "        if ntype is None:\n",
        "            raise KeyError(f\"Node missing '{node_type_key}'. Keys present: {list(n.keys())[:10]}\")\n",
        "        # canonical id: prefer node's own id field if present, else dict key\n",
        "        nid = str(n.get(first_key(n, node_id_key_candidates) or raw_id))\n",
        "        # If nid looks like \"Anatomy::UBERON:0000178\", strip prefix for canonical\n",
        "        canon = nid.split(\"::\", 1)[1] if \"::\" in nid else nid\n",
        "        canon_to_type[canon] = ntype\n",
        "        type_to_ids[ntype].append(canon)\n",
        "else:\n",
        "    for _, n in tqdm(nodes_iter, total=len(nodes_container)):\n",
        "        ntype = n.get(node_type_key)\n",
        "        nid   = str(n[first_key(n, node_id_key_candidates, required=True, context=\"node\")])\n",
        "        canon = nid.split(\"::\", 1)[1] if \"::\" in nid else nid\n",
        "        canon_to_type[canon] = ntype\n",
        "        type_to_ids[ntype].append(canon)\n",
        "\n",
        "# --- Assign per-type contiguous indices ---\n",
        "id_to_local = {ntype: {} for ntype in type_to_ids}\n",
        "type_counts = {}\n",
        "print(\"Assigning per-type node indices...\")\n",
        "for ntype, canon_ids in tqdm(type_to_ids.items()):\n",
        "    for i, canon in enumerate(canon_ids):\n",
        "        id_to_local[ntype][canon] = i\n",
        "    type_counts[ntype] = len(canon_ids)\n",
        "\n",
        "# --- Build a resolver that accepts plain ids and qualified ids ---\n",
        "def resolve_type_and_index(any_id_str):\n",
        "    \"\"\"Accepts:\n",
        "       - 'UBERON:0000178'\n",
        "       - 'Anatomy::UBERON:0000178'\n",
        "    Returns (node_type, local_index).\"\"\"\n",
        "    if \"::\" in any_id_str:\n",
        "        ntype, canon = any_id_str.split(\"::\", 1)\n",
        "        return ntype, id_to_local[ntype][canon]\n",
        "    else:\n",
        "        ntype = canon_to_type[any_id_str]\n",
        "        return ntype, id_to_local[ntype][any_id_str]\n",
        "\n",
        "# --- Prepare edges iterable and detect edge keys ---\n",
        "if isinstance(edges_container, dict):\n",
        "    flat_edges = []\n",
        "    for rel, elist in edges_container.items():\n",
        "        for e in elist:\n",
        "            e = dict(e)\n",
        "            e.setdefault(\"kind\", rel)\n",
        "            flat_edges.append(e)\n",
        "    edges_iterable = flat_edges\n",
        "else:\n",
        "    edges_iterable = edges_container\n",
        "\n",
        "first_edge = edges_iterable[0]\n",
        "edge_src_key = first_key(first_edge, [\"source_id\", \"source\", \"src\", \"start\"], required=True, context=\"edge\")\n",
        "edge_dst_key = first_key(first_edge, [\"target_id\", \"target\", \"dst\", \"end\"], required=True, context=\"edge\")\n",
        "edge_rel_key = first_key(first_edge, [\"kind\", \"type\", \"relation\", \"rel\"], required=True, context=\"edge\")\n",
        "\n",
        "# --- Group edges by (src_type, relation, dst_type) ---\n",
        "edge_lists = defaultdict(list)\n",
        "\n",
        "print(\"Grouping edges...\")\n",
        "for e in tqdm(edges_iterable):\n",
        "    src_raw = e[edge_src_key]\n",
        "    dst_raw = e[edge_dst_key]\n",
        "    rel     = e[edge_rel_key]\n",
        "\n",
        "    src_id = norm_id(src_raw)  # may become \"Anatomy::UBERON:0000178\"\n",
        "    dst_id = norm_id(dst_raw)\n",
        "\n",
        "    try:\n",
        "        src_type, src_idx = resolve_type_and_index(src_id)\n",
        "        dst_type, dst_idx = resolve_type_and_index(dst_id)\n",
        "    except KeyError as ke:\n",
        "        missing = src_id if \"::\" in src_id and src_id.split(\"::\",1)[1] not in canon_to_type else dst_id\n",
        "        raise KeyError(\n",
        "            f\"Edge references unknown node id: {missing}. \"\n",
        "            f\"If your nodes store plain ids (e.g., 'UBERON:...') and edges use qualified ids \"\n",
        "            f\"(e.g., 'Anatomy::UBERON:...'), this loader should handle it. \"\n",
        "            f\"Otherwise, inspect node/edge keys.\"\n",
        "        ) from ke\n",
        "\n",
        "    edge_lists[(src_type, rel, dst_type)].append([src_idx, dst_idx])\n",
        "\n",
        "# --- Build HeteroData ---\n",
        "data = HeteroData()\n",
        "for ntype, count in type_counts.items():\n",
        "    data[ntype].num_nodes = count\n",
        "\n",
        "for key, pairs in tqdm(edge_lists.items(), desc=\"Finalizing edge_index\"):\n",
        "    edge_index = torch.tensor(pairs, dtype=torch.long).t().contiguous()\n",
        "    data[key].edge_index = edge_index\n",
        "\n",
        "print(\"✅ Loaded Hetionet into HeteroData\")\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1RoMotJSoGv",
        "outputId": "76608ad1-32cc-42aa-9d0d-c015a1bba0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metanode kinds (sample): ['Anatomy', 'Biological Process', 'Cellular Component', 'Compound', 'Disease', 'Gene', 'Molecular Function', 'Pathway']\n",
            "Indexing nodes (with canonical IDs)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47031/47031 [00:00<00:00, 674871.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigning per-type node indices...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:00<00:00, 763.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grouping edges...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2250197/2250197 [00:23<00:00, 97069.90it/s] \n",
            "Finalizing edge_index: 100%|██████████| 24/24 [00:00<00:00, 27.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded Hetionet into HeteroData\n",
            "HeteroData(\n",
            "  Molecular Function={ num_nodes=2884 },\n",
            "  Side Effect={ num_nodes=5734 },\n",
            "  Gene={ num_nodes=20945 },\n",
            "  Biological Process={ num_nodes=11381 },\n",
            "  Compound={ num_nodes=1552 },\n",
            "  Pathway={ num_nodes=1822 },\n",
            "  Anatomy={ num_nodes=402 },\n",
            "  Cellular Component={ num_nodes=1391 },\n",
            "  Symptom={ num_nodes=438 },\n",
            "  Disease={ num_nodes=137 },\n",
            "  Pharmacologic Class={ num_nodes=345 },\n",
            "  (Anatomy, upregulates, Gene)={ edge_index=[2, 97848] },\n",
            "  (Anatomy, expresses, Gene)={ edge_index=[2, 526407] },\n",
            "  (Gene, interacts, Gene)={ edge_index=[2, 147164] },\n",
            "  (Gene, participates, Pathway)={ edge_index=[2, 84372] },\n",
            "  (Anatomy, downregulates, Gene)={ edge_index=[2, 102240] },\n",
            "  (Compound, upregulates, Gene)={ edge_index=[2, 18756] },\n",
            "  (Compound, causes, Side Effect)={ edge_index=[2, 138944] },\n",
            "  (Gene, participates, Molecular Function)={ edge_index=[2, 97222] },\n",
            "  (Gene, participates, Biological Process)={ edge_index=[2, 559504] },\n",
            "  (Compound, binds, Gene)={ edge_index=[2, 11571] },\n",
            "  (Gene, regulates, Gene)={ edge_index=[2, 265672] },\n",
            "  (Disease, associates, Gene)={ edge_index=[2, 12623] },\n",
            "  (Gene, participates, Cellular Component)={ edge_index=[2, 73566] },\n",
            "  (Gene, covaries, Gene)={ edge_index=[2, 61690] },\n",
            "  (Compound, downregulates, Gene)={ edge_index=[2, 21102] },\n",
            "  (Disease, localizes, Anatomy)={ edge_index=[2, 3602] },\n",
            "  (Compound, resembles, Compound)={ edge_index=[2, 6486] },\n",
            "  (Disease, upregulates, Gene)={ edge_index=[2, 7731] },\n",
            "  (Disease, downregulates, Gene)={ edge_index=[2, 7623] },\n",
            "  (Compound, treats, Disease)={ edge_index=[2, 755] },\n",
            "  (Pharmacologic Class, includes, Compound)={ edge_index=[2, 1029] },\n",
            "  (Disease, presents, Symptom)={ edge_index=[2, 3357] },\n",
            "  (Disease, resembles, Disease)={ edge_index=[2, 543] },\n",
            "  (Compound, palliates, Disease)={ edge_index=[2, 390] }\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"==== Hetionet Summary ====\")\n",
        "\n",
        "# --- Type counts ---\n",
        "num_node_types = len(data.node_types)\n",
        "num_edge_types = len(data.edge_types)\n",
        "print(f\"Node types: {num_node_types}\")\n",
        "print(f\"Edge relation types: {num_edge_types}\\n\")\n",
        "\n",
        "\n",
        "# --- Totals (robust to missing edge_index) ---\n",
        "total_nodes = sum(int(data[nt].num_nodes) for nt in data.node_types)\n",
        "\n",
        "total_edges = 0\n",
        "for et in data.edge_types:\n",
        "    store = data[et]\n",
        "    if \"edge_index\" in store:\n",
        "        total_edges += int(store.edge_index.size(1))\n",
        "print(f\"Total nodes: {total_nodes:,}\")\n",
        "print(f\"Total edges: {total_edges:,}\\n\")\n",
        "\n",
        "# --- Global averages/density (undirected-style avg degree; density is crude) ---\n",
        "avg_deg = (2 * total_edges / total_nodes) if total_nodes > 0 else float(\"nan\")\n",
        "density = (total_edges / (total_nodes * (total_nodes - 1))) if total_nodes > 1 else float(\"nan\")\n",
        "\n",
        "print(f\"Average degree    = {avg_deg:.2f}\")\n",
        "print(f\"Graph density (approx) ≈ {density:.8f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z7ECidwfzio",
        "outputId": "7ad92295-a92a-4afd-a157-7511b0c297d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Hetionet Summary ====\n",
            "Node types: 11\n",
            "Edge relation types: 25\n",
            "\n",
            "Total nodes: 47,031\n",
            "Total edges: 2,250,197\n",
            "\n",
            "Average degree    = 95.69\n",
            "Graph density (approx) ≈ 0.00101733\n"
          ]
        }
      ]
    }
  ]
}