{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsDKh7tbG4fL"
      },
      "source": [
        "## LINKTELLER: About the paper\n",
        "\n",
        "In this paper, we focus on the edge privacy, and consider a training scenario here the data holder Bob with node features will first send training node features to Alice who owns the adjacency information. Alice will then train\n",
        "a graph neural network (GNN) with the joint information and provide an inference API to Bob. During inference time, Bob is able to provide test node features and query the API to obtain the predictions for test nodes. Under this setting, we first propose a privacy attack LINKTELLER via influence analysis to infer the private edge information held by Alice via designing adversarial queries for Bob."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIXGBlgdC9pe"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVe60uwZCywD",
        "outputId": "3f384bbf-d69c-476f-d517-5ccf4086fcc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.3.1+cu121 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.3.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install --index-url https://download.pytorch.org/whl/cu121 torch==2.3.1\n",
        "\n",
        "# 2) PyG and companions compiled for torch 2.3.1 + cu121\n",
        "!pip -q install -f https://data.pyg.org/whl/torch-2.3.1+cu121.html \\\n",
        "  torch_geometric==2.5.3 torch_scatter==2.1.2 torch_sparse==0.6.18\n",
        "\n",
        "# 3) Pin fsspec to avoid the LocalFileSystem.mv() signature mismatch\n",
        "!pip -q install --force-reinstall --no-deps fsspec==2023.6.0\n",
        "\n",
        "# (Optional utilities)\n",
        "!pip -q install scikit-learn networkx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGyUZyYnD0KS",
        "outputId": "7ea8c0bd-8e3a-4029-ab3e-5e068af2bf41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import to_networkx, dense_to_sparse\n",
        "from torch_geometric.nn import GCNConv\n",
        "import networkx as nx\n",
        "from sklearn.cluster import KMeans\n",
        "import torch, fsspec, torch_geometric\n",
        "from torch_geometric.datasets import TUDataset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math, random\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsuVWOhQEBh_"
      },
      "source": [
        "## Loading Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also preprocess labels"
      ],
      "metadata": {
        "id": "Gv0_jXuqCw5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import BaseTransform\n",
        "class graphToNodeLabel(BaseTransform):\n",
        "    def __call__(self, data):\n",
        "        # Ensure x (node features) exists and is a tensor\n",
        "        y = data.y\n",
        "        node_size = data.x.size(0)\n",
        "        y_expanded = y.expand(node_size)\n",
        "        data.y = y_expanded\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "7fJZCTPZCU_A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOKs7QK-ECmx",
        "outputId": "c5487882-f377-4731-c22d-9ffeff49fd93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(edge_index=[2, 62], x=[28, 7], edge_attr=[62, 4], y=[28])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "dataset = TUDataset(root='data/TUD', name='MUTAG', transform=graphToNodeLabel())  # 188 graphs\n",
        "sizes = [data.num_nodes for data in dataset]\n",
        "idx = int(np.argmax([n if n >= 25 else 0 for n in sizes]))  # pick a larger graph\n",
        "data = dataset[idx]\n",
        "print(data)\n",
        "#print(f\"Graph index {idx}: nodes={data.num_nodes}, edges={data.num_edges // 2} (undirected)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregate Dataset into 1 Batch for a large single graph\n",
        "\n",
        "Linkteller makes no assumption of a single connected component, so we can set the node features to the graph classification and aggregate all graphs into a single graph"
      ],
      "metadata": {
        "id": "TptYtj0EAPhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Batch\n",
        "\n",
        "print(dataset[0])\n",
        "#aggregate all MUTAG data into one graph\n",
        "big_graph = Batch.from_data_list([data for data in dataset])\n",
        "print(big_graph)\n",
        "data = big_graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFypUSpU_GC2",
        "outputId": "1e362872-ad43-4bfb-d683-ac63c118624f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[17])\n",
            "DataBatch(edge_index=[2, 7442], x=[3371, 7], edge_attr=[7442, 4], y=[3371], batch=[3371], ptr=[189])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXqBcZwLHdlX"
      },
      "source": [
        "## BUild X features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeNoQMQkEMVJ",
        "outputId": "82a9e1d4-18ea-46ff-bf83-8a9b9445038a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splits: train 2022, val 674, test 675\n"
          ]
        }
      ],
      "source": [
        "X = data.x.float()  # [N, D]\n",
        "N, D = X.shape\n",
        "\n",
        "\n",
        "#K = min(3, len(torch.unique(X, dim=0))) if len(torch.unique(X, dim=0))>1 else 2\n",
        "#km = KMeans(n_clusters=K, n_init=10, random_state=0).fit(X.numpy())\n",
        "#y_node = torch.from_numpy(km.labels_).long()\n",
        "\n",
        "# Train/val/test node splits\n",
        "idx_all = np.arange(N)\n",
        "idx_train, idx_tmp = train_test_split(idx_all, test_size=0.4, random_state=42, stratify=data.y.numpy())\n",
        "idx_val, idx_test = train_test_split(idx_tmp, test_size=0.5, random_state=42)\n",
        "\n",
        "train_mask = torch.zeros(N, dtype=torch.bool); train_mask[idx_train] = True\n",
        "val_mask   = torch.zeros(N, dtype=torch.bool); val_mask[idx_val] = True\n",
        "test_mask  = torch.zeros(N, dtype=torch.bool); test_mask[idx_test] = True\n",
        "\n",
        "print(f\"Splits: train {train_mask.sum().item()}, val {val_mask.sum().item()}, test {test_mask.sum().item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jFF3umCH1Ps"
      },
      "source": [
        "## Adjacency (A) helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3dXS5STHQVz",
        "outputId": "2ec268c5-ed19-45fa-be6e-d49c24ff61a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True undirected edges: 3721 | density=0.0007\n"
          ]
        }
      ],
      "source": [
        "# Edge index is undirected in PyG; keep it as-is\n",
        "edge_index = data.edge_index  # [2, E]\n",
        "\n",
        "# For evaluation convenience, build a boolean adjacency (without self loops)\n",
        "A = torch.zeros((N, N), dtype=torch.bool)\n",
        "A[edge_index[0], edge_index[1]] = True\n",
        "A[edge_index[1], edge_index[0]] = True\n",
        "A.fill_diagonal_(False)\n",
        "true_edges_undirected = torch.nonzero(torch.triu(A, diagonal=1), as_tuple=False)  # [M, 2]\n",
        "M_true = true_edges_undirected.shape[0]\n",
        "density = M_true / (N*(N-1)/2)\n",
        "print(f\"True undirected edges: {M_true} | density={density:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc-2o0zBJOQb"
      },
      "source": [
        "## 3 small layer GCN for node classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EdgeMLP(nn.Module):\n",
        "    \"\"\"Small MLP to combine node and edge features inside each GINE layer.\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "f1wN1axTH18d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "NVlIpN9wHxC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecce57b2-5137-4ad7-8e18-e94952c7ddf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ True, False, False,  ..., False,  True,  True])\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        ...,\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n"
          ]
        }
      ],
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden, out_channels, dropout=0.2,\n",
        "                 p_noise = 0.2, num_node_feats = 7, node_embed_dim = 7):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden)\n",
        "        self.conv2 = GCNConv(hidden, hidden)\n",
        "        self.conv3 = GCNConv(hidden, out_channels)\n",
        "        self.dropout = dropout\n",
        "        self.p_noise = p_noise\n",
        "\n",
        "    def add_noisy_edges(self, edge_index, num_nodes):\n",
        "        # Sample random edges\n",
        "        num_rand = int(self.p_noise * edge_index.size(1))\n",
        "\n",
        "        # uniformly pick random node pairs\n",
        "        row = torch.randint(0, num_nodes, (num_rand,))\n",
        "        col = torch.randint(0, num_nodes, (num_rand,))\n",
        "        noisy_edges = torch.stack([row, col], dim=0)\n",
        "\n",
        "        # concat & avoid duplicates (optional)\n",
        "        edge_index_noisy = torch.cat([edge_index, noisy_edges], dim=1)\n",
        "        return edge_index_noisy\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        if self.training:\n",
        "          edge_index = self.add_noisy_edges(edge_index, x.size(0))\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.sigmoid(x) #for binary cross entropy\n",
        "        return x  # logits (N, K)\n",
        "#binary classification, so 1 channel\n",
        "model = GCN(D, hidden=32, out_channels=1, dropout=0.1).to(device)\n",
        "X_dev = X.to(device)\n",
        "edge_index_dev = edge_index.to(device)\n",
        "y_dev = data.y.to(device)\n",
        "y_dev = torch.unsqueeze(y_dev, 1).float() #add dimension\n",
        "train_mask_dev = train_mask.to(device)\n",
        "val_mask_dev   = val_mask.to(device)\n",
        "test_mask_dev  = test_mask.to(device)\n",
        "print(train_mask_dev)\n",
        "print(y_dev[train_mask_dev])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNKKh96lJWCX"
      },
      "source": [
        "## train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "ZhFsvxwfIHPm"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_dev, y_dev, edge_index_dev,\n",
        "                train_mask_dev, val_mask_dev):\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    best_val, best_state = -1, None\n",
        "\n",
        "    for epoch in range(400):\n",
        "        model.train()\n",
        "        opt.zero_grad()\n",
        "        logits = model(X_dev, edge_index_dev)\n",
        "        loss = F.binary_cross_entropy(logits[train_mask_dev], y_dev[train_mask_dev])\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if (epoch + 1) % 50 == 0: #train loss\n",
        "          print(f\"Train loss for epoch {epoch + 1}: {loss:.3f}\")\n",
        "\n",
        "        # quick val acc\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_pred = torch.round(logits[val_mask_dev])\n",
        "            val_acc = (val_pred == y_dev[val_mask_dev]).float().mean().item()\n",
        "            if (epoch + 1) % 50 == 0:\n",
        "              print(f\"Val ACC for epoch {epoch + 1}: {val_acc:.3f}\")\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    print(f\"Best val acc: {best_val:.3f}\")\n",
        "    model.load_state_dict({k: v for k, v in best_state.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, X_dev, y_dev, edge_index_dev, train_mask_dev, val_mask_dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMt4dnQ8OakP",
        "outputId": "e2f14ac4-6949-46fa-84ab-341ed125983e"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss for epoch 50: 0.553\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.526\n",
            "Val ACC for epoch 100: 0.748\n",
            "Train loss for epoch 150: 0.528\n",
            "Val ACC for epoch 150: 0.751\n",
            "Train loss for epoch 200: 0.531\n",
            "Val ACC for epoch 200: 0.754\n",
            "Train loss for epoch 250: 0.531\n",
            "Val ACC for epoch 250: 0.737\n",
            "Train loss for epoch 300: 0.524\n",
            "Val ACC for epoch 300: 0.752\n",
            "Train loss for epoch 350: 0.518\n",
            "Val ACC for epoch 350: 0.743\n",
            "Train loss for epoch 400: 0.530\n",
            "Val ACC for epoch 400: 0.757\n",
            "Best val acc: 0.779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFOjI6WoJblz"
      },
      "source": [
        "## Black-box “API” wrapper (returns logits for chosen nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Bf0gPG0PJW2M"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def gbb_api(model, node_ids, X_query):\n",
        "    \"\"\"\n",
        "    node_ids: 1D LongTensor of node indices to fetch from output\n",
        "    X_query: (N, D) full feature matrix Bob provides (Alice uses it with her private edge_index)\n",
        "    returns: logits[node_ids] shape (len(node_ids), K)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    out = model(X_query.to(device), edge_index_dev)  # full-graph forward\n",
        "    return out[node_ids.to(device)].detach().cpu()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNcRvUAJJfYi"
      },
      "source": [
        "## LINKTELLER influence matrix & scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtGZYAx3Jc9J",
        "outputId": "bccd860c-d007-4eb2-b88f-4e6241a0d4b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5680135,\n",
              " [((2397, 2398), 0.40415525436401367),\n",
              "  ((2792, 2793), 0.3863096237182617),\n",
              "  ((2049, 2050), 0.3590106964111328),\n",
              "  ((1132, 1133), 0.3544062376022339),\n",
              "  ((2679, 2680), 0.3474622964859009)])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "def influence_matrix_for_v(model, v, V_I, X_base, delta=1e-2):\n",
        "    \"\"\"\n",
        "    v: node index (int)\n",
        "    V_I: 1D LongTensor of nodes-of-interest to score against\n",
        "    X_base: (N, D) baseline features\n",
        "    returns: Iv (|V_I|, K) = (P' - P)/delta where rows correspond to u in V_I\n",
        "    \"\"\"\n",
        "    node_ids = V_I\n",
        "    P = gbb_api(model, node_ids, X_base)\n",
        "\n",
        "    Xp = X_base.clone()\n",
        "    Xp[v] = (1.0 + delta) * Xp[v]  # upweight features of v\n",
        "    Pp = gbb_api(model, node_ids, Xp)\n",
        "\n",
        "    Iv = (Pp - P) / delta  # finite-diff approximation\n",
        "    return Iv  # (|V_I|, K)\n",
        "\n",
        "def linkteller_scores(model, V_C, X_base, delta=1e-2):\n",
        "    \"\"\"\n",
        "    V_C: nodes-of-interest (attack surface) as 1D LongTensor\n",
        "    returns: dict {(u,v): score} for u != v, unordered pairs\n",
        "    \"\"\"\n",
        "    V_C = V_C.cpu()\n",
        "    scores = {}\n",
        "    for j, v in enumerate(V_C.tolist()):\n",
        "        Iv = influence_matrix_for_v(model, v, V_C, X_base, delta=delta).numpy()  # rows aligned with V_C\n",
        "        # influence value of v on each u = ||Iv[u,:]||_2\n",
        "        norms = np.linalg.norm(Iv, axis=1)\n",
        "        for i, u in enumerate(V_C.tolist()):\n",
        "            if u == v:\n",
        "                continue\n",
        "            key = (min(u,v), max(u,v))\n",
        "            # symmetrical score: max of v→u and u→v will be handled later; accumulate max\n",
        "            scores[key] = max(scores.get(key, 0.0), float(norms[i]))\n",
        "    return scores\n",
        "\n",
        "# Choose attack node set V_C (we’ll use all nodes to make life easy)\n",
        "V_C = torch.arange(N, dtype=torch.long)\n",
        "scores = linkteller_scores(model, V_C, X, delta=1e-2)\n",
        "\n",
        "# Turn scores into a sorted list\n",
        "sorted_pairs = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n",
        "len(sorted_pairs), sorted_pairs[:5]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG6_8ujQJnFy"
      },
      "source": [
        "## Pick top-m pairs using a density belief k̂"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53EthqnAJjS5",
        "outputId": "c303df1d-730a-4fc9-91d9-561f13bfcb4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision=0.829 | Recall=0.829 | F1=0.829 | m_belief=3721 | true M=3721\n"
          ]
        }
      ],
      "source": [
        "n = N\n",
        "m_true = M_true\n",
        "m_belief = int(round(density * (n*(n-1)/2)))\n",
        "\n",
        "pred_edges = set([pair for (pair, _) in sorted_pairs[:m_belief]])\n",
        "\n",
        "# ground truth undirected edges as set of tuples (i,j) with i<j\n",
        "true_edges = set([tuple(e.tolist()) for e in true_edges_undirected])\n",
        "\n",
        "tp = len(pred_edges & true_edges)\n",
        "fp = len(pred_edges - true_edges)\n",
        "fn = len(true_edges - pred_edges)\n",
        "\n",
        "precision = tp / (tp + fp + 1e-12)\n",
        "recall    = tp / (tp + fn + 1e-12)\n",
        "f1        = 2*precision*recall / (precision + recall + 1e-12)\n",
        "print(f\"Precision={precision:.3f} | Recall={recall:.3f} | F1={f1:.3f} | m_belief={m_belief} | true M={m_true}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klEK63GlJrMi"
      },
      "source": [
        "##Sweep density belief k̂ to see sensitivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi6o_tpcJsPB",
        "outputId": "1f8951f9-6092-412d-c477-2376f1e02ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k_hat=0.0003  m=1861  P=0.896 R=0.448 F1=0.598\n",
            "k_hat=0.0005  m=2977  P=0.867 R=0.694 F1=0.771\n",
            "k_hat=0.0007  m=3721  P=0.808 R=0.808 F1=0.808\n",
            "k_hat=0.0008  m=4465  P=0.736 R=0.883 F1=0.803\n",
            "k_hat=0.0010  m=5582  P=0.613 R=0.919 F1=0.735\n"
          ]
        }
      ],
      "source": [
        "def evaluate_at_fraction(frac):\n",
        "    m = int(round(frac * (n*(n-1)/2)))\n",
        "    pred = set([pair for (pair, _) in sorted_pairs[:m]])\n",
        "    tp = len(pred & true_edges)\n",
        "    fp = len(pred - true_edges)\n",
        "    fn = len(true_edges - pred)\n",
        "    p = tp / (tp + fp + 1e-12)\n",
        "    r = tp / (tp + fn + 1e-12)\n",
        "    f1 = 2*p*r / (p + r + 1e-12)\n",
        "    return p, r, f1, m\n",
        "\n",
        "for frac in [0.5*density, 0.8*density, density, 1.2*density, 1.5*density]:\n",
        "    p, r, f1, m = evaluate_at_fraction(frac)\n",
        "    print(f\"k_hat={frac:.4f}  m={m:3d}  P={p:.3f} R={r:.3f} F1={f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate AUROC"
      ],
      "metadata": {
        "id": "K37HXLpd27ww"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ktWFxwZSMe9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "def calculate_auc(scores: dict[tuple:float], true_edges: set[tuple]):\n",
        "  #add set of false edges (every edge in score not in true_edges)\n",
        "  false_edges = set(scores.keys()).difference(true_edges)\n",
        "  #create prediction, score vectors in correct order\n",
        "  y_true = [1] * len(true_edges) + [0] * len(false_edges)\n",
        "  y_score = [scores[e] for e in true_edges] + [scores[e] for e in false_edges]\n",
        "  return roc_auc_score(y_true, y_score)\n",
        "\n",
        "\n",
        "auc = calculate_auc(scores, true_edges)\n",
        "print(\"AUROC:\", auc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "5MOqL8fM266u",
        "outputId": "c561ea54-8841-434b-82dc-e4e064242791"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-75987240.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUROC:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-75987240.py\u001b[0m in \u001b[0;36mcalculate_auc\u001b[0;34m(scores, true_edges)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#create prediction, score vectors in correct order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_edges\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfalse_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrue_edges\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfalse_edges\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout + Dummy Edge Test"
      ],
      "metadata": {
        "id": "rOWtMRuA0vqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dropOutDummyEdgeTest(X_dev, y_dev, edge_index_dev,\n",
        "                train_mask_dev, val_mask_dev, dropoutProb, p_noise):\n",
        "    \"\"\"\n",
        "    Testing to get model attack success for different dropout/dummy edge\n",
        "    incorporation rates\n",
        "    \"\"\"\n",
        "    model = GCN(D, hidden=64, out_channels=1, dropout=dropoutProb, p_noise = p_noise).to(device)\n",
        "    train_model(model, X_dev, y_dev, edge_index_dev, train_mask_dev, val_mask_dev)\n",
        "    # Choose attack node set V_C (we’ll use all nodes to make life easy)\n",
        "    V_C = torch.arange(N, dtype=torch.long)\n",
        "    scores = linkteller_scores(model, V_C, X, delta=1e-2)\n",
        "\n",
        "    # Turn scores into a sorted list\n",
        "    sorted_pairs = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n",
        "    len(sorted_pairs), sorted_pairs[:5]\n",
        "\n",
        "    n = N\n",
        "    m_true = M_true\n",
        "    m_belief = int(round(density * (n*(n-1)/2)))\n",
        "\n",
        "    pred_edges = set([pair for (pair, _) in sorted_pairs[:m_belief]])\n",
        "\n",
        "    # ground truth undirected edges as set of tuples (i,j) with i<j\n",
        "    true_edges = set([tuple(e.tolist()) for e in true_edges_undirected])\n",
        "\n",
        "    tp = len(pred_edges & true_edges)\n",
        "    fp = len(pred_edges - true_edges)\n",
        "    fn = len(true_edges - pred_edges)\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-12)\n",
        "    recall    = tp / (tp + fn + 1e-12)\n",
        "    f1        = 2*precision*recall / (precision + recall + 1e-12)\n",
        "    print(f\"Precision={precision:.3f} | Recall={recall:.3f} | F1={f1:.3f} | m_belief={m_belief} | true M={m_true}\")"
      ],
      "metadata": {
        "id": "wTN-j0mCPODY"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(f\"Dropout Rate: {i/10}\")\n",
        "  dropOutDummyEdgeTest(X_dev, y_dev, edge_index_dev,\n",
        "                  train_mask_dev, val_mask_dev, i/10, 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gIbApddU9ls",
        "outputId": "24c67e25-adf9-4d3c-a6db-1adab910be58"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout Rate: 0.0\n",
            "Train loss for epoch 50: 0.509\n",
            "Val ACC for epoch 50: 0.766\n",
            "Train loss for epoch 100: 0.505\n",
            "Val ACC for epoch 100: 0.773\n",
            "Train loss for epoch 150: 0.503\n",
            "Val ACC for epoch 150: 0.773\n",
            "Train loss for epoch 200: 0.501\n",
            "Val ACC for epoch 200: 0.779\n",
            "Train loss for epoch 250: 0.500\n",
            "Val ACC for epoch 250: 0.782\n",
            "Train loss for epoch 300: 0.499\n",
            "Val ACC for epoch 300: 0.782\n",
            "Train loss for epoch 350: 0.498\n",
            "Val ACC for epoch 350: 0.782\n",
            "Train loss for epoch 400: 0.496\n",
            "Val ACC for epoch 400: 0.785\n",
            "Best val acc: 0.785\n",
            "Precision=0.732 | Recall=0.732 | F1=0.732 | m_belief=3721 | true M=3721\n",
            "Dropout Rate: 0.1\n",
            "Train loss for epoch 50: 0.514\n",
            "Val ACC for epoch 50: 0.764\n",
            "Train loss for epoch 100: 0.508\n",
            "Val ACC for epoch 100: 0.769\n",
            "Train loss for epoch 150: 0.502\n",
            "Val ACC for epoch 150: 0.769\n",
            "Train loss for epoch 200: 0.505\n",
            "Val ACC for epoch 200: 0.776\n",
            "Train loss for epoch 250: 0.504\n",
            "Val ACC for epoch 250: 0.773\n",
            "Train loss for epoch 300: 0.503\n",
            "Val ACC for epoch 300: 0.773\n",
            "Train loss for epoch 350: 0.506\n",
            "Val ACC for epoch 350: 0.774\n",
            "Train loss for epoch 400: 0.498\n",
            "Val ACC for epoch 400: 0.776\n",
            "Best val acc: 0.785\n",
            "Precision=0.778 | Recall=0.778 | F1=0.778 | m_belief=3721 | true M=3721\n",
            "Dropout Rate: 0.2\n",
            "Train loss for epoch 50: 0.515\n",
            "Val ACC for epoch 50: 0.755\n",
            "Train loss for epoch 100: 0.507\n",
            "Val ACC for epoch 100: 0.763\n",
            "Train loss for epoch 150: 0.506\n",
            "Val ACC for epoch 150: 0.770\n",
            "Train loss for epoch 200: 0.503\n",
            "Val ACC for epoch 200: 0.770\n",
            "Train loss for epoch 250: 0.504\n",
            "Val ACC for epoch 250: 0.772\n",
            "Train loss for epoch 300: 0.504\n",
            "Val ACC for epoch 300: 0.776\n",
            "Train loss for epoch 350: 0.504\n",
            "Val ACC for epoch 350: 0.780\n",
            "Train loss for epoch 400: 0.506\n",
            "Val ACC for epoch 400: 0.770\n",
            "Best val acc: 0.783\n",
            "Precision=0.822 | Recall=0.822 | F1=0.822 | m_belief=3721 | true M=3721\n",
            "Dropout Rate: 0.3\n",
            "Train loss for epoch 50: 0.518\n",
            "Val ACC for epoch 50: 0.760\n",
            "Train loss for epoch 100: 0.504\n",
            "Val ACC for epoch 100: 0.769\n",
            "Train loss for epoch 150: 0.510\n",
            "Val ACC for epoch 150: 0.776\n",
            "Train loss for epoch 200: 0.503\n",
            "Val ACC for epoch 200: 0.769\n",
            "Train loss for epoch 250: 0.506\n",
            "Val ACC for epoch 250: 0.777\n",
            "Train loss for epoch 300: 0.503\n",
            "Val ACC for epoch 300: 0.769\n",
            "Train loss for epoch 350: 0.503\n",
            "Val ACC for epoch 350: 0.770\n",
            "Train loss for epoch 400: 0.509\n",
            "Val ACC for epoch 400: 0.766\n",
            "Best val acc: 0.785\n",
            "Precision=0.819 | Recall=0.819 | F1=0.819 | m_belief=3721 | true M=3721\n",
            "Dropout Rate: 0.4\n",
            "Train loss for epoch 50: 0.519\n",
            "Val ACC for epoch 50: 0.745\n",
            "Train loss for epoch 100: 0.510\n",
            "Val ACC for epoch 100: 0.774\n",
            "Train loss for epoch 150: 0.513\n",
            "Val ACC for epoch 150: 0.761\n",
            "Train loss for epoch 200: 0.515\n",
            "Val ACC for epoch 200: 0.773\n",
            "Train loss for epoch 250: 0.514\n",
            "Val ACC for epoch 250: 0.773\n",
            "Train loss for epoch 300: 0.506\n",
            "Val ACC for epoch 300: 0.772\n",
            "Train loss for epoch 350: 0.509\n",
            "Val ACC for epoch 350: 0.776\n",
            "Train loss for epoch 400: 0.509\n",
            "Val ACC for epoch 400: 0.774\n",
            "Best val acc: 0.782\n",
            "Precision=0.818 | Recall=0.818 | F1=0.818 | m_belief=3721 | true M=3721\n",
            "Dropout Rate: 0.5\n",
            "Train loss for epoch 50: 0.520\n",
            "Val ACC for epoch 50: 0.751\n",
            "Train loss for epoch 100: 0.515\n",
            "Val ACC for epoch 100: 0.769\n",
            "Train loss for epoch 150: 0.511\n",
            "Val ACC for epoch 150: 0.767\n",
            "Train loss for epoch 200: 0.509\n",
            "Val ACC for epoch 200: 0.764\n",
            "Train loss for epoch 250: 0.519\n",
            "Val ACC for epoch 250: 0.763\n",
            "Train loss for epoch 300: 0.510\n",
            "Val ACC for epoch 300: 0.770\n",
            "Train loss for epoch 350: 0.511\n",
            "Val ACC for epoch 350: 0.774\n",
            "Train loss for epoch 400: 0.508\n",
            "Val ACC for epoch 400: 0.763\n",
            "Best val acc: 0.776\n",
            "Precision=0.813 | Recall=0.813 | F1=0.813 | m_belief=3721 | true M=3721\n",
            "Dropout Rate: 0.6\n",
            "Train loss for epoch 50: 0.527\n",
            "Val ACC for epoch 50: 0.724\n",
            "Train loss for epoch 100: 0.525\n",
            "Val ACC for epoch 100: 0.754\n",
            "Train loss for epoch 150: 0.518\n",
            "Val ACC for epoch 150: 0.755\n",
            "Train loss for epoch 200: 0.513\n",
            "Val ACC for epoch 200: 0.752\n",
            "Train loss for epoch 250: 0.513\n",
            "Val ACC for epoch 250: 0.757\n",
            "Train loss for epoch 300: 0.512\n",
            "Val ACC for epoch 300: 0.755\n",
            "Train loss for epoch 350: 0.513\n",
            "Val ACC for epoch 350: 0.754\n",
            "Train loss for epoch 400: 0.521\n",
            "Val ACC for epoch 400: 0.760\n",
            "Best val acc: 0.772\n",
            "Precision=0.810 | Recall=0.810 | F1=0.810 | m_belief=3721 | true M=3721\n",
            "Dropout Rate: 0.7\n",
            "Train loss for epoch 50: 0.523\n",
            "Val ACC for epoch 50: 0.734\n",
            "Train loss for epoch 100: 0.525\n",
            "Val ACC for epoch 100: 0.751\n",
            "Train loss for epoch 150: 0.520\n",
            "Val ACC for epoch 150: 0.751\n",
            "Train loss for epoch 200: 0.526\n",
            "Val ACC for epoch 200: 0.755\n",
            "Train loss for epoch 250: 0.517\n",
            "Val ACC for epoch 250: 0.757\n",
            "Train loss for epoch 300: 0.515\n",
            "Val ACC for epoch 300: 0.749\n",
            "Train loss for epoch 350: 0.522\n",
            "Val ACC for epoch 350: 0.749\n",
            "Train loss for epoch 400: 0.524\n",
            "Val ACC for epoch 400: 0.761\n",
            "Best val acc: 0.772\n",
            "Precision=0.771 | Recall=0.771 | F1=0.771 | m_belief=3721 | true M=3721\n",
            "Dropout Rate: 0.8\n",
            "Train loss for epoch 50: 0.543\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.533\n",
            "Val ACC for epoch 100: 0.723\n",
            "Train loss for epoch 150: 0.530\n",
            "Val ACC for epoch 150: 0.723\n",
            "Train loss for epoch 200: 0.527\n",
            "Val ACC for epoch 200: 0.742\n",
            "Train loss for epoch 250: 0.530\n",
            "Val ACC for epoch 250: 0.736\n",
            "Train loss for epoch 300: 0.527\n",
            "Val ACC for epoch 300: 0.754\n",
            "Train loss for epoch 350: 0.528\n",
            "Val ACC for epoch 350: 0.746\n",
            "Train loss for epoch 400: 0.531\n",
            "Val ACC for epoch 400: 0.743\n",
            "Best val acc: 0.760\n",
            "Precision=0.738 | Recall=0.738 | F1=0.738 | m_belief=3721 | true M=3721\n",
            "Dropout Rate: 0.9\n",
            "Train loss for epoch 50: 0.564\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.553\n",
            "Val ACC for epoch 100: 0.721\n",
            "Train loss for epoch 150: 0.554\n",
            "Val ACC for epoch 150: 0.721\n",
            "Train loss for epoch 200: 0.550\n",
            "Val ACC for epoch 200: 0.715\n",
            "Train loss for epoch 250: 0.539\n",
            "Val ACC for epoch 250: 0.727\n",
            "Train loss for epoch 300: 0.555\n",
            "Val ACC for epoch 300: 0.730\n",
            "Train loss for epoch 350: 0.538\n",
            "Val ACC for epoch 350: 0.739\n",
            "Train loss for epoch 400: 0.547\n",
            "Val ACC for epoch 400: 0.746\n",
            "Best val acc: 0.748\n",
            "Precision=0.803 | Recall=0.803 | F1=0.803 | m_belief=3721 | true M=3721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(11):\n",
        "  print(f\"Noisy Edge Prob.: {i/10}\")\n",
        "  dropOutDummyEdgeTest(X_dev, y_dev, edge_index_dev,\n",
        "                  train_mask_dev, val_mask_dev, 0.5, i / 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sAgXDk5NXPE",
        "outputId": "fcf9245d-a785-4c54-83f5-3d05f7ba055a"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noisy Edge Prob.: 0.0\n",
            "Train loss for epoch 50: 0.520\n",
            "Val ACC for epoch 50: 0.745\n",
            "Train loss for epoch 100: 0.507\n",
            "Val ACC for epoch 100: 0.758\n",
            "Train loss for epoch 150: 0.515\n",
            "Val ACC for epoch 150: 0.757\n",
            "Train loss for epoch 200: 0.514\n",
            "Val ACC for epoch 200: 0.758\n",
            "Train loss for epoch 250: 0.510\n",
            "Val ACC for epoch 250: 0.761\n",
            "Train loss for epoch 300: 0.513\n",
            "Val ACC for epoch 300: 0.752\n",
            "Train loss for epoch 350: 0.513\n",
            "Val ACC for epoch 350: 0.763\n",
            "Train loss for epoch 400: 0.510\n",
            "Val ACC for epoch 400: 0.770\n",
            "Best val acc: 0.774\n",
            "Precision=0.794 | Recall=0.794 | F1=0.794 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 0.1\n",
            "Train loss for epoch 50: 0.534\n",
            "Val ACC for epoch 50: 0.724\n",
            "Train loss for epoch 100: 0.530\n",
            "Val ACC for epoch 100: 0.754\n",
            "Train loss for epoch 150: 0.533\n",
            "Val ACC for epoch 150: 0.749\n",
            "Train loss for epoch 200: 0.524\n",
            "Val ACC for epoch 200: 0.748\n",
            "Train loss for epoch 250: 0.522\n",
            "Val ACC for epoch 250: 0.748\n",
            "Train loss for epoch 300: 0.530\n",
            "Val ACC for epoch 300: 0.752\n",
            "Train loss for epoch 350: 0.514\n",
            "Val ACC for epoch 350: 0.751\n",
            "Train loss for epoch 400: 0.522\n",
            "Val ACC for epoch 400: 0.752\n",
            "Best val acc: 0.770\n",
            "Precision=0.840 | Recall=0.840 | F1=0.840 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 0.2\n",
            "Train loss for epoch 50: 0.543\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.529\n",
            "Val ACC for epoch 100: 0.736\n",
            "Train loss for epoch 150: 0.534\n",
            "Val ACC for epoch 150: 0.737\n",
            "Train loss for epoch 200: 0.526\n",
            "Val ACC for epoch 200: 0.745\n",
            "Train loss for epoch 250: 0.529\n",
            "Val ACC for epoch 250: 0.757\n",
            "Train loss for epoch 300: 0.531\n",
            "Val ACC for epoch 300: 0.739\n",
            "Train loss for epoch 350: 0.529\n",
            "Val ACC for epoch 350: 0.743\n",
            "Train loss for epoch 400: 0.535\n",
            "Val ACC for epoch 400: 0.743\n",
            "Best val acc: 0.763\n",
            "Precision=0.834 | Recall=0.834 | F1=0.834 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 0.3\n",
            "Train loss for epoch 50: 0.550\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.538\n",
            "Val ACC for epoch 100: 0.736\n",
            "Train loss for epoch 150: 0.537\n",
            "Val ACC for epoch 150: 0.736\n",
            "Train loss for epoch 200: 0.540\n",
            "Val ACC for epoch 200: 0.726\n",
            "Train loss for epoch 250: 0.539\n",
            "Val ACC for epoch 250: 0.736\n",
            "Train loss for epoch 300: 0.549\n",
            "Val ACC for epoch 300: 0.737\n",
            "Train loss for epoch 350: 0.539\n",
            "Val ACC for epoch 350: 0.737\n",
            "Train loss for epoch 400: 0.534\n",
            "Val ACC for epoch 400: 0.736\n",
            "Best val acc: 0.755\n",
            "Precision=0.868 | Recall=0.868 | F1=0.868 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 0.4\n",
            "Train loss for epoch 50: 0.560\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.539\n",
            "Val ACC for epoch 100: 0.724\n",
            "Train loss for epoch 150: 0.542\n",
            "Val ACC for epoch 150: 0.734\n",
            "Train loss for epoch 200: 0.532\n",
            "Val ACC for epoch 200: 0.734\n",
            "Train loss for epoch 250: 0.540\n",
            "Val ACC for epoch 250: 0.728\n",
            "Train loss for epoch 300: 0.532\n",
            "Val ACC for epoch 300: 0.737\n",
            "Train loss for epoch 350: 0.543\n",
            "Val ACC for epoch 350: 0.730\n",
            "Train loss for epoch 400: 0.540\n",
            "Val ACC for epoch 400: 0.734\n",
            "Best val acc: 0.751\n",
            "Precision=0.834 | Recall=0.834 | F1=0.834 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 0.5\n",
            "Train loss for epoch 50: 0.558\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.548\n",
            "Val ACC for epoch 100: 0.723\n",
            "Train loss for epoch 150: 0.552\n",
            "Val ACC for epoch 150: 0.734\n",
            "Train loss for epoch 200: 0.550\n",
            "Val ACC for epoch 200: 0.733\n",
            "Train loss for epoch 250: 0.545\n",
            "Val ACC for epoch 250: 0.731\n",
            "Train loss for epoch 300: 0.555\n",
            "Val ACC for epoch 300: 0.727\n",
            "Train loss for epoch 350: 0.549\n",
            "Val ACC for epoch 350: 0.736\n",
            "Train loss for epoch 400: 0.545\n",
            "Val ACC for epoch 400: 0.727\n",
            "Best val acc: 0.755\n",
            "Precision=0.827 | Recall=0.827 | F1=0.827 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 0.6\n",
            "Train loss for epoch 50: 0.556\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.554\n",
            "Val ACC for epoch 100: 0.723\n",
            "Train loss for epoch 150: 0.557\n",
            "Val ACC for epoch 150: 0.721\n",
            "Train loss for epoch 200: 0.556\n",
            "Val ACC for epoch 200: 0.726\n",
            "Train loss for epoch 250: 0.553\n",
            "Val ACC for epoch 250: 0.720\n",
            "Train loss for epoch 300: 0.556\n",
            "Val ACC for epoch 300: 0.727\n",
            "Train loss for epoch 350: 0.544\n",
            "Val ACC for epoch 350: 0.731\n",
            "Train loss for epoch 400: 0.551\n",
            "Val ACC for epoch 400: 0.742\n",
            "Best val acc: 0.743\n",
            "Precision=0.841 | Recall=0.841 | F1=0.841 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 0.7\n",
            "Train loss for epoch 50: 0.561\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.548\n",
            "Val ACC for epoch 100: 0.723\n",
            "Train loss for epoch 150: 0.562\n",
            "Val ACC for epoch 150: 0.723\n",
            "Train loss for epoch 200: 0.549\n",
            "Val ACC for epoch 200: 0.720\n",
            "Train loss for epoch 250: 0.543\n",
            "Val ACC for epoch 250: 0.730\n",
            "Train loss for epoch 300: 0.554\n",
            "Val ACC for epoch 300: 0.728\n",
            "Train loss for epoch 350: 0.558\n",
            "Val ACC for epoch 350: 0.734\n",
            "Train loss for epoch 400: 0.560\n",
            "Val ACC for epoch 400: 0.730\n",
            "Best val acc: 0.740\n",
            "Precision=0.793 | Recall=0.793 | F1=0.793 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 0.8\n",
            "Train loss for epoch 50: 0.566\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.558\n",
            "Val ACC for epoch 100: 0.723\n",
            "Train loss for epoch 150: 0.554\n",
            "Val ACC for epoch 150: 0.727\n",
            "Train loss for epoch 200: 0.559\n",
            "Val ACC for epoch 200: 0.714\n",
            "Train loss for epoch 250: 0.551\n",
            "Val ACC for epoch 250: 0.717\n",
            "Train loss for epoch 300: 0.555\n",
            "Val ACC for epoch 300: 0.724\n",
            "Train loss for epoch 350: 0.551\n",
            "Val ACC for epoch 350: 0.733\n",
            "Train loss for epoch 400: 0.555\n",
            "Val ACC for epoch 400: 0.727\n",
            "Best val acc: 0.745\n",
            "Precision=0.838 | Recall=0.838 | F1=0.838 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 0.9\n",
            "Train loss for epoch 50: 0.571\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.559\n",
            "Val ACC for epoch 100: 0.723\n",
            "Train loss for epoch 150: 0.560\n",
            "Val ACC for epoch 150: 0.721\n",
            "Train loss for epoch 200: 0.556\n",
            "Val ACC for epoch 200: 0.724\n",
            "Train loss for epoch 250: 0.558\n",
            "Val ACC for epoch 250: 0.727\n",
            "Train loss for epoch 300: 0.560\n",
            "Val ACC for epoch 300: 0.727\n",
            "Train loss for epoch 350: 0.559\n",
            "Val ACC for epoch 350: 0.723\n",
            "Train loss for epoch 400: 0.554\n",
            "Val ACC for epoch 400: 0.726\n",
            "Best val acc: 0.736\n",
            "Precision=0.851 | Recall=0.851 | F1=0.851 | m_belief=3721 | true M=3721\n",
            "Noisy Edge Prob.: 1.0\n",
            "Train loss for epoch 50: 0.572\n",
            "Val ACC for epoch 50: 0.723\n",
            "Train loss for epoch 100: 0.558\n",
            "Val ACC for epoch 100: 0.723\n",
            "Train loss for epoch 150: 0.568\n",
            "Val ACC for epoch 150: 0.726\n",
            "Train loss for epoch 200: 0.560\n",
            "Val ACC for epoch 200: 0.726\n",
            "Train loss for epoch 250: 0.557\n",
            "Val ACC for epoch 250: 0.726\n",
            "Train loss for epoch 300: 0.559\n",
            "Val ACC for epoch 300: 0.726\n",
            "Train loss for epoch 350: 0.559\n",
            "Val ACC for epoch 350: 0.723\n",
            "Train loss for epoch 400: 0.553\n",
            "Val ACC for epoch 400: 0.724\n",
            "Best val acc: 0.733\n",
            "Precision=0.851 | Recall=0.851 | F1=0.851 | m_belief=3721 | true M=3721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(D, hidden=32, out_channels=1, dropout=1).to(device)\n",
        "y_pred = model(X_dev, edge_index_dev)\n",
        "print(torch.sum(y_pred[val_mask_dev] <= 0.5)/y_pred[val_mask_dev].size(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi7PsnWVaSYs",
        "outputId": "5df3bbf4-0397-4bed-d38a-6861fce798a4"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}