{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1877fbf6",
   "metadata": {},
   "source": [
    "# Node-level membership inference attack (MIA) adapted for graph level predictions\n",
    "## Keenan Hom\n",
    "The original choice for MIA is here: https://arxiv.org/pdf/2102.05429. This notebook adapts it to be used for graph level predictions, suitable for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67d0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298bc499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# # Add the project root (the folder containing \"main\") to sys.path\n",
    "# sys.path.append('..')\n",
    "# from membership_inference_attack.ml_util import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94b7769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch_geometric import nn as gnn, transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import accuracy_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from ml_util import (\n",
    "    CustomGATModel, GenericAttackModel, train_model_multi_graph, train_model, \n",
    "    load_model, get_accuracy, get_auroc_score, predict, predict_multi_graph\n",
    ")\n",
    "from util import create_attack_dataset, graph_train_test_split, onehot_transform\n",
    "from train_models import get_dataset, shadow_target_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2423ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ogbg-molhiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2378d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = ('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1286f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(dataset_name)\n",
    "num_feat = dataset[0].x.shape[1]\n",
    "num_categories = dataset[0].y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa907a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset_train, t_dataset_test, s_dataset_train, s_dataset_test = shadow_target_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f756c112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomGATModel(\n",
       "  (embedding): Linear(in_features=9, out_features=144, bias=True)\n",
       "  (gat_layers): ModuleList(\n",
       "    (0-2): 3 x GATConv(144, 18, heads=8)\n",
       "    (3): GATConv(144, 144, heads=1)\n",
       "  )\n",
       "  (bns): ModuleList(\n",
       "    (0-3): 4 x BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (linear): MLP(144, 72, 36, 2)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr = 0.001\n",
    "# epochs = 100\n",
    "# batch_size = 8\n",
    "# weight_decay = 1e-4\n",
    "t_model = CustomGATModel(num_feat=num_feat, num_classes=num_categories).to(DEVICE)\n",
    "# optimizer = optim.Adam(t_model.parameters(), lr=lr)\n",
    "# t_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "#                                                        factor=0.5,\n",
    "#                                                        patience=50,\n",
    "#                                                        min_lr=1e-6,\n",
    "#                                                        verbose=True)\n",
    "\n",
    "# weight = compute_class_weight('balanced', classes=np.unique(t_dataset_train.y.argmax(dim=1)), y=t_dataset_train.y.argmax(dim=1).numpy())\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weight).to(DEVICE))\n",
    "t_save_path = 'mia-models/t_model_gat_ogbg-molhiv.pth'\n",
    "# t_save_path = None\n",
    "\n",
    "# train_model_multi_graph(t_model, optimizer, t_dataset_train, loss_fn, epochs, batch_size, val_dataset=t_dataset_test, \n",
    "#                         save_path=t_save_path, save_freq=10, scheduler=t_scheduler, device=DEVICE)\n",
    "t_model, t_dataset_train, t_dataset_test = load_model(t_model, t_save_path)\n",
    "t_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6e5310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomGATModel(\n",
       "  (embedding): Linear(in_features=9, out_features=144, bias=True)\n",
       "  (gat_layers): ModuleList(\n",
       "    (0-2): 3 x GATConv(144, 18, heads=8)\n",
       "    (3): GATConv(144, 144, heads=1)\n",
       "  )\n",
       "  (bns): ModuleList(\n",
       "    (0-3): 4 x BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (linear): MLP(144, 72, 36, 2)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train shadow model\n",
    "# lr = 0.001\n",
    "# epochs = 100\n",
    "# batch_size = 8\n",
    "# weight_decay = 1e-4\n",
    "s_model = CustomGATModel(num_feat=num_feat, num_classes=num_categories).to(DEVICE)\n",
    "# optimizer = optim.Adam(s_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# s_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "#                                                        factor=0.5,\n",
    "#                                                        patience=50,\n",
    "#                                                        min_lr=1e-6,\n",
    "#                                                        verbose=True)\n",
    "# weight = compute_class_weight('balanced', classes=np.unique(s_dataset_train.y.argmax(dim=1)), y=s_dataset_train.y.argmax(dim=1).numpy())\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weight).to(DEVICE))\n",
    "s_save_path = 'mia-models/s_model_gat_ogbg-molhiv.pth'\n",
    "# s_save_path = None\n",
    "\n",
    "# train_model_multi_graph(s_model, optimizer, s_dataset_train, loss_fn, epochs, batch_size, val_dataset=s_dataset_test, save_path=s_save_path, save_freq=10, device=DEVICE)\n",
    "s_model, s_dataset_train, s_dataset_test = load_model(s_model, s_save_path)\n",
    "s_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "685d1cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860140323609299"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = predict_multi_graph(s_model, s_dataset_train, device=DEVICE, logits=True, return_type='pt')\n",
    "targets = torch.cat([g.y for g in s_dataset_train])\n",
    "# get_accuracy(logits, targets)\n",
    "get_auroc_score(logits, targets, multiclass=(num_categories>2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd272128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676190295615655"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = predict_multi_graph(s_model, s_dataset_test, device=DEVICE, logits=True, return_type='pt')\n",
    "targets = torch.cat([g.y for g in s_dataset_test])\n",
    "# get_accuracy(logits, targets)\n",
    "get_auroc_score(logits, targets, multiclass=(num_categories>2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b03001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "No learning rate scheduling!\n",
      "Training for 50 epochs, with batch size=16\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/50-----\n",
      "Batch 50/1286 | loss: 0.6844947509783652 (0.124s) | train acc: 0.5363 | train auc: 0.547243\n",
      "Batch 100/1286 | loss: 0.688839272769275 (0.063s) | train acc: 0.5600 | train auc: 0.548061\n",
      "Batch 150/1286 | loss: 0.6939873649318651 (0.064s) | train acc: 0.5429 | train auc: 0.532302\n",
      "Batch 200/1286 | loss: 0.7099263077524466 (0.064s) | train acc: 0.5297 | train auc: 0.526566\n",
      "Batch 250/1286 | loss: 0.7034724149711938 (0.064s) | train acc: 0.5078 | train auc: 0.519976\n",
      "Batch 300/1286 | loss: 0.6995981910919029 (0.064s) | train acc: 0.4967 | train auc: 0.513419\n",
      "Batch 350/1286 | loss: 0.6980951836745811 (0.064s) | train acc: 0.4984 | train auc: 0.511492\n",
      "Batch 400/1286 | loss: 0.6888716328825586 (0.064s) | train acc: 0.5030 | train auc: 0.512050\n",
      "Batch 450/1286 | loss: 0.6966380457550782 (0.063s) | train acc: 0.5054 | train auc: 0.512636\n",
      "Batch 500/1286 | loss: 0.6966809822675476 (0.063s) | train acc: 0.5024 | train auc: 0.510271\n",
      "Batch 550/1286 | loss: 0.7032535342264109 (0.064s) | train acc: 0.4983 | train auc: 0.508481\n",
      "Batch 600/1286 | loss: 0.6953190509348655 (0.063s) | train acc: 0.4970 | train auc: 0.507124\n",
      "Batch 650/1286 | loss: 0.6902357668126377 (0.063s) | train acc: 0.4989 | train auc: 0.507440\n",
      "Batch 700/1286 | loss: 0.6847834017843766 (0.063s) | train acc: 0.5048 | train auc: 0.509401\n",
      "Batch 750/1286 | loss: 0.7010219031140659 (0.063s) | train acc: 0.5039 | train auc: 0.507085\n",
      "Batch 800/1286 | loss: 0.6979999849116219 (0.063s) | train acc: 0.5048 | train auc: 0.505735\n",
      "Batch 850/1286 | loss: 0.7009175672490681 (0.063s) | train acc: 0.5020 | train auc: 0.505358\n",
      "Batch 900/1286 | loss: 0.6966578752284943 (0.063s) | train acc: 0.4990 | train auc: 0.504287\n",
      "Batch 950/1286 | loss: 0.687851893182833 (0.064s) | train acc: 0.5018 | train auc: 0.505554\n",
      "Batch 1000/1286 | loss: 0.6969692721083763 (0.063s) | train acc: 0.5030 | train auc: 0.505915\n",
      "Batch 1050/1286 | loss: 0.6961844976660788 (0.064s) | train acc: 0.5021 | train auc: 0.505100\n",
      "Batch 1100/1286 | loss: 0.7005214640758023 (0.063s) | train acc: 0.5002 | train auc: 0.504732\n",
      "Batch 1150/1286 | loss: 0.6957252375593785 (0.063s) | train acc: 0.5002 | train auc: 0.505680\n",
      "Batch 1200/1286 | loss: 0.7025831043012276 (0.063s) | train acc: 0.5001 | train auc: 0.506613\n",
      "Batch 1250/1286 | loss: 0.6946690329180819 (0.063s) | train acc: 0.4993 | train auc: 0.506486\n",
      "Batch 1286/1286 | loss: 0.6829863598469106 (0.045s) | train acc: 0.5000 | train auc: 0.506818\n",
      "Validation: val loss: 0.697 | val acc: 0.571 | val F1: 0.702 | val AUC: 0.507\n",
      "\n",
      "-----Epoch 2/50-----\n",
      "Batch 50/1286 | loss: 0.6905816426261804 (0.062s) | train acc: 0.5938 | train auc: 0.497992\n",
      "Batch 100/1286 | loss: 0.6947077436276844 (0.062s) | train acc: 0.5650 | train auc: 0.499576\n",
      "Batch 150/1286 | loss: 0.6942256108331649 (0.062s) | train acc: 0.5450 | train auc: 0.492595\n",
      "Batch 200/1286 | loss: 0.6906701748932241 (0.063s) | train acc: 0.5516 | train auc: 0.488944\n",
      "Batch 250/1286 | loss: 0.6992149662068522 (0.063s) | train acc: 0.5505 | train auc: 0.490658\n",
      "Batch 300/1286 | loss: 0.6960815671355918 (0.063s) | train acc: 0.5406 | train auc: 0.492963\n",
      "Batch 350/1286 | loss: 0.6924143371478917 (0.063s) | train acc: 0.5375 | train auc: 0.492747\n",
      "Batch 400/1286 | loss: 0.6966839616854859 (0.063s) | train acc: 0.5348 | train auc: 0.491994\n",
      "Batch 450/1286 | loss: 0.6905350935776946 (0.065s) | train acc: 0.5339 | train auc: 0.494503\n",
      "Batch 500/1286 | loss: 0.6932225666918268 (0.064s) | train acc: 0.5345 | train auc: 0.496614\n",
      "Batch 550/1286 | loss: 0.6835620712768815 (0.064s) | train acc: 0.5336 | train auc: 0.495516\n",
      "Batch 600/1286 | loss: 0.6920226370967115 (0.063s) | train acc: 0.5374 | train auc: 0.494854\n",
      "Batch 650/1286 | loss: 0.6977858971445211 (0.063s) | train acc: 0.5372 | train auc: 0.495168\n",
      "Batch 700/1286 | loss: 0.6852067380596073 (0.063s) | train acc: 0.5352 | train auc: 0.495862\n",
      "Batch 750/1286 | loss: 0.6939455742536335 (0.063s) | train acc: 0.5358 | train auc: 0.494368\n",
      "Batch 800/1286 | loss: 0.6980321972873798 (0.063s) | train acc: 0.5341 | train auc: 0.493880\n",
      "Batch 850/1286 | loss: 0.6963374865332125 (0.063s) | train acc: 0.5326 | train auc: 0.494611\n",
      "Batch 900/1286 | loss: 0.7043248449986025 (0.063s) | train acc: 0.5292 | train auc: 0.495794\n",
      "Batch 950/1286 | loss: 0.7025511638635702 (0.063s) | train acc: 0.5261 | train auc: 0.500321\n",
      "Batch 1000/1286 | loss: 0.698064539373466 (0.063s) | train acc: 0.5200 | train auc: 0.499150\n",
      "Batch 1050/1286 | loss: 0.696194914063622 (0.063s) | train acc: 0.5173 | train auc: 0.499394\n",
      "Batch 1100/1286 | loss: 0.6903902601423259 (0.063s) | train acc: 0.5172 | train auc: 0.500227\n",
      "Batch 1150/1286 | loss: 0.6915709616252833 (0.064s) | train acc: 0.5191 | train auc: 0.501727\n",
      "Batch 1200/1286 | loss: 0.6954547389992412 (0.063s) | train acc: 0.5182 | train auc: 0.501242\n",
      "Batch 1250/1286 | loss: 0.7029773630836067 (0.063s) | train acc: 0.5165 | train auc: 0.502042\n",
      "Batch 1286/1286 | loss: 0.7029496891413114 (0.046s) | train acc: 0.5140 | train auc: 0.501892\n",
      "Validation: val loss: 0.737 | val acc: 0.469 | val F1: 0.445 | val AUC: 0.498\n",
      "\n",
      "-----Epoch 3/50-----\n",
      "Batch 50/1286 | loss: 0.696785708614161 (0.063s) | train acc: 0.4275 | train auc: 0.494481\n",
      "Batch 100/1286 | loss: 0.6934069692747359 (0.063s) | train acc: 0.4556 | train auc: 0.489656\n",
      "Batch 150/1286 | loss: 0.6971825301839882 (0.063s) | train acc: 0.4704 | train auc: 0.487053\n",
      "Batch 200/1286 | loss: 0.696033588324825 (0.063s) | train acc: 0.4784 | train auc: 0.487617\n",
      "Batch 250/1286 | loss: 0.6932159122189817 (0.063s) | train acc: 0.4763 | train auc: 0.494401\n",
      "Batch 300/1286 | loss: 0.6869264064092115 (0.063s) | train acc: 0.4829 | train auc: 0.502485\n",
      "Batch 350/1286 | loss: 0.6985776961839788 (0.063s) | train acc: 0.4850 | train auc: 0.498346\n",
      "Batch 400/1286 | loss: 0.6899709892769691 (0.063s) | train acc: 0.4914 | train auc: 0.501091\n",
      "Batch 450/1286 | loss: 0.6897289355157978 (0.063s) | train acc: 0.4963 | train auc: 0.502756\n",
      "Batch 500/1286 | loss: 0.7051730844059441 (0.063s) | train acc: 0.4989 | train auc: 0.504444\n",
      "Batch 550/1286 | loss: 0.7006640344457015 (0.063s) | train acc: 0.4924 | train auc: 0.502667\n",
      "Batch 600/1286 | loss: 0.6928233954512146 (0.063s) | train acc: 0.4861 | train auc: 0.500416\n",
      "Batch 650/1286 | loss: 0.6907982573070873 (0.063s) | train acc: 0.4846 | train auc: 0.500598\n",
      "Batch 700/1286 | loss: 0.6893975830756917 (0.063s) | train acc: 0.4902 | train auc: 0.501173\n",
      "Batch 750/1286 | loss: 0.690717382335452 (0.063s) | train acc: 0.4939 | train auc: 0.501217\n",
      "Batch 800/1286 | loss: 0.6917928545053121 (0.063s) | train acc: 0.4977 | train auc: 0.502620\n",
      "Batch 850/1286 | loss: 0.7009042587678845 (0.063s) | train acc: 0.4988 | train auc: 0.503993\n",
      "Batch 900/1286 | loss: 0.6847126664704599 (0.063s) | train acc: 0.5001 | train auc: 0.504840\n",
      "Batch 950/1286 | loss: 0.693559267612079 (0.064s) | train acc: 0.5002 | train auc: 0.504945\n",
      "Batch 1000/1286 | loss: 0.6930114315226712 (0.063s) | train acc: 0.4994 | train auc: 0.503333\n",
      "Batch 1050/1286 | loss: 0.7022282761854339 (0.063s) | train acc: 0.4999 | train auc: 0.502920\n",
      "Batch 1100/1286 | loss: 0.6970363767063599 (0.063s) | train acc: 0.4957 | train auc: 0.501482\n",
      "Batch 1150/1286 | loss: 0.6806976309079417 (0.063s) | train acc: 0.4971 | train auc: 0.502451\n",
      "Batch 1200/1286 | loss: 0.7084472380581661 (0.064s) | train acc: 0.4979 | train auc: 0.501599\n",
      "Batch 1250/1286 | loss: 0.6949461712899456 (0.064s) | train acc: 0.4950 | train auc: 0.502023\n",
      "Batch 1286/1286 | loss: 0.6958554972077345 (0.046s) | train acc: 0.4952 | train auc: 0.502176\n",
      "Validation: val loss: 0.696 | val acc: 0.484 | val F1: 0.484 | val AUC: 0.505\n",
      "\n",
      "-----Epoch 4/50-----\n",
      "Batch 50/1286 | loss: 0.6928713253813666 (0.063s) | train acc: 0.5200 | train auc: 0.498653\n",
      "Batch 100/1286 | loss: 0.6959908089770745 (0.063s) | train acc: 0.4956 | train auc: 0.503682\n",
      "Batch 150/1286 | loss: 0.6852877707805498 (0.063s) | train acc: 0.4983 | train auc: 0.518372\n",
      "Batch 200/1286 | loss: 0.701393563159774 (0.063s) | train acc: 0.4959 | train auc: 0.507333\n",
      "Batch 250/1286 | loss: 0.6990277358751764 (0.063s) | train acc: 0.4955 | train auc: 0.508692\n",
      "Batch 300/1286 | loss: 0.6993103086781925 (0.063s) | train acc: 0.4875 | train auc: 0.509463\n",
      "Batch 350/1286 | loss: 0.6923482587881626 (0.063s) | train acc: 0.4798 | train auc: 0.507341\n",
      "Batch 400/1286 | loss: 0.6999588505952719 (0.064s) | train acc: 0.4783 | train auc: 0.505154\n",
      "Batch 450/1286 | loss: 0.6920876856473663 (0.063s) | train acc: 0.4793 | train auc: 0.503689\n",
      "Batch 500/1286 | loss: 0.6895830206894474 (0.063s) | train acc: 0.4864 | train auc: 0.504268\n",
      "Batch 550/1286 | loss: 0.6847072084390065 (0.064s) | train acc: 0.4911 | train auc: 0.506537\n",
      "Batch 600/1286 | loss: 0.6916429740939141 (0.064s) | train acc: 0.4972 | train auc: 0.505986\n",
      "Batch 650/1286 | loss: 0.7016760037834754 (0.063s) | train acc: 0.5034 | train auc: 0.504920\n",
      "Batch 700/1286 | loss: 0.6892798686692738 (0.064s) | train acc: 0.5045 | train auc: 0.503535\n",
      "Batch 750/1286 | loss: 0.6958304514089704 (0.064s) | train acc: 0.5033 | train auc: 0.501246\n",
      "Batch 800/1286 | loss: 0.6890876748143395 (0.063s) | train acc: 0.5052 | train auc: 0.501008\n",
      "Batch 850/1286 | loss: 0.6933325593764573 (0.063s) | train acc: 0.5082 | train auc: 0.501627\n",
      "Batch 900/1286 | loss: 0.693958880897601 (0.064s) | train acc: 0.5106 | train auc: 0.500906\n",
      "Batch 950/1286 | loss: 0.683488393037344 (0.063s) | train acc: 0.5149 | train auc: 0.501946\n",
      "Batch 1000/1286 | loss: 0.6980572774476635 (0.064s) | train acc: 0.5161 | train auc: 0.501323\n",
      "Batch 1050/1286 | loss: 0.7012213680340008 (0.063s) | train acc: 0.5156 | train auc: 0.502243\n",
      "Batch 1100/1286 | loss: 0.688196921517076 (0.064s) | train acc: 0.5135 | train auc: 0.501778\n",
      "Batch 1150/1286 | loss: 0.6900645932641883 (0.063s) | train acc: 0.5116 | train auc: 0.500626\n",
      "Batch 1200/1286 | loss: 0.7000825780500642 (0.064s) | train acc: 0.5121 | train auc: 0.500012\n",
      "Batch 1250/1286 | loss: 0.6987743735660973 (0.063s) | train acc: 0.5123 | train auc: 0.501907\n",
      "Batch 1286/1286 | loss: 0.7021574339120841 (0.046s) | train acc: 0.5094 | train auc: 0.500819\n",
      "Validation: val loss: 0.694 | val acc: 0.498 | val F1: 0.537 | val AUC: 0.503\n",
      "\n",
      "-----Epoch 5/50-----\n",
      "Batch 50/1286 | loss: 0.6994257489811879 (0.063s) | train acc: 0.4300 | train auc: 0.495127\n",
      "Batch 100/1286 | loss: 0.6949915657947053 (0.063s) | train acc: 0.4600 | train auc: 0.522713\n",
      "Batch 150/1286 | loss: 0.6881675482789382 (0.063s) | train acc: 0.4667 | train auc: 0.514930\n",
      "Batch 200/1286 | loss: 0.6953606886428771 (0.063s) | train acc: 0.4813 | train auc: 0.514413\n",
      "Batch 250/1286 | loss: 0.6957747043796841 (0.063s) | train acc: 0.4765 | train auc: 0.507942\n",
      "Batch 300/1286 | loss: 0.6879292349827659 (0.064s) | train acc: 0.4748 | train auc: 0.506094\n",
      "Batch 350/1286 | loss: 0.6845005726241372 (0.063s) | train acc: 0.4882 | train auc: 0.511263\n",
      "Batch 400/1286 | loss: 0.7058849119418449 (0.063s) | train acc: 0.4889 | train auc: 0.504704\n",
      "Batch 450/1286 | loss: 0.6896469338355206 (0.063s) | train acc: 0.4869 | train auc: 0.502116\n",
      "Batch 500/1286 | loss: 0.6996168503241208 (0.063s) | train acc: 0.4903 | train auc: 0.502118\n",
      "Batch 550/1286 | loss: 0.7022152173065358 (0.064s) | train acc: 0.4860 | train auc: 0.502593\n",
      "Batch 600/1286 | loss: 0.6963484669012611 (0.063s) | train acc: 0.4815 | train auc: 0.501457\n",
      "Batch 650/1286 | loss: 0.6984434750898535 (0.063s) | train acc: 0.4779 | train auc: 0.501257\n",
      "Batch 700/1286 | loss: 0.6898894854706709 (0.063s) | train acc: 0.4759 | train auc: 0.500756\n",
      "Batch 750/1286 | loss: 0.7011171636594763 (0.064s) | train acc: 0.4738 | train auc: 0.499146\n",
      "Batch 800/1286 | loss: 0.6886461714170955 (0.063s) | train acc: 0.4696 | train auc: 0.498616\n",
      "Batch 850/1286 | loss: 0.6844847299011846 (0.063s) | train acc: 0.4740 | train auc: 0.500256\n",
      "Batch 900/1286 | loss: 0.6898950691845234 (0.063s) | train acc: 0.4800 | train auc: 0.500720\n",
      "Batch 950/1286 | loss: 0.683813234897574 (0.063s) | train acc: 0.4863 | train auc: 0.502661\n",
      "Batch 1000/1286 | loss: 0.6971229692686863 (0.063s) | train acc: 0.4911 | train auc: 0.503658\n",
      "Batch 1050/1286 | loss: 0.7027020425251475 (0.064s) | train acc: 0.4904 | train auc: 0.502991\n",
      "Batch 1100/1286 | loss: 0.6921784824759313 (0.064s) | train acc: 0.4880 | train auc: 0.503160\n",
      "Batch 1150/1286 | loss: 0.6851534280961808 (0.063s) | train acc: 0.4876 | train auc: 0.503750\n",
      "Batch 1200/1286 | loss: 0.6989959966767436 (0.063s) | train acc: 0.4907 | train auc: 0.503191\n",
      "Batch 1250/1286 | loss: 0.6938477749100794 (0.063s) | train acc: 0.4904 | train auc: 0.502587\n",
      "Batch 1286/1286 | loss: 0.6904782940076978 (0.046s) | train acc: 0.4886 | train auc: 0.501820\n",
      "Validation: val loss: 0.695 | val acc: 0.573 | val F1: 0.704 | val AUC: 0.504\n",
      "\n",
      "-----Epoch 6/50-----\n",
      "Batch 50/1286 | loss: 0.6817858179791152 (0.063s) | train acc: 0.5575 | train auc: 0.517674\n",
      "Batch 100/1286 | loss: 0.6985831957872183 (0.063s) | train acc: 0.5425 | train auc: 0.486170\n",
      "Batch 150/1286 | loss: 0.6992305533992124 (0.063s) | train acc: 0.5354 | train auc: 0.492906\n",
      "Batch 200/1286 | loss: 0.701571332766455 (0.063s) | train acc: 0.5281 | train auc: 0.505500\n",
      "Batch 250/1286 | loss: 0.6897939519315439 (0.063s) | train acc: 0.5172 | train auc: 0.509802\n",
      "Batch 300/1286 | loss: 0.6935990624586181 (0.063s) | train acc: 0.5052 | train auc: 0.504455\n",
      "Batch 350/1286 | loss: 0.6913029943443693 (0.063s) | train acc: 0.5002 | train auc: 0.503416\n",
      "Batch 400/1286 | loss: 0.6997359843521322 (0.063s) | train acc: 0.4928 | train auc: 0.504911\n",
      "Batch 450/1286 | loss: 0.698344747900072 (0.063s) | train acc: 0.4854 | train auc: 0.504165\n",
      "Batch 500/1286 | loss: 0.6928091388572001 (0.063s) | train acc: 0.4799 | train auc: 0.504425\n",
      "Batch 550/1286 | loss: 0.680251797595898 (0.063s) | train acc: 0.4835 | train auc: 0.505532\n",
      "Batch 600/1286 | loss: 0.6959831557590723 (0.063s) | train acc: 0.4914 | train auc: 0.503474\n",
      "Batch 650/1286 | loss: 0.689968788457589 (0.063s) | train acc: 0.4979 | train auc: 0.503452\n",
      "Batch 700/1286 | loss: 0.6991445994933616 (0.063s) | train acc: 0.4978 | train auc: 0.502653\n",
      "Batch 750/1286 | loss: 0.6976162353625718 (0.063s) | train acc: 0.4987 | train auc: 0.504480\n",
      "Batch 800/1286 | loss: 0.6943046909419234 (0.063s) | train acc: 0.4972 | train auc: 0.505780\n",
      "Batch 850/1286 | loss: 0.6943361686618779 (0.063s) | train acc: 0.4955 | train auc: 0.506125\n",
      "Batch 900/1286 | loss: 0.6864011803134714 (0.063s) | train acc: 0.4936 | train auc: 0.505024\n",
      "Batch 950/1286 | loss: 0.7001145225261499 (0.063s) | train acc: 0.4942 | train auc: 0.503907\n",
      "Batch 1000/1286 | loss: 0.6935072680235783 (0.063s) | train acc: 0.4929 | train auc: 0.503638\n",
      "Batch 1050/1286 | loss: 0.6940403737417283 (0.064s) | train acc: 0.4929 | train auc: 0.503990\n",
      "Batch 1100/1286 | loss: 0.6917036762254871 (0.063s) | train acc: 0.4939 | train auc: 0.503497\n",
      "Batch 1150/1286 | loss: 0.6869148033423703 (0.063s) | train acc: 0.4964 | train auc: 0.503537\n",
      "Batch 1200/1286 | loss: 0.6968700432172982 (0.063s) | train acc: 0.4992 | train auc: 0.502430\n",
      "Batch 1250/1286 | loss: 0.6987849089603019 (0.063s) | train acc: 0.4999 | train auc: 0.502254\n",
      "Batch 1286/1286 | loss: 0.6896270490444196 (0.045s) | train acc: 0.5003 | train auc: 0.501790\n",
      "Validation: val loss: 0.694 | val acc: 0.592 | val F1: 0.737 | val AUC: 0.503\n",
      "\n",
      "-----Epoch 7/50-----\n",
      "Batch 50/1286 | loss: 0.6981656360303855 (0.063s) | train acc: 0.5162 | train auc: 0.491337\n",
      "Batch 100/1286 | loss: 0.6953560831396234 (0.062s) | train acc: 0.4675 | train auc: 0.475914\n",
      "Batch 150/1286 | loss: 0.6988609932214753 (0.062s) | train acc: 0.4633 | train auc: 0.490031\n",
      "Batch 200/1286 | loss: 0.6894932811849434 (0.063s) | train acc: 0.4597 | train auc: 0.482763\n",
      "Batch 250/1286 | loss: 0.7028401716010759 (0.063s) | train acc: 0.4682 | train auc: 0.485379\n",
      "Batch 300/1286 | loss: 0.6992168842987505 (0.063s) | train acc: 0.4592 | train auc: 0.485929\n",
      "Batch 350/1286 | loss: 0.6899504612623096 (0.063s) | train acc: 0.4570 | train auc: 0.491105\n",
      "Batch 400/1286 | loss: 0.6984398797442819 (0.063s) | train acc: 0.4572 | train auc: 0.491190\n",
      "Batch 450/1286 | loss: 0.6884599215377716 (0.063s) | train acc: 0.4562 | train auc: 0.491374\n",
      "Batch 500/1286 | loss: 0.6952068711425581 (0.064s) | train acc: 0.4579 | train auc: 0.493457\n",
      "Batch 550/1286 | loss: 0.683626460373717 (0.065s) | train acc: 0.4582 | train auc: 0.495384\n",
      "Batch 600/1286 | loss: 0.6959996527070217 (0.064s) | train acc: 0.4604 | train auc: 0.495888\n",
      "Batch 650/1286 | loss: 0.6914522428020803 (0.064s) | train acc: 0.4614 | train auc: 0.496795\n",
      "Batch 700/1286 | loss: 0.6915379809712903 (0.064s) | train acc: 0.4587 | train auc: 0.495744\n",
      "Batch 750/1286 | loss: 0.6919962143470905 (0.064s) | train acc: 0.4607 | train auc: 0.497371\n",
      "Batch 800/1286 | loss: 0.7019257138629008 (0.064s) | train acc: 0.4634 | train auc: 0.498784\n",
      "Batch 850/1286 | loss: 0.6929788436085239 (0.064s) | train acc: 0.4619 | train auc: 0.498516\n",
      "Batch 900/1286 | loss: 0.6947114292560377 (0.064s) | train acc: 0.4619 | train auc: 0.499649\n",
      "Batch 950/1286 | loss: 0.7002023090402676 (0.064s) | train acc: 0.4592 | train auc: 0.498447\n",
      "Batch 1000/1286 | loss: 0.6863853714942053 (0.064s) | train acc: 0.4566 | train auc: 0.496516\n",
      "Batch 1050/1286 | loss: 0.6830735688819787 (0.063s) | train acc: 0.4609 | train auc: 0.498730\n",
      "Batch 1100/1286 | loss: 0.6967164961516308 (0.064s) | train acc: 0.4667 | train auc: 0.498964\n",
      "Batch 1150/1286 | loss: 0.6857991515680203 (0.064s) | train acc: 0.4698 | train auc: 0.500061\n",
      "Batch 1200/1286 | loss: 0.6882824703928475 (0.063s) | train acc: 0.4738 | train auc: 0.501115\n",
      "Batch 1250/1286 | loss: 0.694139841220432 (0.063s) | train acc: 0.4760 | train auc: 0.501297\n",
      "Batch 1286/1286 | loss: 0.6990591275322099 (0.046s) | train acc: 0.4783 | train auc: 0.501637\n",
      "Validation: val loss: 0.693 | val acc: 0.574 | val F1: 0.711 | val AUC: 0.505\n",
      "\n",
      "-----Epoch 8/50-----\n",
      "Batch 50/1286 | loss: 0.6946901356617488 (0.063s) | train acc: 0.5212 | train auc: 0.480174\n",
      "Batch 100/1286 | loss: 0.6992418161935307 (0.063s) | train acc: 0.4931 | train auc: 0.470241\n",
      "Batch 150/1286 | loss: 0.6990882339126446 (0.063s) | train acc: 0.4717 | train auc: 0.477374\n",
      "Batch 200/1286 | loss: 0.6954336098887048 (0.063s) | train acc: 0.4575 | train auc: 0.484130\n",
      "Batch 250/1286 | loss: 0.692149069097448 (0.063s) | train acc: 0.4515 | train auc: 0.491945\n",
      "Batch 300/1286 | loss: 0.6899411585520867 (0.063s) | train acc: 0.4494 | train auc: 0.489925\n",
      "Batch 350/1286 | loss: 0.6997772551567139 (0.063s) | train acc: 0.4545 | train auc: 0.489162\n",
      "Batch 400/1286 | loss: 0.6917670081667793 (0.064s) | train acc: 0.4558 | train auc: 0.489601\n",
      "Batch 450/1286 | loss: 0.6919779915337441 (0.063s) | train acc: 0.4547 | train auc: 0.490912\n",
      "Batch 500/1286 | loss: 0.6965230138118748 (0.063s) | train acc: 0.4534 | train auc: 0.489848\n",
      "Batch 550/1286 | loss: 0.6997947396824258 (0.063s) | train acc: 0.4527 | train auc: 0.492141\n",
      "Batch 600/1286 | loss: 0.6872383398898063 (0.063s) | train acc: 0.4510 | train auc: 0.492951\n",
      "Batch 650/1286 | loss: 0.6927265978882321 (0.063s) | train acc: 0.4546 | train auc: 0.491722\n",
      "Batch 700/1286 | loss: 0.6894776256816351 (0.063s) | train acc: 0.4587 | train auc: 0.492596\n",
      "Batch 750/1286 | loss: 0.6924554853934078 (0.063s) | train acc: 0.4644 | train auc: 0.492414\n",
      "Batch 800/1286 | loss: 0.6968398295647665 (0.063s) | train acc: 0.4716 | train auc: 0.494096\n",
      "Batch 850/1286 | loss: 0.7016587795898982 (0.063s) | train acc: 0.4717 | train auc: 0.495067\n",
      "Batch 900/1286 | loss: 0.6967790720596482 (0.063s) | train acc: 0.4690 | train auc: 0.496452\n",
      "Batch 950/1286 | loss: 0.691974399714285 (0.063s) | train acc: 0.4657 | train auc: 0.496042\n",
      "Batch 1000/1286 | loss: 0.6912166144416871 (0.063s) | train acc: 0.4627 | train auc: 0.494355\n",
      "Batch 1050/1286 | loss: 0.6853477003350068 (0.063s) | train acc: 0.4682 | train auc: 0.496166\n",
      "Batch 1100/1286 | loss: 0.6772559646727924 (0.063s) | train acc: 0.4762 | train auc: 0.500089\n",
      "Batch 1150/1286 | loss: 0.6947005428654461 (0.063s) | train acc: 0.4816 | train auc: 0.500301\n",
      "Batch 1200/1286 | loss: 0.6934859511813234 (0.063s) | train acc: 0.4851 | train auc: 0.500926\n",
      "Batch 1250/1286 | loss: 0.6870429202850562 (0.063s) | train acc: 0.4880 | train auc: 0.503570\n",
      "Batch 1286/1286 | loss: 0.7005997890133862 (0.046s) | train acc: 0.4891 | train auc: 0.503511\n",
      "Validation: val loss: 0.693 | val acc: 0.549 | val F1: 0.657 | val AUC: 0.508\n",
      "\n",
      "-----Epoch 9/50-----\n",
      "Batch 50/1286 | loss: 0.694974897292044 (0.063s) | train acc: 0.5325 | train auc: 0.516463\n",
      "Batch 100/1286 | loss: 0.6808583116452966 (0.062s) | train acc: 0.5406 | train auc: 0.529854\n",
      "Batch 150/1286 | loss: 0.6942905974320587 (0.062s) | train acc: 0.5517 | train auc: 0.528662\n",
      "Batch 200/1286 | loss: 0.6928138880236601 (0.063s) | train acc: 0.5516 | train auc: 0.516012\n",
      "Batch 250/1286 | loss: 0.6952163260126808 (0.063s) | train acc: 0.5503 | train auc: 0.509142\n",
      "Batch 300/1286 | loss: 0.6906609223743787 (0.063s) | train acc: 0.5533 | train auc: 0.508008\n",
      "Batch 350/1286 | loss: 0.6942764066423028 (0.062s) | train acc: 0.5543 | train auc: 0.503227\n",
      "Batch 400/1286 | loss: 0.6994941317454891 (0.064s) | train acc: 0.5534 | train auc: 0.504488\n",
      "Batch 450/1286 | loss: 0.6971935753059101 (0.063s) | train acc: 0.5440 | train auc: 0.505998\n",
      "Batch 500/1286 | loss: 0.6947880602022497 (0.063s) | train acc: 0.5355 | train auc: 0.505884\n",
      "Batch 550/1286 | loss: 0.6984256432068918 (0.063s) | train acc: 0.5264 | train auc: 0.506180\n",
      "Batch 600/1286 | loss: 0.6884964844998319 (0.063s) | train acc: 0.5176 | train auc: 0.505011\n",
      "Batch 650/1286 | loss: 0.6939741466895185 (0.063s) | train acc: 0.5157 | train auc: 0.503819\n",
      "Batch 700/1286 | loss: 0.6958558534644862 (0.063s) | train acc: 0.5141 | train auc: 0.503317\n",
      "Batch 750/1286 | loss: 0.695143111829982 (0.065s) | train acc: 0.5097 | train auc: 0.502341\n",
      "Batch 800/1286 | loss: 0.692546171711906 (0.063s) | train acc: 0.5062 | train auc: 0.501569\n",
      "Batch 850/1286 | loss: 0.6923150066851669 (0.063s) | train acc: 0.5012 | train auc: 0.500525\n",
      "Batch 900/1286 | loss: 0.6962484620713958 (0.063s) | train acc: 0.5025 | train auc: 0.500048\n",
      "Batch 950/1286 | loss: 0.6851597782114629 (0.063s) | train acc: 0.5030 | train auc: 0.499437\n",
      "Batch 1000/1286 | loss: 0.6848373921009925 (0.063s) | train acc: 0.5086 | train auc: 0.500886\n",
      "Batch 1050/1286 | loss: 0.6886565704017913 (0.063s) | train acc: 0.5137 | train auc: 0.501543\n",
      "Batch 1100/1286 | loss: 0.6948101690535538 (0.063s) | train acc: 0.5174 | train auc: 0.501116\n",
      "Batch 1150/1286 | loss: 0.6877482773638849 (0.063s) | train acc: 0.5219 | train auc: 0.500939\n",
      "Batch 1200/1286 | loss: 0.7007534576436631 (0.063s) | train acc: 0.5220 | train auc: 0.500302\n",
      "Batch 1250/1286 | loss: 0.6946327726055685 (0.063s) | train acc: 0.5193 | train auc: 0.500231\n",
      "Batch 1286/1286 | loss: 0.7024891116985588 (0.045s) | train acc: 0.5180 | train auc: 0.500466\n",
      "Validation: val loss: 0.693 | val acc: 0.400 | val F1: 0.000 | val AUC: 0.506\n",
      "\n",
      "-----Epoch 10/50-----\n",
      "Batch 50/1286 | loss: 0.6829684199770292 (0.062s) | train acc: 0.4200 | train auc: 0.476241\n",
      "Batch 100/1286 | loss: 0.6942877281452229 (0.062s) | train acc: 0.4900 | train auc: 0.462299\n",
      "Batch 150/1286 | loss: 0.6850596669915686 (0.062s) | train acc: 0.5325 | train auc: 0.473867\n",
      "Batch 200/1286 | loss: 0.6985233149078433 (0.063s) | train acc: 0.5409 | train auc: 0.477613\n",
      "Batch 250/1286 | loss: 0.6928068836670773 (0.063s) | train acc: 0.5443 | train auc: 0.481271\n",
      "Batch 300/1286 | loss: 0.6970899814744503 (0.063s) | train acc: 0.5352 | train auc: 0.490313\n",
      "Batch 350/1286 | loss: 0.7012363029800531 (0.062s) | train acc: 0.5216 | train auc: 0.497289\n",
      "Batch 400/1286 | loss: 0.6921762540991518 (0.063s) | train acc: 0.5078 | train auc: 0.498395\n",
      "Batch 450/1286 | loss: 0.6930309619262891 (0.063s) | train acc: 0.5000 | train auc: 0.495021\n",
      "Batch 500/1286 | loss: 0.6893550696888462 (0.064s) | train acc: 0.4996 | train auc: 0.493670\n",
      "Batch 550/1286 | loss: 0.6923641595138241 (0.064s) | train acc: 0.4974 | train auc: 0.493312\n",
      "Batch 600/1286 | loss: 0.6909607417099047 (0.063s) | train acc: 0.4982 | train auc: 0.493614\n",
      "Batch 650/1286 | loss: 0.6956013758249686 (0.063s) | train acc: 0.4996 | train auc: 0.493243\n",
      "Batch 700/1286 | loss: 0.6955563662195877 (0.063s) | train acc: 0.4954 | train auc: 0.492991\n",
      "Batch 750/1286 | loss: 0.694293359801514 (0.063s) | train acc: 0.4950 | train auc: 0.494201\n",
      "Batch 800/1286 | loss: 0.6990090175573997 (0.063s) | train acc: 0.4918 | train auc: 0.493580\n",
      "Batch 850/1286 | loss: 0.6898445480419219 (0.063s) | train acc: 0.4905 | train auc: 0.494770\n",
      "Batch 900/1286 | loss: 0.6973401498297759 (0.064s) | train acc: 0.4888 | train auc: 0.494396\n",
      "Batch 950/1286 | loss: 0.6896681819553838 (0.063s) | train acc: 0.4856 | train auc: 0.494953\n",
      "Batch 1000/1286 | loss: 0.688260397656368 (0.063s) | train acc: 0.4864 | train auc: 0.496291\n",
      "Batch 1050/1286 | loss: 0.6826950274590428 (0.063s) | train acc: 0.4924 | train auc: 0.498003\n",
      "Batch 1100/1286 | loss: 0.6992743024092544 (0.063s) | train acc: 0.4961 | train auc: 0.496215\n",
      "Batch 1150/1286 | loss: 0.7002007907996775 (0.064s) | train acc: 0.4953 | train auc: 0.495170\n",
      "Batch 1200/1286 | loss: 0.6955017622246165 (0.063s) | train acc: 0.4920 | train auc: 0.495856\n",
      "Batch 1250/1286 | loss: 0.6916253638661177 (0.063s) | train acc: 0.4898 | train auc: 0.496269\n",
      "Batch 1286/1286 | loss: 0.7041320414526947 (0.046s) | train acc: 0.4892 | train auc: 0.496467\n",
      "Validation: val loss: 0.694 | val acc: 0.400 | val F1: 0.000 | val AUC: 0.496\n",
      "\n",
      "-----Epoch 11/50-----\n",
      "Batch 50/1286 | loss: 0.690035932441884 (0.063s) | train acc: 0.3862 | train auc: 0.480035\n",
      "Batch 100/1286 | loss: 0.6906915139110194 (0.062s) | train acc: 0.4188 | train auc: 0.494640\n",
      "Batch 150/1286 | loss: 0.6993983594923825 (0.063s) | train acc: 0.4292 | train auc: 0.490940\n",
      "Batch 200/1286 | loss: 0.6924286534748674 (0.063s) | train acc: 0.4213 | train auc: 0.492567\n",
      "Batch 250/1286 | loss: 0.6954356688797771 (0.063s) | train acc: 0.4273 | train auc: 0.491770\n",
      "Batch 300/1286 | loss: 0.6938658847785872 (0.063s) | train acc: 0.4273 | train auc: 0.493122\n",
      "Batch 350/1286 | loss: 0.6920752166291011 (0.063s) | train acc: 0.4293 | train auc: 0.498464\n",
      "Batch 400/1286 | loss: 0.6915665947635632 (0.063s) | train acc: 0.4297 | train auc: 0.492156\n",
      "Batch 450/1286 | loss: 0.7046242070023301 (0.063s) | train acc: 0.4315 | train auc: 0.488775\n",
      "Batch 500/1286 | loss: 0.696938549729345 (0.063s) | train acc: 0.4296 | train auc: 0.490341\n",
      "Batch 550/1286 | loss: 0.6913244142737515 (0.063s) | train acc: 0.4270 | train auc: 0.489593\n",
      "Batch 600/1286 | loss: 0.6886792561735932 (0.063s) | train acc: 0.4272 | train auc: 0.490872\n",
      "Batch 650/1286 | loss: 0.6913941671883596 (0.063s) | train acc: 0.4365 | train auc: 0.492183\n",
      "Batch 700/1286 | loss: 0.6984230678485587 (0.063s) | train acc: 0.4436 | train auc: 0.492092\n",
      "Batch 750/1286 | loss: 0.6952052752774018 (0.063s) | train acc: 0.4465 | train auc: 0.493484\n",
      "Batch 800/1286 | loss: 0.6856951819343373 (0.063s) | train acc: 0.4474 | train auc: 0.495318\n",
      "Batch 850/1286 | loss: 0.6809709254080346 (0.063s) | train acc: 0.4557 | train auc: 0.499738\n",
      "Batch 900/1286 | loss: 0.696250237183714 (0.063s) | train acc: 0.4629 | train auc: 0.498690\n",
      "Batch 950/1286 | loss: 0.6872687632272317 (0.063s) | train acc: 0.4712 | train auc: 0.500315\n",
      "Batch 1000/1286 | loss: 0.6924330875395885 (0.063s) | train acc: 0.4776 | train auc: 0.500314\n",
      "Batch 1050/1286 | loss: 0.6966182129353963 (0.063s) | train acc: 0.4827 | train auc: 0.499054\n",
      "Batch 1100/1286 | loss: 0.6962138151401507 (0.063s) | train acc: 0.4864 | train auc: 0.498812\n",
      "Batch 1150/1286 | loss: 0.7019799116675232 (0.063s) | train acc: 0.4859 | train auc: 0.499177\n",
      "Batch 1200/1286 | loss: 0.6955810026120989 (0.063s) | train acc: 0.4827 | train auc: 0.499081\n",
      "Batch 1250/1286 | loss: 0.6878288611434766 (0.063s) | train acc: 0.4793 | train auc: 0.498400\n",
      "Batch 1286/1286 | loss: 0.6929124892074408 (0.045s) | train acc: 0.4789 | train auc: 0.497743\n",
      "Validation: val loss: 0.693 | val acc: 0.440 | val F1: 0.299 | val AUC: 0.506\n",
      "\n",
      "-----Epoch 12/50-----\n",
      "Batch 50/1286 | loss: 0.6979855661652334 (0.063s) | train acc: 0.4400 | train auc: 0.506043\n",
      "Batch 100/1286 | loss: 0.6958041949329912 (0.062s) | train acc: 0.4444 | train auc: 0.488982\n",
      "Batch 150/1286 | loss: 0.6946572936613725 (0.063s) | train acc: 0.4304 | train auc: 0.483343\n",
      "Batch 200/1286 | loss: 0.690725383966645 (0.063s) | train acc: 0.4294 | train auc: 0.480867\n",
      "Batch 250/1286 | loss: 0.6895838570905631 (0.063s) | train acc: 0.4450 | train auc: 0.487064\n",
      "Batch 300/1286 | loss: 0.6930062369583332 (0.063s) | train acc: 0.4637 | train auc: 0.489069\n",
      "Batch 350/1286 | loss: 0.6911026195857342 (0.063s) | train acc: 0.4752 | train auc: 0.490626\n",
      "Batch 400/1286 | loss: 0.6915315077178574 (0.063s) | train acc: 0.4880 | train auc: 0.494237\n",
      "Batch 450/1286 | loss: 0.6952147799469472 (0.063s) | train acc: 0.4974 | train auc: 0.496439\n",
      "Batch 500/1286 | loss: 0.6985855596541097 (0.063s) | train acc: 0.4974 | train auc: 0.496081\n",
      "Batch 550/1286 | loss: 0.6907318980013274 (0.063s) | train acc: 0.4881 | train auc: 0.492938\n",
      "Batch 600/1286 | loss: 0.6867749022723705 (0.063s) | train acc: 0.4936 | train auc: 0.495501\n",
      "Batch 650/1286 | loss: 0.6856972608746255 (0.063s) | train acc: 0.5027 | train auc: 0.499189\n",
      "Batch 700/1286 | loss: 0.6967558205142044 (0.063s) | train acc: 0.5057 | train auc: 0.496124\n",
      "Batch 750/1286 | loss: 0.6923188926163497 (0.063s) | train acc: 0.5113 | train auc: 0.497125\n",
      "Batch 800/1286 | loss: 0.697988757372678 (0.063s) | train acc: 0.5148 | train auc: 0.498253\n",
      "Batch 850/1286 | loss: 0.6910554342369235 (0.063s) | train acc: 0.5169 | train auc: 0.497917\n",
      "Batch 900/1286 | loss: 0.6894026723243204 (0.063s) | train acc: 0.5194 | train auc: 0.497582\n",
      "Batch 950/1286 | loss: 0.695103671717899 (0.063s) | train acc: 0.5218 | train auc: 0.497289\n",
      "Batch 1000/1286 | loss: 0.6888591028181614 (0.063s) | train acc: 0.5238 | train auc: 0.497307\n",
      "Batch 1050/1286 | loss: 0.6972640101361524 (0.063s) | train acc: 0.5252 | train auc: 0.496355\n",
      "Batch 1100/1286 | loss: 0.6980509395367939 (0.063s) | train acc: 0.5261 | train auc: 0.495988\n",
      "Batch 1150/1286 | loss: 0.6944102672541463 (0.063s) | train acc: 0.5212 | train auc: 0.496310\n",
      "Batch 1200/1286 | loss: 0.6870714491243248 (0.063s) | train acc: 0.5182 | train auc: 0.495602\n",
      "Batch 1250/1286 | loss: 0.6991171298868418 (0.063s) | train acc: 0.5167 | train auc: 0.495949\n",
      "Batch 1286/1286 | loss: 0.69659489349433 (0.045s) | train acc: 0.5137 | train auc: 0.496315\n",
      "Validation: val loss: 0.693 | val acc: 0.400 | val F1: 0.000 | val AUC: 0.501\n",
      "\n",
      "-----Epoch 13/50-----\n",
      "Batch 50/1286 | loss: 0.6869476125747387 (0.062s) | train acc: 0.4675 | train auc: 0.500654\n",
      "Batch 100/1286 | loss: 0.6925477545996551 (0.062s) | train acc: 0.5244 | train auc: 0.508690\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(weight)\u001b[38;5;241m.\u001b[39mto(DEVICE))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# loss_fn = nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matt_dataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matt_dataset_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m att_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/CSE-8803-MLG/biomedical-gnn-privacy/membership_inference_attack/ml_util.py:139\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataset, loss_fn, epochs, batch_size, val_dataset, save_freq, save_path, scheduler, device)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-----Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m--> 139\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    141\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:803\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/profiler.py:636\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_enter_new(\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit:\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "att_dataset_train = create_attack_dataset(s_model, s_dataset_train, s_dataset_test, device=DEVICE)\n",
    "att_dataset_test = create_attack_dataset(t_model, t_dataset_train, t_dataset_test, device=DEVICE)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "weight_decay = 1e-3\n",
    "att_model = GenericAttackModel(num_feat=num_categories).to(DEVICE)\n",
    "optimizer = optim.Adam(att_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "weight = compute_class_weight('balanced', \n",
    "                              classes=np.unique(att_dataset_train.tensors[1].argmax(dim=1)), \n",
    "                              y=att_dataset_train.tensors[1].argmax(dim=1).numpy())\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weight).to(DEVICE))\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(att_model, optimizer, att_dataset_train, loss_fn, epochs, batch_size, val_dataset=att_dataset_test, device=DEVICE)\n",
    "att_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac59db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5027751177567983"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scores_test = predict(att_model, att_dataset_test, device=DEVICE, logits=True, return_type='pt').cpu()\n",
    "true_scores_test = att_dataset_test.tensors[1]\n",
    "get_auroc_score(pred_scores_test, true_scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6370152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142866732901909"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scores_train = predict(att_model, att_dataset_train, device=DEVICE, logits=True, return_type='pt').cpu()\n",
    "true_scores_train = att_dataset_train.tensors[1]\n",
    "get_auroc_score(pred_scores_train, true_scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17390f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x15520673a2c0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "fpr, tpr, thresholds = roc_curve(true_scores_test.argmax(dim=1), pred_scores_test[:,1])\n",
    "\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e2c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e8c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
