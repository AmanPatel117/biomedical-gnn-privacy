{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac748383",
   "metadata": {},
   "source": [
    "# Old version of membership inference attack (MIA)\n",
    "## Keenan Hom\n",
    "The original choice for MIA is here: https://arxiv.org/pdf/2102.05429. Because there was no provided code, I attempted to reproduce it from the descriptions in the paper, but was not able to achieve the same success that the authors did. In addition, this architecture is poorly suited to our project's needs because this attack method only works on a GNN trained on a single large graph, but all of our datasets are multi-graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7f65de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25117b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the project root (the folder containing \"main\") to sys.path\n",
    "sys.path.append('..')\n",
    "from membership_inference_attack.ml_util import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e31a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric import nn as gnn, transforms as T\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from ml_util import train_model, train_model_single_graph, get_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55a26e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = ('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4790848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cora_transform(data):\n",
    "    '''\n",
    "    Transform Cora data. Performs the following operations:\n",
    "    - Turn y labels into one-hot vectors\n",
    "    '''\n",
    "    data.y = torch.Tensor(OneHotEncoder().fit_transform(data.y.reshape(-1,1)).todense())\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96fd52ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b95edc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATPPIModel(nn.Module):\n",
    "    def __init__(self, num_feat, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gat1 = gnn.conv.GATConv(num_feat, 32, heads=4, dropout=0.)\n",
    "        self.gat2 = gnn.conv.GATConv(32 * 4, 128, heads=2, dropout=0.)\n",
    "        self.gat3 = gnn.conv.GATConv(128 * 2, 128, heads=2, dropout=0.)\n",
    "        self.gat4 = gnn.conv.GATConv(128 * 2, num_classes, heads=1, dropout=0)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        out1 = self.gat1(x, edge_index).relu()\n",
    "        out2 = self.gat2(out1, edge_index).relu()\n",
    "        out3 = self.gat3(out2, edge_index).relu()\n",
    "        out4 = self.gat4(out3, edge_index)\n",
    "        \n",
    "        return out4\n",
    "    \n",
    "    \n",
    "class GenericAttackModel(nn.Module):\n",
    "    def __init__(self, num_feat):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_feat, 128),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2),\n",
    "#             nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class GenericDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        '''\n",
    "        Generic dataset for training PyTorch models/\n",
    "        \n",
    "        x should be shape (num_samples, num_features)\n",
    "        y should be shape (num_samples, num_classes) and be one-hot encoded\n",
    "        '''\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c4b37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Planetoid(root='/home/hice1/khom9/scratch/CSE-8803-MLG-Data/', name='Citeseer', split='full', transform=cora_transform)\n",
    "dataset = PPI(root='/home/hice1/khom9/scratch/CSE-8803-MLG-Data/PPI', split='train')\n",
    "\n",
    "d = dataset[18]\n",
    "t_data, s_data = target_shadow_split(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4940eb0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.003\n",
      "No learning rate scheduling!\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/200-----\n",
      "Loss: 0.697044849395752 (0.011s), train acc: 0.000, val loss: 0.669, val acc: 0.000\n",
      "\n",
      "-----Epoch 2/200-----\n",
      "Loss: 0.6698315143585205 (0.006s), train acc: 0.000, val loss: 0.633, val acc: 0.000\n",
      "\n",
      "-----Epoch 3/200-----\n",
      "Loss: 0.6351044178009033 (0.005s), train acc: 0.000, val loss: 0.583, val acc: 0.000\n",
      "\n",
      "-----Epoch 4/200-----\n",
      "Loss: 0.5875763297080994 (0.005s), train acc: 0.000, val loss: 0.569, val acc: 0.000\n",
      "\n",
      "-----Epoch 5/200-----\n",
      "Loss: 0.5781262516975403 (0.005s), train acc: 0.000, val loss: 0.566, val acc: 0.000\n",
      "\n",
      "-----Epoch 6/200-----\n",
      "Loss: 0.5757008194923401 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 7/200-----\n",
      "Loss: 0.5555394291877747 (0.005s), train acc: 0.000, val loss: 0.543, val acc: 0.000\n",
      "\n",
      "-----Epoch 8/200-----\n",
      "Loss: 0.5488331317901611 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 9/200-----\n",
      "Loss: 0.5520561933517456 (0.005s), train acc: 0.000, val loss: 0.551, val acc: 0.000\n",
      "\n",
      "-----Epoch 10/200-----\n",
      "Loss: 0.5545660257339478 (0.005s), train acc: 0.000, val loss: 0.550, val acc: 0.000\n",
      "\n",
      "-----Epoch 11/200-----\n",
      "Loss: 0.5534812808036804 (0.005s), train acc: 0.000, val loss: 0.547, val acc: 0.000\n",
      "\n",
      "-----Epoch 12/200-----\n",
      "Loss: 0.5512664914131165 (0.005s), train acc: 0.000, val loss: 0.546, val acc: 0.000\n",
      "\n",
      "-----Epoch 13/200-----\n",
      "Loss: 0.5513651967048645 (0.005s), train acc: 0.000, val loss: 0.546, val acc: 0.000\n",
      "\n",
      "-----Epoch 14/200-----\n",
      "Loss: 0.5520716905593872 (0.005s), train acc: 0.000, val loss: 0.544, val acc: 0.000\n",
      "\n",
      "-----Epoch 15/200-----\n",
      "Loss: 0.5500563979148865 (0.005s), train acc: 0.000, val loss: 0.542, val acc: 0.000\n",
      "\n",
      "-----Epoch 16/200-----\n",
      "Loss: 0.5475562810897827 (0.005s), train acc: 0.000, val loss: 0.542, val acc: 0.000\n",
      "\n",
      "-----Epoch 17/200-----\n",
      "Loss: 0.5468418002128601 (0.005s), train acc: 0.000, val loss: 0.542, val acc: 0.000\n",
      "\n",
      "-----Epoch 18/200-----\n",
      "Loss: 0.5469586849212646 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 19/200-----\n",
      "Loss: 0.5464770793914795 (0.005s), train acc: 0.000, val loss: 0.540, val acc: 0.000\n",
      "\n",
      "-----Epoch 20/200-----\n",
      "Loss: 0.5454013347625732 (0.005s), train acc: 0.000, val loss: 0.539, val acc: 0.000\n",
      "\n",
      "-----Epoch 21/200-----\n",
      "Loss: 0.5448998212814331 (0.005s), train acc: 0.000, val loss: 0.539, val acc: 0.000\n",
      "\n",
      "-----Epoch 22/200-----\n",
      "Loss: 0.5451391339302063 (0.005s), train acc: 0.000, val loss: 0.538, val acc: 0.000\n",
      "\n",
      "-----Epoch 23/200-----\n",
      "Loss: 0.5445716977119446 (0.005s), train acc: 0.000, val loss: 0.538, val acc: 0.000\n",
      "\n",
      "-----Epoch 24/200-----\n",
      "Loss: 0.5435152053833008 (0.005s), train acc: 0.000, val loss: 0.538, val acc: 0.000\n",
      "\n",
      "-----Epoch 25/200-----\n",
      "Loss: 0.5430948734283447 (0.005s), train acc: 0.000, val loss: 0.538, val acc: 0.000\n",
      "\n",
      "-----Epoch 26/200-----\n",
      "Loss: 0.542701244354248 (0.005s), train acc: 0.000, val loss: 0.537, val acc: 0.000\n",
      "\n",
      "-----Epoch 27/200-----\n",
      "Loss: 0.5418821573257446 (0.005s), train acc: 0.000, val loss: 0.536, val acc: 0.000\n",
      "\n",
      "-----Epoch 28/200-----\n",
      "Loss: 0.5414214730262756 (0.005s), train acc: 0.000, val loss: 0.535, val acc: 0.000\n",
      "\n",
      "-----Epoch 29/200-----\n",
      "Loss: 0.540783166885376 (0.005s), train acc: 0.000, val loss: 0.535, val acc: 0.000\n",
      "\n",
      "-----Epoch 30/200-----\n",
      "Loss: 0.5396736860275269 (0.005s), train acc: 0.000, val loss: 0.535, val acc: 0.000\n",
      "\n",
      "-----Epoch 31/200-----\n",
      "Loss: 0.5388511419296265 (0.005s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 32/200-----\n",
      "Loss: 0.5376443266868591 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 33/200-----\n",
      "Loss: 0.5366976857185364 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 34/200-----\n",
      "Loss: 0.5353270173072815 (0.005s), train acc: 0.000, val loss: 0.532, val acc: 0.000\n",
      "\n",
      "-----Epoch 35/200-----\n",
      "Loss: 0.5340953469276428 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 36/200-----\n",
      "Loss: 0.5332670211791992 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 37/200-----\n",
      "Loss: 0.532029390335083 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 38/200-----\n",
      "Loss: 0.5303454995155334 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 39/200-----\n",
      "Loss: 0.5296825170516968 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 40/200-----\n",
      "Loss: 0.5284796953201294 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 41/200-----\n",
      "Loss: 0.5264739990234375 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 42/200-----\n",
      "Loss: 0.5269114971160889 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 43/200-----\n",
      "Loss: 0.5251894593238831 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 44/200-----\n",
      "Loss: 0.5240689516067505 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 45/200-----\n",
      "Loss: 0.5227525234222412 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 46/200-----\n",
      "Loss: 0.520624041557312 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 47/200-----\n",
      "Loss: 0.5204705595970154 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 48/200-----\n",
      "Loss: 0.5183320641517639 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 49/200-----\n",
      "Loss: 0.5188328623771667 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 50/200-----\n",
      "Loss: 0.5187097787857056 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 51/200-----\n",
      "Loss: 0.5155493021011353 (0.005s), train acc: 0.000, val loss: 0.535, val acc: 0.000\n",
      "\n",
      "-----Epoch 52/200-----\n",
      "Loss: 0.5203348398208618 (0.005s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 53/200-----\n",
      "Loss: 0.5195713043212891 (0.005s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 54/200-----\n",
      "Loss: 0.5191639065742493 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 55/200-----\n",
      "Loss: 0.5131298303604126 (0.005s), train acc: 0.000, val loss: 0.532, val acc: 0.000\n",
      "\n",
      "-----Epoch 56/200-----\n",
      "Loss: 0.5155017375946045 (0.005s), train acc: 0.000, val loss: 0.532, val acc: 0.000\n",
      "\n",
      "-----Epoch 57/200-----\n",
      "Loss: 0.5141041874885559 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 58/200-----\n",
      "Loss: 0.515127956867218 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 59/200-----\n",
      "Loss: 0.508766770362854 (0.005s), train acc: 0.000, val loss: 0.535, val acc: 0.000\n",
      "\n",
      "-----Epoch 60/200-----\n",
      "Loss: 0.514563262462616 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 61/200-----\n",
      "Loss: 0.5085867047309875 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 62/200-----\n",
      "Loss: 0.5120236277580261 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 63/200-----\n",
      "Loss: 0.5051262974739075 (0.005s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 64/200-----\n",
      "Loss: 0.5110554695129395 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 65/200-----\n",
      "Loss: 0.5031943321228027 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 66/200-----\n",
      "Loss: 0.5068126916885376 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 67/200-----\n",
      "Loss: 0.5013354420661926 (0.005s), train acc: 0.000, val loss: 0.532, val acc: 0.000\n",
      "\n",
      "-----Epoch 68/200-----\n",
      "Loss: 0.5044782757759094 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 69/200-----\n",
      "Loss: 0.4984190762042999 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 70/200-----\n",
      "Loss: 0.5005595088005066 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 71/200-----\n",
      "Loss: 0.4959615468978882 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 72/200-----\n",
      "Loss: 0.4982266128063202 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 73/200-----\n",
      "Loss: 0.4953651428222656 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 74/200-----\n",
      "Loss: 0.4924987256526947 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 75/200-----\n",
      "Loss: 0.49275413155555725 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 76/200-----\n",
      "Loss: 0.4903087913990021 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 77/200-----\n",
      "Loss: 0.4892102777957916 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 78/200-----\n",
      "Loss: 0.4876137673854828 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 79/200-----\n",
      "Loss: 0.4856804609298706 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 80/200-----\n",
      "Loss: 0.48464131355285645 (0.005s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 81/200-----\n",
      "Loss: 0.4832923412322998 (0.005s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 82/200-----\n",
      "Loss: 0.48126471042633057 (0.005s), train acc: 0.000, val loss: 0.535, val acc: 0.000\n",
      "\n",
      "-----Epoch 83/200-----\n",
      "Loss: 0.47914591431617737 (0.005s), train acc: 0.000, val loss: 0.536, val acc: 0.000\n",
      "\n",
      "-----Epoch 84/200-----\n",
      "Loss: 0.4776202440261841 (0.005s), train acc: 0.000, val loss: 0.538, val acc: 0.000\n",
      "\n",
      "-----Epoch 85/200-----\n",
      "Loss: 0.4760754704475403 (0.005s), train acc: 0.000, val loss: 0.539, val acc: 0.000\n",
      "\n",
      "-----Epoch 86/200-----\n",
      "Loss: 0.47444701194763184 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 87/200-----\n",
      "Loss: 0.4738929569721222 (0.005s), train acc: 0.000, val loss: 0.561, val acc: 0.000\n",
      "\n",
      "-----Epoch 88/200-----\n",
      "Loss: 0.4916253387928009 (0.005s), train acc: 0.000, val loss: 0.625, val acc: 0.000\n",
      "\n",
      "-----Epoch 89/200-----\n",
      "Loss: 0.5640067458152771 (0.005s), train acc: 0.000, val loss: 0.549, val acc: 0.000\n",
      "\n",
      "-----Epoch 90/200-----\n",
      "Loss: 0.49544408917427063 (0.005s), train acc: 0.000, val loss: 0.586, val acc: 0.000\n",
      "\n",
      "-----Epoch 91/200-----\n",
      "Loss: 0.5470463037490845 (0.005s), train acc: 0.000, val loss: 0.542, val acc: 0.000\n",
      "\n",
      "-----Epoch 92/200-----\n",
      "Loss: 0.49848368763923645 (0.005s), train acc: 0.000, val loss: 0.537, val acc: 0.000\n",
      "\n",
      "-----Epoch 93/200-----\n",
      "Loss: 0.48906999826431274 (0.005s), train acc: 0.000, val loss: 0.553, val acc: 0.000\n",
      "\n",
      "-----Epoch 94/200-----\n",
      "Loss: 0.5064292550086975 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 95/200-----\n",
      "Loss: 0.4955475628376007 (0.005s), train acc: 0.000, val loss: 0.532, val acc: 0.000\n",
      "\n",
      "-----Epoch 96/200-----\n",
      "Loss: 0.48723700642585754 (0.005s), train acc: 0.000, val loss: 0.537, val acc: 0.000\n",
      "\n",
      "-----Epoch 97/200-----\n",
      "Loss: 0.4918881058692932 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 98/200-----\n",
      "Loss: 0.4864497780799866 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 99/200-----\n",
      "Loss: 0.4839107096195221 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 100/200-----\n",
      "Loss: 0.4882959723472595 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 101/200-----\n",
      "Loss: 0.48859190940856934 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 102/200-----\n",
      "Loss: 0.48442086577415466 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 103/200-----\n",
      "Loss: 0.4817138910293579 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 104/200-----\n",
      "Loss: 0.48208168148994446 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 105/200-----\n",
      "Loss: 0.4808739721775055 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 106/200-----\n",
      "Loss: 0.47787198424339294 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 107/200-----\n",
      "Loss: 0.4766577482223511 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 108/200-----\n",
      "Loss: 0.4764644205570221 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 109/200-----\n",
      "Loss: 0.4745887517929077 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 110/200-----\n",
      "Loss: 0.47181305289268494 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 111/200-----\n",
      "Loss: 0.4710456132888794 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 112/200-----\n",
      "Loss: 0.46971291303634644 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 113/200-----\n",
      "Loss: 0.4668084979057312 (0.005s), train acc: 0.000, val loss: 0.532, val acc: 0.000\n",
      "\n",
      "-----Epoch 114/200-----\n",
      "Loss: 0.4657716155052185 (0.005s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 115/200-----\n",
      "Loss: 0.46453744173049927 (0.005s), train acc: 0.000, val loss: 0.536, val acc: 0.000\n",
      "\n",
      "-----Epoch 116/200-----\n",
      "Loss: 0.4622456729412079 (0.005s), train acc: 0.000, val loss: 0.540, val acc: 0.000\n",
      "\n",
      "-----Epoch 117/200-----\n",
      "Loss: 0.46145346760749817 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 118/200-----\n",
      "Loss: 0.45992720127105713 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 119/200-----\n",
      "Loss: 0.4575531482696533 (0.005s), train acc: 0.000, val loss: 0.543, val acc: 0.000\n",
      "\n",
      "-----Epoch 120/200-----\n",
      "Loss: 0.4568973779678345 (0.005s), train acc: 0.000, val loss: 0.542, val acc: 0.000\n",
      "\n",
      "-----Epoch 121/200-----\n",
      "Loss: 0.45570316910743713 (0.005s), train acc: 0.000, val loss: 0.546, val acc: 0.000\n",
      "\n",
      "-----Epoch 122/200-----\n",
      "Loss: 0.45630013942718506 (0.005s), train acc: 0.000, val loss: 0.547, val acc: 0.000\n",
      "\n",
      "-----Epoch 123/200-----\n",
      "Loss: 0.4585971534252167 (0.005s), train acc: 0.000, val loss: 0.551, val acc: 0.000\n",
      "\n",
      "-----Epoch 124/200-----\n",
      "Loss: 0.4581269919872284 (0.005s), train acc: 0.000, val loss: 0.545, val acc: 0.000\n",
      "\n",
      "-----Epoch 125/200-----\n",
      "Loss: 0.45503464341163635 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 126/200-----\n",
      "Loss: 0.44969555735588074 (0.005s), train acc: 0.000, val loss: 0.545, val acc: 0.000\n",
      "\n",
      "-----Epoch 127/200-----\n",
      "Loss: 0.45059219002723694 (0.005s), train acc: 0.000, val loss: 0.550, val acc: 0.000\n",
      "\n",
      "-----Epoch 128/200-----\n",
      "Loss: 0.45053011178970337 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 129/200-----\n",
      "Loss: 0.4475642442703247 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 130/200-----\n",
      "Loss: 0.44454842805862427 (0.005s), train acc: 0.000, val loss: 0.550, val acc: 0.000\n",
      "\n",
      "-----Epoch 131/200-----\n",
      "Loss: 0.44359448552131653 (0.005s), train acc: 0.000, val loss: 0.553, val acc: 0.000\n",
      "\n",
      "-----Epoch 132/200-----\n",
      "Loss: 0.44499441981315613 (0.005s), train acc: 0.000, val loss: 0.564, val acc: 0.000\n",
      "\n",
      "-----Epoch 133/200-----\n",
      "Loss: 0.45207324624061584 (0.005s), train acc: 0.000, val loss: 0.577, val acc: 0.000\n",
      "\n",
      "-----Epoch 134/200-----\n",
      "Loss: 0.4699585437774658 (0.005s), train acc: 0.000, val loss: 0.546, val acc: 0.000\n",
      "\n",
      "-----Epoch 135/200-----\n",
      "Loss: 0.4529951512813568 (0.005s), train acc: 0.000, val loss: 0.545, val acc: 0.000\n",
      "\n",
      "-----Epoch 136/200-----\n",
      "Loss: 0.45547986030578613 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 137/200-----\n",
      "Loss: 0.451206237077713 (0.005s), train acc: 0.000, val loss: 0.547, val acc: 0.000\n",
      "\n",
      "-----Epoch 138/200-----\n",
      "Loss: 0.4533274471759796 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 139/200-----\n",
      "Loss: 0.4433075189590454 (0.005s), train acc: 0.000, val loss: 0.553, val acc: 0.000\n",
      "\n",
      "-----Epoch 140/200-----\n",
      "Loss: 0.4510471224784851 (0.005s), train acc: 0.000, val loss: 0.547, val acc: 0.000\n",
      "\n",
      "-----Epoch 141/200-----\n",
      "Loss: 0.4435451924800873 (0.005s), train acc: 0.000, val loss: 0.551, val acc: 0.000\n",
      "\n",
      "-----Epoch 142/200-----\n",
      "Loss: 0.44586917757987976 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 143/200-----\n",
      "Loss: 0.44186991453170776 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 144/200-----\n",
      "Loss: 0.4410766363143921 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 145/200-----\n",
      "Loss: 0.4395618736743927 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 146/200-----\n",
      "Loss: 0.43702468276023865 (0.005s), train acc: 0.000, val loss: 0.551, val acc: 0.000\n",
      "\n",
      "-----Epoch 147/200-----\n",
      "Loss: 0.43800362944602966 (0.005s), train acc: 0.000, val loss: 0.549, val acc: 0.000\n",
      "\n",
      "-----Epoch 148/200-----\n",
      "Loss: 0.4330192506313324 (0.005s), train acc: 0.000, val loss: 0.556, val acc: 0.000\n",
      "\n",
      "-----Epoch 149/200-----\n",
      "Loss: 0.435517281293869 (0.005s), train acc: 0.000, val loss: 0.553, val acc: 0.000\n",
      "\n",
      "-----Epoch 150/200-----\n",
      "Loss: 0.43071630597114563 (0.005s), train acc: 0.000, val loss: 0.558, val acc: 0.000\n",
      "\n",
      "-----Epoch 151/200-----\n",
      "Loss: 0.43147867918014526 (0.005s), train acc: 0.000, val loss: 0.561, val acc: 0.000\n",
      "\n",
      "-----Epoch 152/200-----\n",
      "Loss: 0.4313196837902069 (0.005s), train acc: 0.000, val loss: 0.558, val acc: 0.000\n",
      "\n",
      "-----Epoch 153/200-----\n",
      "Loss: 0.42640191316604614 (0.005s), train acc: 0.000, val loss: 0.562, val acc: 0.000\n",
      "\n",
      "-----Epoch 154/200-----\n",
      "Loss: 0.4286666810512543 (0.005s), train acc: 0.000, val loss: 0.565, val acc: 0.000\n",
      "\n",
      "-----Epoch 155/200-----\n",
      "Loss: 0.428382933139801 (0.005s), train acc: 0.000, val loss: 0.560, val acc: 0.000\n",
      "\n",
      "-----Epoch 156/200-----\n",
      "Loss: 0.4230937957763672 (0.005s), train acc: 0.000, val loss: 0.562, val acc: 0.000\n",
      "\n",
      "-----Epoch 157/200-----\n",
      "Loss: 0.423629492521286 (0.005s), train acc: 0.000, val loss: 0.570, val acc: 0.000\n",
      "\n",
      "-----Epoch 158/200-----\n",
      "Loss: 0.42625245451927185 (0.005s), train acc: 0.000, val loss: 0.565, val acc: 0.000\n",
      "\n",
      "-----Epoch 159/200-----\n",
      "Loss: 0.4215221703052521 (0.005s), train acc: 0.000, val loss: 0.566, val acc: 0.000\n",
      "\n",
      "-----Epoch 160/200-----\n",
      "Loss: 0.41808974742889404 (0.005s), train acc: 0.000, val loss: 0.572, val acc: 0.000\n",
      "\n",
      "-----Epoch 161/200-----\n",
      "Loss: 0.4186251759529114 (0.005s), train acc: 0.000, val loss: 0.577, val acc: 0.000\n",
      "\n",
      "-----Epoch 162/200-----\n",
      "Loss: 0.4204423725605011 (0.005s), train acc: 0.000, val loss: 0.583, val acc: 0.000\n",
      "\n",
      "-----Epoch 163/200-----\n",
      "Loss: 0.4231649339199066 (0.005s), train acc: 0.000, val loss: 0.577, val acc: 0.000\n",
      "\n",
      "-----Epoch 164/200-----\n",
      "Loss: 0.41991138458251953 (0.005s), train acc: 0.000, val loss: 0.577, val acc: 0.000\n",
      "\n",
      "-----Epoch 165/200-----\n",
      "Loss: 0.41588976979255676 (0.005s), train acc: 0.000, val loss: 0.572, val acc: 0.000\n",
      "\n",
      "-----Epoch 166/200-----\n",
      "Loss: 0.41151583194732666 (0.005s), train acc: 0.000, val loss: 0.574, val acc: 0.000\n",
      "\n",
      "-----Epoch 167/200-----\n",
      "Loss: 0.41179338097572327 (0.005s), train acc: 0.000, val loss: 0.586, val acc: 0.000\n",
      "\n",
      "-----Epoch 168/200-----\n",
      "Loss: 0.4160139858722687 (0.005s), train acc: 0.000, val loss: 0.586, val acc: 0.000\n",
      "\n",
      "-----Epoch 169/200-----\n",
      "Loss: 0.4193952679634094 (0.005s), train acc: 0.000, val loss: 0.595, val acc: 0.000\n",
      "\n",
      "-----Epoch 170/200-----\n",
      "Loss: 0.4231560230255127 (0.005s), train acc: 0.000, val loss: 0.575, val acc: 0.000\n",
      "\n",
      "-----Epoch 171/200-----\n",
      "Loss: 0.4090685546398163 (0.005s), train acc: 0.000, val loss: 0.578, val acc: 0.000\n",
      "\n",
      "-----Epoch 172/200-----\n",
      "Loss: 0.409196674823761 (0.005s), train acc: 0.000, val loss: 0.596, val acc: 0.000\n",
      "\n",
      "-----Epoch 173/200-----\n",
      "Loss: 0.4182911515235901 (0.005s), train acc: 0.000, val loss: 0.581, val acc: 0.000\n",
      "\n",
      "-----Epoch 174/200-----\n",
      "Loss: 0.4081193506717682 (0.005s), train acc: 0.000, val loss: 0.578, val acc: 0.000\n",
      "\n",
      "-----Epoch 175/200-----\n",
      "Loss: 0.40349653363227844 (0.006s), train acc: 0.000, val loss: 0.588, val acc: 0.000\n",
      "\n",
      "-----Epoch 176/200-----\n",
      "Loss: 0.4077933132648468 (0.005s), train acc: 0.000, val loss: 0.585, val acc: 0.000\n",
      "\n",
      "-----Epoch 177/200-----\n",
      "Loss: 0.4030977785587311 (0.005s), train acc: 0.000, val loss: 0.586, val acc: 0.000\n",
      "\n",
      "-----Epoch 178/200-----\n",
      "Loss: 0.3992123305797577 (0.005s), train acc: 0.000, val loss: 0.592, val acc: 0.000\n",
      "\n",
      "-----Epoch 179/200-----\n",
      "Loss: 0.39963385462760925 (0.005s), train acc: 0.000, val loss: 0.593, val acc: 0.000\n",
      "\n",
      "-----Epoch 180/200-----\n",
      "Loss: 0.4015907943248749 (0.005s), train acc: 0.000, val loss: 0.602, val acc: 0.000\n",
      "\n",
      "-----Epoch 181/200-----\n",
      "Loss: 0.40213656425476074 (0.005s), train acc: 0.000, val loss: 0.594, val acc: 0.000\n",
      "\n",
      "-----Epoch 182/200-----\n",
      "Loss: 0.396954208612442 (0.005s), train acc: 0.000, val loss: 0.601, val acc: 0.000\n",
      "\n",
      "-----Epoch 183/200-----\n",
      "Loss: 0.39296841621398926 (0.005s), train acc: 0.000, val loss: 0.605, val acc: 0.000\n",
      "\n",
      "-----Epoch 184/200-----\n",
      "Loss: 0.3914414048194885 (0.005s), train acc: 0.000, val loss: 0.603, val acc: 0.000\n",
      "\n",
      "-----Epoch 185/200-----\n",
      "Loss: 0.39287424087524414 (0.005s), train acc: 0.000, val loss: 0.620, val acc: 0.000\n",
      "\n",
      "-----Epoch 186/200-----\n",
      "Loss: 0.40010204911231995 (0.005s), train acc: 0.000, val loss: 0.625, val acc: 0.000\n",
      "\n",
      "-----Epoch 187/200-----\n",
      "Loss: 0.41208481788635254 (0.005s), train acc: 0.000, val loss: 0.650, val acc: 0.000\n",
      "\n",
      "-----Epoch 188/200-----\n",
      "Loss: 0.42767831683158875 (0.005s), train acc: 0.000, val loss: 0.600, val acc: 0.000\n",
      "\n",
      "-----Epoch 189/200-----\n",
      "Loss: 0.3948257565498352 (0.005s), train acc: 0.000, val loss: 0.604, val acc: 0.000\n",
      "\n",
      "-----Epoch 190/200-----\n",
      "Loss: 0.40352460741996765 (0.005s), train acc: 0.000, val loss: 0.611, val acc: 0.000\n",
      "\n",
      "-----Epoch 191/200-----\n",
      "Loss: 0.40358784794807434 (0.005s), train acc: 0.000, val loss: 0.598, val acc: 0.000\n",
      "\n",
      "-----Epoch 192/200-----\n",
      "Loss: 0.395435094833374 (0.005s), train acc: 0.000, val loss: 0.604, val acc: 0.000\n",
      "\n",
      "-----Epoch 193/200-----\n",
      "Loss: 0.4050896465778351 (0.005s), train acc: 0.000, val loss: 0.595, val acc: 0.000\n",
      "\n",
      "-----Epoch 194/200-----\n",
      "Loss: 0.3904933035373688 (0.005s), train acc: 0.000, val loss: 0.604, val acc: 0.000\n",
      "\n",
      "-----Epoch 195/200-----\n",
      "Loss: 0.39569908380508423 (0.005s), train acc: 0.000, val loss: 0.601, val acc: 0.000\n",
      "\n",
      "-----Epoch 196/200-----\n",
      "Loss: 0.3913347125053406 (0.005s), train acc: 0.000, val loss: 0.606, val acc: 0.000\n",
      "\n",
      "-----Epoch 197/200-----\n",
      "Loss: 0.38742372393608093 (0.005s), train acc: 0.000, val loss: 0.610, val acc: 0.000\n",
      "\n",
      "-----Epoch 198/200-----\n",
      "Loss: 0.38864004611968994 (0.005s), train acc: 0.000, val loss: 0.606, val acc: 0.000\n",
      "\n",
      "-----Epoch 199/200-----\n",
      "Loss: 0.3858642876148224 (0.005s), train acc: 0.000, val loss: 0.605, val acc: 0.000\n",
      "\n",
      "-----Epoch 200/200-----\n",
      "Loss: 0.3839377164840698 (0.005s), train acc: 0.000, val loss: 0.610, val acc: 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GATPPIModel(\n",
       "  (gat1): GATConv(50, 32, heads=4)\n",
       "  (gat2): GATConv(128, 128, heads=2)\n",
       "  (gat3): GATConv(256, 128, heads=2)\n",
       "  (gat4): GATConv(256, 121, heads=1)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.003\n",
    "epochs = 200\n",
    "t_model = GATPPIModel(num_feat=50, num_classes=121).to(DEVICE)\n",
    "optimizer = optim.Adam(t_model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_model_single_graph(t_model, optimizer, t_data, loss_fn, epochs, device=DEVICE)\n",
    "t_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8effd847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.003\n",
      "No learning rate scheduling!\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/200-----\n",
      "Loss: 0.6930162906646729 (0.006s), train acc: 0.000, val loss: 0.662, val acc: 0.000\n",
      "\n",
      "-----Epoch 2/200-----\n",
      "Loss: 0.663419783115387 (0.005s), train acc: 0.000, val loss: 0.618, val acc: 0.000\n",
      "\n",
      "-----Epoch 3/200-----\n",
      "Loss: 0.6211495399475098 (0.005s), train acc: 0.000, val loss: 0.571, val acc: 0.000\n",
      "\n",
      "-----Epoch 4/200-----\n",
      "Loss: 0.5798873901367188 (0.005s), train acc: 0.000, val loss: 0.580, val acc: 0.000\n",
      "\n",
      "-----Epoch 5/200-----\n",
      "Loss: 0.5964093208312988 (0.005s), train acc: 0.000, val loss: 0.551, val acc: 0.000\n",
      "\n",
      "-----Epoch 6/200-----\n",
      "Loss: 0.565484881401062 (0.005s), train acc: 0.000, val loss: 0.538, val acc: 0.000\n",
      "\n",
      "-----Epoch 7/200-----\n",
      "Loss: 0.5496479868888855 (0.005s), train acc: 0.000, val loss: 0.544, val acc: 0.000\n",
      "\n",
      "-----Epoch 8/200-----\n",
      "Loss: 0.5528281331062317 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 9/200-----\n",
      "Loss: 0.5567325949668884 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 10/200-----\n",
      "Loss: 0.5563843250274658 (0.006s), train acc: 0.000, val loss: 0.544, val acc: 0.000\n",
      "\n",
      "-----Epoch 11/200-----\n",
      "Loss: 0.5541217923164368 (0.005s), train acc: 0.000, val loss: 0.542, val acc: 0.000\n",
      "\n",
      "-----Epoch 12/200-----\n",
      "Loss: 0.55340176820755 (0.005s), train acc: 0.000, val loss: 0.542, val acc: 0.000\n",
      "\n",
      "-----Epoch 13/200-----\n",
      "Loss: 0.5539126992225647 (0.005s), train acc: 0.000, val loss: 0.540, val acc: 0.000\n",
      "\n",
      "-----Epoch 14/200-----\n",
      "Loss: 0.5520292520523071 (0.005s), train acc: 0.000, val loss: 0.538, val acc: 0.000\n",
      "\n",
      "-----Epoch 15/200-----\n",
      "Loss: 0.5489528775215149 (0.005s), train acc: 0.000, val loss: 0.537, val acc: 0.000\n",
      "\n",
      "-----Epoch 16/200-----\n",
      "Loss: 0.5473593473434448 (0.005s), train acc: 0.000, val loss: 0.538, val acc: 0.000\n",
      "\n",
      "-----Epoch 17/200-----\n",
      "Loss: 0.5471463799476624 (0.005s), train acc: 0.000, val loss: 0.537, val acc: 0.000\n",
      "\n",
      "-----Epoch 18/200-----\n",
      "Loss: 0.5467674732208252 (0.005s), train acc: 0.000, val loss: 0.536, val acc: 0.000\n",
      "\n",
      "-----Epoch 19/200-----\n",
      "Loss: 0.5457983613014221 (0.005s), train acc: 0.000, val loss: 0.535, val acc: 0.000\n",
      "\n",
      "-----Epoch 20/200-----\n",
      "Loss: 0.5454427599906921 (0.006s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 21/200-----\n",
      "Loss: 0.5458002090454102 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 22/200-----\n",
      "Loss: 0.5448567867279053 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 23/200-----\n",
      "Loss: 0.5439159274101257 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 24/200-----\n",
      "Loss: 0.5436632037162781 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 25/200-----\n",
      "Loss: 0.5431655049324036 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 26/200-----\n",
      "Loss: 0.5423422455787659 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 27/200-----\n",
      "Loss: 0.5421218276023865 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 28/200-----\n",
      "Loss: 0.5414987206459045 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 29/200-----\n",
      "Loss: 0.5403974652290344 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 30/200-----\n",
      "Loss: 0.5395870804786682 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 31/200-----\n",
      "Loss: 0.5390305519104004 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 32/200-----\n",
      "Loss: 0.5381349921226501 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 33/200-----\n",
      "Loss: 0.5370579957962036 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 34/200-----\n",
      "Loss: 0.5363604426383972 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 35/200-----\n",
      "Loss: 0.5360908508300781 (0.005s), train acc: 0.000, val loss: 0.525, val acc: 0.000\n",
      "\n",
      "-----Epoch 36/200-----\n",
      "Loss: 0.534140944480896 (0.005s), train acc: 0.000, val loss: 0.525, val acc: 0.000\n",
      "\n",
      "-----Epoch 37/200-----\n",
      "Loss: 0.5355686545372009 (0.005s), train acc: 0.000, val loss: 0.525, val acc: 0.000\n",
      "\n",
      "-----Epoch 38/200-----\n",
      "Loss: 0.5331845879554749 (0.005s), train acc: 0.000, val loss: 0.526, val acc: 0.000\n",
      "\n",
      "-----Epoch 39/200-----\n",
      "Loss: 0.5333549380302429 (0.005s), train acc: 0.000, val loss: 0.522, val acc: 0.000\n",
      "\n",
      "-----Epoch 40/200-----\n",
      "Loss: 0.5302925705909729 (0.005s), train acc: 0.000, val loss: 0.525, val acc: 0.000\n",
      "\n",
      "-----Epoch 41/200-----\n",
      "Loss: 0.5327228307723999 (0.005s), train acc: 0.000, val loss: 0.523, val acc: 0.000\n",
      "\n",
      "-----Epoch 42/200-----\n",
      "Loss: 0.529682457447052 (0.005s), train acc: 0.000, val loss: 0.524, val acc: 0.000\n",
      "\n",
      "-----Epoch 43/200-----\n",
      "Loss: 0.5309034585952759 (0.005s), train acc: 0.000, val loss: 0.520, val acc: 0.000\n",
      "\n",
      "-----Epoch 44/200-----\n",
      "Loss: 0.5272384881973267 (0.005s), train acc: 0.000, val loss: 0.523, val acc: 0.000\n",
      "\n",
      "-----Epoch 45/200-----\n",
      "Loss: 0.5301473736763 (0.005s), train acc: 0.000, val loss: 0.519, val acc: 0.000\n",
      "\n",
      "-----Epoch 46/200-----\n",
      "Loss: 0.5251849293708801 (0.005s), train acc: 0.000, val loss: 0.521, val acc: 0.000\n",
      "\n",
      "-----Epoch 47/200-----\n",
      "Loss: 0.5267197489738464 (0.005s), train acc: 0.000, val loss: 0.519, val acc: 0.000\n",
      "\n",
      "-----Epoch 48/200-----\n",
      "Loss: 0.5237468481063843 (0.005s), train acc: 0.000, val loss: 0.524, val acc: 0.000\n",
      "\n",
      "-----Epoch 49/200-----\n",
      "Loss: 0.527113676071167 (0.005s), train acc: 0.000, val loss: 0.519, val acc: 0.000\n",
      "\n",
      "-----Epoch 50/200-----\n",
      "Loss: 0.5229352712631226 (0.005s), train acc: 0.000, val loss: 0.520, val acc: 0.000\n",
      "\n",
      "-----Epoch 51/200-----\n",
      "Loss: 0.5239488482475281 (0.005s), train acc: 0.000, val loss: 0.516, val acc: 0.000\n",
      "\n",
      "-----Epoch 52/200-----\n",
      "Loss: 0.5205471515655518 (0.005s), train acc: 0.000, val loss: 0.518, val acc: 0.000\n",
      "\n",
      "-----Epoch 53/200-----\n",
      "Loss: 0.5234104990959167 (0.005s), train acc: 0.000, val loss: 0.517, val acc: 0.000\n",
      "\n",
      "-----Epoch 54/200-----\n",
      "Loss: 0.5206685066223145 (0.005s), train acc: 0.000, val loss: 0.518, val acc: 0.000\n",
      "\n",
      "-----Epoch 55/200-----\n",
      "Loss: 0.5209201574325562 (0.005s), train acc: 0.000, val loss: 0.514, val acc: 0.000\n",
      "\n",
      "-----Epoch 56/200-----\n",
      "Loss: 0.5176988840103149 (0.005s), train acc: 0.000, val loss: 0.515, val acc: 0.000\n",
      "\n",
      "-----Epoch 57/200-----\n",
      "Loss: 0.518139660358429 (0.005s), train acc: 0.000, val loss: 0.517, val acc: 0.000\n",
      "\n",
      "-----Epoch 58/200-----\n",
      "Loss: 0.5174602270126343 (0.005s), train acc: 0.000, val loss: 0.516, val acc: 0.000\n",
      "\n",
      "-----Epoch 59/200-----\n",
      "Loss: 0.5156010985374451 (0.005s), train acc: 0.000, val loss: 0.517, val acc: 0.000\n",
      "\n",
      "-----Epoch 60/200-----\n",
      "Loss: 0.516959011554718 (0.006s), train acc: 0.000, val loss: 0.515, val acc: 0.000\n",
      "\n",
      "-----Epoch 61/200-----\n",
      "Loss: 0.5126613974571228 (0.005s), train acc: 0.000, val loss: 0.516, val acc: 0.000\n",
      "\n",
      "-----Epoch 62/200-----\n",
      "Loss: 0.5129474401473999 (0.005s), train acc: 0.000, val loss: 0.515, val acc: 0.000\n",
      "\n",
      "-----Epoch 63/200-----\n",
      "Loss: 0.5116758346557617 (0.005s), train acc: 0.000, val loss: 0.513, val acc: 0.000\n",
      "\n",
      "-----Epoch 64/200-----\n",
      "Loss: 0.5092776417732239 (0.005s), train acc: 0.000, val loss: 0.514, val acc: 0.000\n",
      "\n",
      "-----Epoch 65/200-----\n",
      "Loss: 0.5094051957130432 (0.005s), train acc: 0.000, val loss: 0.513, val acc: 0.000\n",
      "\n",
      "-----Epoch 66/200-----\n",
      "Loss: 0.5080298185348511 (0.005s), train acc: 0.000, val loss: 0.513, val acc: 0.000\n",
      "\n",
      "-----Epoch 67/200-----\n",
      "Loss: 0.5061642527580261 (0.005s), train acc: 0.000, val loss: 0.512, val acc: 0.000\n",
      "\n",
      "-----Epoch 68/200-----\n",
      "Loss: 0.5048728585243225 (0.005s), train acc: 0.000, val loss: 0.513, val acc: 0.000\n",
      "\n",
      "-----Epoch 69/200-----\n",
      "Loss: 0.504474937915802 (0.005s), train acc: 0.000, val loss: 0.516, val acc: 0.000\n",
      "\n",
      "-----Epoch 70/200-----\n",
      "Loss: 0.506130039691925 (0.005s), train acc: 0.000, val loss: 0.517, val acc: 0.000\n",
      "\n",
      "-----Epoch 71/200-----\n",
      "Loss: 0.5086473822593689 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 72/200-----\n",
      "Loss: 0.518691897392273 (0.006s), train acc: 0.000, val loss: 0.509, val acc: 0.000\n",
      "\n",
      "-----Epoch 73/200-----\n",
      "Loss: 0.4999626874923706 (0.005s), train acc: 0.000, val loss: 0.524, val acc: 0.000\n",
      "\n",
      "-----Epoch 74/200-----\n",
      "Loss: 0.5178627371788025 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 75/200-----\n",
      "Loss: 0.5154002904891968 (0.005s), train acc: 0.000, val loss: 0.524, val acc: 0.000\n",
      "\n",
      "-----Epoch 76/200-----\n",
      "Loss: 0.5128118395805359 (0.005s), train acc: 0.000, val loss: 0.511, val acc: 0.000\n",
      "\n",
      "-----Epoch 77/200-----\n",
      "Loss: 0.5031402111053467 (0.005s), train acc: 0.000, val loss: 0.517, val acc: 0.000\n",
      "\n",
      "-----Epoch 78/200-----\n",
      "Loss: 0.5096921920776367 (0.005s), train acc: 0.000, val loss: 0.511, val acc: 0.000\n",
      "\n",
      "-----Epoch 79/200-----\n",
      "Loss: 0.49968549609184265 (0.005s), train acc: 0.000, val loss: 0.520, val acc: 0.000\n",
      "\n",
      "-----Epoch 80/200-----\n",
      "Loss: 0.5080865621566772 (0.005s), train acc: 0.000, val loss: 0.510, val acc: 0.000\n",
      "\n",
      "-----Epoch 81/200-----\n",
      "Loss: 0.49788784980773926 (0.005s), train acc: 0.000, val loss: 0.512, val acc: 0.000\n",
      "\n",
      "-----Epoch 82/200-----\n",
      "Loss: 0.5021448731422424 (0.005s), train acc: 0.000, val loss: 0.510, val acc: 0.000\n",
      "\n",
      "-----Epoch 83/200-----\n",
      "Loss: 0.49881887435913086 (0.005s), train acc: 0.000, val loss: 0.509, val acc: 0.000\n",
      "\n",
      "-----Epoch 84/200-----\n",
      "Loss: 0.4958076775074005 (0.005s), train acc: 0.000, val loss: 0.514, val acc: 0.000\n",
      "\n",
      "-----Epoch 85/200-----\n",
      "Loss: 0.4993806779384613 (0.005s), train acc: 0.000, val loss: 0.507, val acc: 0.000\n",
      "\n",
      "-----Epoch 86/200-----\n",
      "Loss: 0.4927065372467041 (0.005s), train acc: 0.000, val loss: 0.509, val acc: 0.000\n",
      "\n",
      "-----Epoch 87/200-----\n",
      "Loss: 0.49541106820106506 (0.005s), train acc: 0.000, val loss: 0.507, val acc: 0.000\n",
      "\n",
      "-----Epoch 88/200-----\n",
      "Loss: 0.49191442131996155 (0.005s), train acc: 0.000, val loss: 0.508, val acc: 0.000\n",
      "\n",
      "-----Epoch 89/200-----\n",
      "Loss: 0.4907107949256897 (0.005s), train acc: 0.000, val loss: 0.510, val acc: 0.000\n",
      "\n",
      "-----Epoch 90/200-----\n",
      "Loss: 0.49124953150749207 (0.005s), train acc: 0.000, val loss: 0.505, val acc: 0.000\n",
      "\n",
      "-----Epoch 91/200-----\n",
      "Loss: 0.48649513721466064 (0.005s), train acc: 0.000, val loss: 0.509, val acc: 0.000\n",
      "\n",
      "-----Epoch 92/200-----\n",
      "Loss: 0.48927032947540283 (0.005s), train acc: 0.000, val loss: 0.507, val acc: 0.000\n",
      "\n",
      "-----Epoch 93/200-----\n",
      "Loss: 0.48428037762641907 (0.005s), train acc: 0.000, val loss: 0.510, val acc: 0.000\n",
      "\n",
      "-----Epoch 94/200-----\n",
      "Loss: 0.48583677411079407 (0.005s), train acc: 0.000, val loss: 0.506, val acc: 0.000\n",
      "\n",
      "-----Epoch 95/200-----\n",
      "Loss: 0.48163506388664246 (0.005s), train acc: 0.000, val loss: 0.508, val acc: 0.000\n",
      "\n",
      "-----Epoch 96/200-----\n",
      "Loss: 0.4819912314414978 (0.005s), train acc: 0.000, val loss: 0.510, val acc: 0.000\n",
      "\n",
      "-----Epoch 97/200-----\n",
      "Loss: 0.4815186560153961 (0.005s), train acc: 0.000, val loss: 0.507, val acc: 0.000\n",
      "\n",
      "-----Epoch 98/200-----\n",
      "Loss: 0.4769081771373749 (0.005s), train acc: 0.000, val loss: 0.515, val acc: 0.000\n",
      "\n",
      "-----Epoch 99/200-----\n",
      "Loss: 0.4825309216976166 (0.005s), train acc: 0.000, val loss: 0.516, val acc: 0.000\n",
      "\n",
      "-----Epoch 100/200-----\n",
      "Loss: 0.4858189821243286 (0.005s), train acc: 0.000, val loss: 0.506, val acc: 0.000\n",
      "\n",
      "-----Epoch 101/200-----\n",
      "Loss: 0.47809118032455444 (0.005s), train acc: 0.000, val loss: 0.512, val acc: 0.000\n",
      "\n",
      "-----Epoch 102/200-----\n",
      "Loss: 0.4869046211242676 (0.005s), train acc: 0.000, val loss: 0.511, val acc: 0.000\n",
      "\n",
      "-----Epoch 103/200-----\n",
      "Loss: 0.4818960130214691 (0.005s), train acc: 0.000, val loss: 0.506, val acc: 0.000\n",
      "\n",
      "-----Epoch 104/200-----\n",
      "Loss: 0.476946622133255 (0.005s), train acc: 0.000, val loss: 0.509, val acc: 0.000\n",
      "\n",
      "-----Epoch 105/200-----\n",
      "Loss: 0.48187151551246643 (0.005s), train acc: 0.000, val loss: 0.505, val acc: 0.000\n",
      "\n",
      "-----Epoch 106/200-----\n",
      "Loss: 0.47312453389167786 (0.005s), train acc: 0.000, val loss: 0.511, val acc: 0.000\n",
      "\n",
      "-----Epoch 107/200-----\n",
      "Loss: 0.47669717669487 (0.005s), train acc: 0.000, val loss: 0.507, val acc: 0.000\n",
      "\n",
      "-----Epoch 108/200-----\n",
      "Loss: 0.47322696447372437 (0.005s), train acc: 0.000, val loss: 0.507, val acc: 0.000\n",
      "\n",
      "-----Epoch 109/200-----\n",
      "Loss: 0.47006484866142273 (0.005s), train acc: 0.000, val loss: 0.512, val acc: 0.000\n",
      "\n",
      "-----Epoch 110/200-----\n",
      "Loss: 0.4718416631221771 (0.005s), train acc: 0.000, val loss: 0.508, val acc: 0.000\n",
      "\n",
      "-----Epoch 111/200-----\n",
      "Loss: 0.4673939049243927 (0.005s), train acc: 0.000, val loss: 0.509, val acc: 0.000\n",
      "\n",
      "-----Epoch 112/200-----\n",
      "Loss: 0.46738752722740173 (0.005s), train acc: 0.000, val loss: 0.513, val acc: 0.000\n",
      "\n",
      "-----Epoch 113/200-----\n",
      "Loss: 0.46772801876068115 (0.005s), train acc: 0.000, val loss: 0.509, val acc: 0.000\n",
      "\n",
      "-----Epoch 114/200-----\n",
      "Loss: 0.46240201592445374 (0.005s), train acc: 0.000, val loss: 0.514, val acc: 0.000\n",
      "\n",
      "-----Epoch 115/200-----\n",
      "Loss: 0.46450063586235046 (0.005s), train acc: 0.000, val loss: 0.515, val acc: 0.000\n",
      "\n",
      "-----Epoch 116/200-----\n",
      "Loss: 0.4637351334095001 (0.005s), train acc: 0.000, val loss: 0.513, val acc: 0.000\n",
      "\n",
      "-----Epoch 117/200-----\n",
      "Loss: 0.46054401993751526 (0.005s), train acc: 0.000, val loss: 0.513, val acc: 0.000\n",
      "\n",
      "-----Epoch 118/200-----\n",
      "Loss: 0.4575692415237427 (0.005s), train acc: 0.000, val loss: 0.517, val acc: 0.000\n",
      "\n",
      "-----Epoch 119/200-----\n",
      "Loss: 0.4589432179927826 (0.005s), train acc: 0.000, val loss: 0.519, val acc: 0.000\n",
      "\n",
      "-----Epoch 120/200-----\n",
      "Loss: 0.45870256423950195 (0.005s), train acc: 0.000, val loss: 0.523, val acc: 0.000\n",
      "\n",
      "-----Epoch 121/200-----\n",
      "Loss: 0.4605996310710907 (0.005s), train acc: 0.000, val loss: 0.523, val acc: 0.000\n",
      "\n",
      "-----Epoch 122/200-----\n",
      "Loss: 0.45842236280441284 (0.005s), train acc: 0.000, val loss: 0.523, val acc: 0.000\n",
      "\n",
      "-----Epoch 123/200-----\n",
      "Loss: 0.4572315812110901 (0.005s), train acc: 0.000, val loss: 0.519, val acc: 0.000\n",
      "\n",
      "-----Epoch 124/200-----\n",
      "Loss: 0.45210883021354675 (0.005s), train acc: 0.000, val loss: 0.519, val acc: 0.000\n",
      "\n",
      "-----Epoch 125/200-----\n",
      "Loss: 0.4491311013698578 (0.005s), train acc: 0.000, val loss: 0.522, val acc: 0.000\n",
      "\n",
      "-----Epoch 126/200-----\n",
      "Loss: 0.4491860568523407 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 127/200-----\n",
      "Loss: 0.4529004395008087 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 128/200-----\n",
      "Loss: 0.4698205888271332 (0.005s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 129/200-----\n",
      "Loss: 0.471733421087265 (0.005s), train acc: 0.000, val loss: 0.526, val acc: 0.000\n",
      "\n",
      "-----Epoch 130/200-----\n",
      "Loss: 0.4584598243236542 (0.005s), train acc: 0.000, val loss: 0.519, val acc: 0.000\n",
      "\n",
      "-----Epoch 131/200-----\n",
      "Loss: 0.4514555037021637 (0.005s), train acc: 0.000, val loss: 0.527, val acc: 0.000\n",
      "\n",
      "-----Epoch 132/200-----\n",
      "Loss: 0.4609813988208771 (0.005s), train acc: 0.000, val loss: 0.517, val acc: 0.000\n",
      "\n",
      "-----Epoch 133/200-----\n",
      "Loss: 0.44750070571899414 (0.005s), train acc: 0.000, val loss: 0.526, val acc: 0.000\n",
      "\n",
      "-----Epoch 134/200-----\n",
      "Loss: 0.45510923862457275 (0.005s), train acc: 0.000, val loss: 0.523, val acc: 0.000\n",
      "\n",
      "-----Epoch 135/200-----\n",
      "Loss: 0.44928431510925293 (0.005s), train acc: 0.000, val loss: 0.523, val acc: 0.000\n",
      "\n",
      "-----Epoch 136/200-----\n",
      "Loss: 0.4474999010562897 (0.005s), train acc: 0.000, val loss: 0.524, val acc: 0.000\n",
      "\n",
      "-----Epoch 137/200-----\n",
      "Loss: 0.44793954491615295 (0.005s), train acc: 0.000, val loss: 0.520, val acc: 0.000\n",
      "\n",
      "-----Epoch 138/200-----\n",
      "Loss: 0.4438153803348541 (0.005s), train acc: 0.000, val loss: 0.523, val acc: 0.000\n",
      "\n",
      "-----Epoch 139/200-----\n",
      "Loss: 0.4453853666782379 (0.005s), train acc: 0.000, val loss: 0.523, val acc: 0.000\n",
      "\n",
      "-----Epoch 140/200-----\n",
      "Loss: 0.44072437286376953 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 141/200-----\n",
      "Loss: 0.44166430830955505 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 142/200-----\n",
      "Loss: 0.43874821066856384 (0.005s), train acc: 0.000, val loss: 0.531, val acc: 0.000\n",
      "\n",
      "-----Epoch 143/200-----\n",
      "Loss: 0.43738678097724915 (0.005s), train acc: 0.000, val loss: 0.532, val acc: 0.000\n",
      "\n",
      "-----Epoch 144/200-----\n",
      "Loss: 0.43651729822158813 (0.005s), train acc: 0.000, val loss: 0.533, val acc: 0.000\n",
      "\n",
      "-----Epoch 145/200-----\n",
      "Loss: 0.4357362985610962 (0.005s), train acc: 0.000, val loss: 0.530, val acc: 0.000\n",
      "\n",
      "-----Epoch 146/200-----\n",
      "Loss: 0.43169906735420227 (0.005s), train acc: 0.000, val loss: 0.532, val acc: 0.000\n",
      "\n",
      "-----Epoch 147/200-----\n",
      "Loss: 0.4321363568305969 (0.005s), train acc: 0.000, val loss: 0.537, val acc: 0.000\n",
      "\n",
      "-----Epoch 148/200-----\n",
      "Loss: 0.4317765533924103 (0.005s), train acc: 0.000, val loss: 0.540, val acc: 0.000\n",
      "\n",
      "-----Epoch 149/200-----\n",
      "Loss: 0.43329861760139465 (0.005s), train acc: 0.000, val loss: 0.544, val acc: 0.000\n",
      "\n",
      "-----Epoch 150/200-----\n",
      "Loss: 0.4314171373844147 (0.005s), train acc: 0.000, val loss: 0.541, val acc: 0.000\n",
      "\n",
      "-----Epoch 151/200-----\n",
      "Loss: 0.4301636815071106 (0.005s), train acc: 0.000, val loss: 0.542, val acc: 0.000\n",
      "\n",
      "-----Epoch 152/200-----\n",
      "Loss: 0.42618125677108765 (0.005s), train acc: 0.000, val loss: 0.542, val acc: 0.000\n",
      "\n",
      "-----Epoch 153/200-----\n",
      "Loss: 0.42433789372444153 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 154/200-----\n",
      "Loss: 0.4221484363079071 (0.005s), train acc: 0.000, val loss: 0.545, val acc: 0.000\n",
      "\n",
      "-----Epoch 155/200-----\n",
      "Loss: 0.42138180136680603 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 156/200-----\n",
      "Loss: 0.42169541120529175 (0.005s), train acc: 0.000, val loss: 0.554, val acc: 0.000\n",
      "\n",
      "-----Epoch 157/200-----\n",
      "Loss: 0.4294910132884979 (0.005s), train acc: 0.000, val loss: 0.584, val acc: 0.000\n",
      "\n",
      "-----Epoch 158/200-----\n",
      "Loss: 0.4546399712562561 (0.005s), train acc: 0.000, val loss: 0.626, val acc: 0.000\n",
      "\n",
      "-----Epoch 159/200-----\n",
      "Loss: 0.5090110898017883 (0.005s), train acc: 0.000, val loss: 0.558, val acc: 0.000\n",
      "\n",
      "-----Epoch 160/200-----\n",
      "Loss: 0.44561120867729187 (0.005s), train acc: 0.000, val loss: 0.562, val acc: 0.000\n",
      "\n",
      "-----Epoch 161/200-----\n",
      "Loss: 0.4545392394065857 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 162/200-----\n",
      "Loss: 0.43731632828712463 (0.005s), train acc: 0.000, val loss: 0.565, val acc: 0.000\n",
      "\n",
      "-----Epoch 163/200-----\n",
      "Loss: 0.4525964558124542 (0.005s), train acc: 0.000, val loss: 0.554, val acc: 0.000\n",
      "\n",
      "-----Epoch 164/200-----\n",
      "Loss: 0.4352855682373047 (0.005s), train acc: 0.000, val loss: 0.563, val acc: 0.000\n",
      "\n",
      "-----Epoch 165/200-----\n",
      "Loss: 0.44559940695762634 (0.005s), train acc: 0.000, val loss: 0.536, val acc: 0.000\n",
      "\n",
      "-----Epoch 166/200-----\n",
      "Loss: 0.4306941330432892 (0.005s), train acc: 0.000, val loss: 0.532, val acc: 0.000\n",
      "\n",
      "-----Epoch 167/200-----\n",
      "Loss: 0.43426012992858887 (0.005s), train acc: 0.000, val loss: 0.529, val acc: 0.000\n",
      "\n",
      "-----Epoch 168/200-----\n",
      "Loss: 0.4369526207447052 (0.005s), train acc: 0.000, val loss: 0.526, val acc: 0.000\n",
      "\n",
      "-----Epoch 169/200-----\n",
      "Loss: 0.4327879548072815 (0.005s), train acc: 0.000, val loss: 0.528, val acc: 0.000\n",
      "\n",
      "-----Epoch 170/200-----\n",
      "Loss: 0.42985719442367554 (0.005s), train acc: 0.000, val loss: 0.534, val acc: 0.000\n",
      "\n",
      "-----Epoch 171/200-----\n",
      "Loss: 0.4276959002017975 (0.005s), train acc: 0.000, val loss: 0.538, val acc: 0.000\n",
      "\n",
      "-----Epoch 172/200-----\n",
      "Loss: 0.42407751083374023 (0.005s), train acc: 0.000, val loss: 0.543, val acc: 0.000\n",
      "\n",
      "-----Epoch 173/200-----\n",
      "Loss: 0.4231965243816376 (0.005s), train acc: 0.000, val loss: 0.549, val acc: 0.000\n",
      "\n",
      "-----Epoch 174/200-----\n",
      "Loss: 0.4250018298625946 (0.006s), train acc: 0.000, val loss: 0.549, val acc: 0.000\n",
      "\n",
      "-----Epoch 175/200-----\n",
      "Loss: 0.4204511344432831 (0.005s), train acc: 0.000, val loss: 0.549, val acc: 0.000\n",
      "\n",
      "-----Epoch 176/200-----\n",
      "Loss: 0.41762667894363403 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 177/200-----\n",
      "Loss: 0.4175449013710022 (0.005s), train acc: 0.000, val loss: 0.545, val acc: 0.000\n",
      "\n",
      "-----Epoch 178/200-----\n",
      "Loss: 0.4151461124420166 (0.005s), train acc: 0.000, val loss: 0.545, val acc: 0.000\n",
      "\n",
      "-----Epoch 179/200-----\n",
      "Loss: 0.4138718843460083 (0.005s), train acc: 0.000, val loss: 0.548, val acc: 0.000\n",
      "\n",
      "-----Epoch 180/200-----\n",
      "Loss: 0.4114486575126648 (0.005s), train acc: 0.000, val loss: 0.556, val acc: 0.000\n",
      "\n",
      "-----Epoch 181/200-----\n",
      "Loss: 0.41043397784233093 (0.005s), train acc: 0.000, val loss: 0.562, val acc: 0.000\n",
      "\n",
      "-----Epoch 182/200-----\n",
      "Loss: 0.4091475009918213 (0.005s), train acc: 0.000, val loss: 0.562, val acc: 0.000\n",
      "\n",
      "-----Epoch 183/200-----\n",
      "Loss: 0.4066426157951355 (0.005s), train acc: 0.000, val loss: 0.563, val acc: 0.000\n",
      "\n",
      "-----Epoch 184/200-----\n",
      "Loss: 0.4057827591896057 (0.005s), train acc: 0.000, val loss: 0.564, val acc: 0.000\n",
      "\n",
      "-----Epoch 185/200-----\n",
      "Loss: 0.4037950038909912 (0.005s), train acc: 0.000, val loss: 0.566, val acc: 0.000\n",
      "\n",
      "-----Epoch 186/200-----\n",
      "Loss: 0.40297213196754456 (0.005s), train acc: 0.000, val loss: 0.567, val acc: 0.000\n",
      "\n",
      "-----Epoch 187/200-----\n",
      "Loss: 0.401382714509964 (0.005s), train acc: 0.000, val loss: 0.571, val acc: 0.000\n",
      "\n",
      "-----Epoch 188/200-----\n",
      "Loss: 0.40021055936813354 (0.005s), train acc: 0.000, val loss: 0.574, val acc: 0.000\n",
      "\n",
      "-----Epoch 189/200-----\n",
      "Loss: 0.3983481228351593 (0.005s), train acc: 0.000, val loss: 0.577, val acc: 0.000\n",
      "\n",
      "-----Epoch 190/200-----\n",
      "Loss: 0.397887259721756 (0.005s), train acc: 0.000, val loss: 0.579, val acc: 0.000\n",
      "\n",
      "-----Epoch 191/200-----\n",
      "Loss: 0.39597994089126587 (0.005s), train acc: 0.000, val loss: 0.581, val acc: 0.000\n",
      "\n",
      "-----Epoch 192/200-----\n",
      "Loss: 0.3944895565509796 (0.005s), train acc: 0.000, val loss: 0.583, val acc: 0.000\n",
      "\n",
      "-----Epoch 193/200-----\n",
      "Loss: 0.3930202126502991 (0.005s), train acc: 0.000, val loss: 0.586, val acc: 0.000\n",
      "\n",
      "-----Epoch 194/200-----\n",
      "Loss: 0.39203181862831116 (0.006s), train acc: 0.000, val loss: 0.587, val acc: 0.000\n",
      "\n",
      "-----Epoch 195/200-----\n",
      "Loss: 0.3909258246421814 (0.005s), train acc: 0.000, val loss: 0.592, val acc: 0.000\n",
      "\n",
      "-----Epoch 196/200-----\n",
      "Loss: 0.39082014560699463 (0.005s), train acc: 0.000, val loss: 0.594, val acc: 0.000\n",
      "\n",
      "-----Epoch 197/200-----\n",
      "Loss: 0.3918468952178955 (0.005s), train acc: 0.000, val loss: 0.605, val acc: 0.000\n",
      "\n",
      "-----Epoch 198/200-----\n",
      "Loss: 0.3957020342350006 (0.005s), train acc: 0.000, val loss: 0.609, val acc: 0.000\n",
      "\n",
      "-----Epoch 199/200-----\n",
      "Loss: 0.40642017126083374 (0.005s), train acc: 0.000, val loss: 0.611, val acc: 0.000\n",
      "\n",
      "-----Epoch 200/200-----\n",
      "Loss: 0.4012809991836548 (0.005s), train acc: 0.000, val loss: 0.591, val acc: 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GATPPIModel(\n",
       "  (gat1): GATConv(50, 32, heads=4)\n",
       "  (gat2): GATConv(128, 128, heads=2)\n",
       "  (gat3): GATConv(256, 128, heads=2)\n",
       "  (gat4): GATConv(256, 121, heads=1)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.003\n",
    "epochs = 200\n",
    "s_model = GATPPIModel(num_feat=50, num_classes=121).to(DEVICE)\n",
    "optimizer = optim.Adam(s_model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_model_single_graph(s_model, optimizer, s_data, loss_fn, epochs, device=DEVICE)\n",
    "s_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2786fb60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "No learning rate scheduling!\n",
      "Training for 500 epochs, with batch size=32\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/500-----\n",
      "Batch 10/50, loss: 0.6936513245105743 (0.010s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.691608989238739 (0.010s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6950693368911743 (0.010s), train acc: 0.512\n",
      "Batch 40/50, loss: 0.6946593999862671 (0.010s), train acc: 0.507\n",
      "Batch 50/50, loss: 0.6941666007041931 (0.010s), train acc: 0.496\n",
      "\n",
      "-----Epoch 2/500-----\n",
      "Batch 10/50, loss: 0.6928510546684266 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6946166932582856 (0.009s), train acc: 0.509\n",
      "Batch 30/50, loss: 0.6935193419456482 (0.010s), train acc: 0.507\n",
      "Batch 40/50, loss: 0.6920548856258393 (0.010s), train acc: 0.513\n",
      "Batch 50/50, loss: 0.696242380142212 (0.010s), train acc: 0.499\n",
      "\n",
      "-----Epoch 3/500-----\n",
      "Batch 10/50, loss: 0.6933790266513824 (0.009s), train acc: 0.494\n",
      "Batch 20/50, loss: 0.692911809682846 (0.009s), train acc: 0.503\n",
      "Batch 30/50, loss: 0.6944271147251129 (0.009s), train acc: 0.493\n",
      "Batch 40/50, loss: 0.6929656445980072 (0.010s), train acc: 0.499\n",
      "Batch 50/50, loss: 0.693536227941513 (0.010s), train acc: 0.497\n",
      "\n",
      "-----Epoch 4/500-----\n",
      "Batch 10/50, loss: 0.6942972540855408 (0.009s), train acc: 0.450\n",
      "Batch 20/50, loss: 0.6933818101882935 (0.009s), train acc: 0.461\n",
      "Batch 30/50, loss: 0.6939841330051422 (0.010s), train acc: 0.466\n",
      "Batch 40/50, loss: 0.6938537418842315 (0.010s), train acc: 0.471\n",
      "Batch 50/50, loss: 0.6930356085300445 (0.010s), train acc: 0.476\n",
      "\n",
      "-----Epoch 5/500-----\n",
      "Batch 10/50, loss: 0.6931525707244873 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6933215677738189 (0.010s), train acc: 0.498\n",
      "Batch 30/50, loss: 0.6928447365760804 (0.010s), train acc: 0.510\n",
      "Batch 40/50, loss: 0.6932804346084595 (0.010s), train acc: 0.509\n",
      "Batch 50/50, loss: 0.693611866235733 (0.010s), train acc: 0.500\n",
      "\n",
      "-----Epoch 6/500-----\n",
      "Batch 10/50, loss: 0.6929699718952179 (0.009s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.6929180026054382 (0.010s), train acc: 0.512\n",
      "Batch 30/50, loss: 0.6937104880809783 (0.010s), train acc: 0.504\n",
      "Batch 40/50, loss: 0.6924081385135651 (0.010s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.6951840341091156 (0.010s), train acc: 0.501\n",
      "\n",
      "-----Epoch 7/500-----\n",
      "Batch 10/50, loss: 0.6937607645988464 (0.009s), train acc: 0.459\n",
      "Batch 20/50, loss: 0.6925166845321655 (0.009s), train acc: 0.495\n",
      "Batch 30/50, loss: 0.6936905026435852 (0.010s), train acc: 0.494\n",
      "Batch 40/50, loss: 0.6930693030357361 (0.010s), train acc: 0.498\n",
      "Batch 50/50, loss: 0.6942204892635345 (0.010s), train acc: 0.491\n",
      "\n",
      "-----Epoch 8/500-----\n",
      "Batch 10/50, loss: 0.693236666917801 (0.009s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.6931722939014435 (0.009s), train acc: 0.503\n",
      "Batch 30/50, loss: 0.6932976961135864 (0.009s), train acc: 0.507\n",
      "Batch 40/50, loss: 0.6932525038719177 (0.010s), train acc: 0.499\n",
      "Batch 50/50, loss: 0.6930330097675323 (0.010s), train acc: 0.506\n",
      "\n",
      "-----Epoch 9/500-----\n",
      "Batch 10/50, loss: 0.6928200542926788 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6931583404541015 (0.010s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.693183547258377 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6935983180999756 (0.009s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.6934057831764221 (0.010s), train acc: 0.506\n",
      "\n",
      "-----Epoch 10/500-----\n",
      "Batch 10/50, loss: 0.6925048410892487 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6941857695579529 (0.009s), train acc: 0.503\n",
      "Batch 30/50, loss: 0.6935737907886506 (0.010s), train acc: 0.494\n",
      "Batch 40/50, loss: 0.6933077335357666 (0.010s), train acc: 0.492\n",
      "Batch 50/50, loss: 0.6930571258068084 (0.010s), train acc: 0.494\n",
      "\n",
      "-----Epoch 11/500-----\n",
      "Batch 10/50, loss: 0.6931870341300964 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6940822720527648 (0.009s), train acc: 0.487\n",
      "Batch 30/50, loss: 0.6928271889686585 (0.010s), train acc: 0.497\n",
      "Batch 40/50, loss: 0.6933663725852967 (0.010s), train acc: 0.494\n",
      "Batch 50/50, loss: 0.6935793101787567 (0.010s), train acc: 0.496\n",
      "\n",
      "-----Epoch 12/500-----\n",
      "Batch 10/50, loss: 0.6931511163711548 (0.009s), train acc: 0.494\n",
      "Batch 20/50, loss: 0.69286607503891 (0.009s), train acc: 0.516\n",
      "Batch 30/50, loss: 0.6934315145015717 (0.009s), train acc: 0.515\n",
      "Batch 40/50, loss: 0.6935640156269074 (0.010s), train acc: 0.504\n",
      "Batch 50/50, loss: 0.6930574655532837 (0.010s), train acc: 0.507\n",
      "\n",
      "-----Epoch 13/500-----\n",
      "Batch 10/50, loss: 0.6930987238883972 (0.009s), train acc: 0.478\n",
      "Batch 20/50, loss: 0.6930159389972687 (0.009s), train acc: 0.492\n",
      "Batch 30/50, loss: 0.6928096175193786 (0.010s), train acc: 0.499\n",
      "Batch 40/50, loss: 0.6942890822887421 (0.010s), train acc: 0.491\n",
      "Batch 50/50, loss: 0.6930303394794464 (0.010s), train acc: 0.485\n",
      "\n",
      "-----Epoch 14/500-----\n",
      "Batch 10/50, loss: 0.693547660112381 (0.009s), train acc: 0.481\n",
      "Batch 20/50, loss: 0.692852008342743 (0.009s), train acc: 0.509\n",
      "Batch 30/50, loss: 0.691808432340622 (0.010s), train acc: 0.523\n",
      "Batch 40/50, loss: 0.693714702129364 (0.010s), train acc: 0.514\n",
      "Batch 50/50, loss: 0.6934353232383728 (0.010s), train acc: 0.509\n",
      "\n",
      "-----Epoch 15/500-----\n",
      "Batch 10/50, loss: 0.693076628446579 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6930872440338135 (0.009s), train acc: 0.506\n",
      "Batch 30/50, loss: 0.692820930480957 (0.010s), train acc: 0.514\n",
      "Batch 40/50, loss: 0.6936823010444642 (0.010s), train acc: 0.501\n",
      "Batch 50/50, loss: 0.6931404232978821 (0.010s), train acc: 0.499\n",
      "\n",
      "-----Epoch 16/500-----\n",
      "Batch 10/50, loss: 0.6928004384040832 (0.009s), train acc: 0.500\n",
      "Batch 20/50, loss: 0.6945485174655914 (0.009s), train acc: 0.492\n",
      "Batch 30/50, loss: 0.6931438982486725 (0.010s), train acc: 0.497\n",
      "Batch 40/50, loss: 0.6935918271541596 (0.010s), train acc: 0.490\n",
      "Batch 50/50, loss: 0.6928889393806458 (0.010s), train acc: 0.499\n",
      "\n",
      "-----Epoch 17/500-----\n",
      "Batch 10/50, loss: 0.6924709558486939 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.692817884683609 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6931496083736419 (0.009s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6930066347122192 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6937653481960296 (0.010s), train acc: 0.514\n",
      "\n",
      "-----Epoch 18/500-----\n",
      "Batch 10/50, loss: 0.6927401721477509 (0.009s), train acc: 0.500\n",
      "Batch 20/50, loss: 0.6928167283535004 (0.009s), train acc: 0.506\n",
      "Batch 30/50, loss: 0.69277263879776 (0.009s), train acc: 0.511\n",
      "Batch 40/50, loss: 0.6932995438575744 (0.009s), train acc: 0.505\n",
      "Batch 50/50, loss: 0.6940759599208832 (0.010s), train acc: 0.499\n",
      "\n",
      "-----Epoch 19/500-----\n",
      "Batch 10/50, loss: 0.6924462378025055 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6920925438404083 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6930345475673676 (0.009s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6940095901489258 (0.010s), train acc: 0.524\n",
      "Batch 50/50, loss: 0.6942885160446167 (0.010s), train acc: 0.513\n",
      "\n",
      "-----Epoch 20/500-----\n",
      "Batch 10/50, loss: 0.6929771363735199 (0.009s), train acc: 0.469\n",
      "Batch 20/50, loss: 0.6936168313026428 (0.009s), train acc: 0.477\n",
      "Batch 30/50, loss: 0.6925840675830841 (0.010s), train acc: 0.490\n",
      "Batch 40/50, loss: 0.6931088149547577 (0.010s), train acc: 0.503\n",
      "Batch 50/50, loss: 0.693074768781662 (0.010s), train acc: 0.506\n",
      "\n",
      "-----Epoch 21/500-----\n",
      "Batch 10/50, loss: 0.6925236105918884 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.692919921875 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6935049295425415 (0.010s), train acc: 0.511\n",
      "Batch 40/50, loss: 0.6927009642124176 (0.009s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.6934122264385223 (0.010s), train acc: 0.506\n",
      "\n",
      "-----Epoch 22/500-----\n",
      "Batch 10/50, loss: 0.6929612219333648 (0.009s), train acc: 0.472\n",
      "Batch 20/50, loss: 0.6929464936256409 (0.010s), train acc: 0.492\n",
      "Batch 30/50, loss: 0.6928310215473175 (0.010s), train acc: 0.502\n",
      "Batch 40/50, loss: 0.6930788576602935 (0.010s), train acc: 0.497\n",
      "Batch 50/50, loss: 0.6926802635192871 (0.010s), train acc: 0.491\n",
      "\n",
      "-----Epoch 23/500-----\n",
      "Batch 10/50, loss: 0.6918575823307037 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6932292878627777 (0.010s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6933650016784668 (0.010s), train acc: 0.519\n",
      "Batch 40/50, loss: 0.6934647858142853 (0.010s), train acc: 0.501\n",
      "Batch 50/50, loss: 0.693578588962555 (0.010s), train acc: 0.501\n",
      "\n",
      "-----Epoch 24/500-----\n",
      "Batch 10/50, loss: 0.6927092432975769 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6924522161483765 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6925761640071869 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6938396096229553 (0.010s), train acc: 0.520\n",
      "Batch 50/50, loss: 0.6939592659473419 (0.010s), train acc: 0.510\n",
      "\n",
      "-----Epoch 25/500-----\n",
      "Batch 10/50, loss: 0.6936113655567169 (0.009s), train acc: 0.475\n",
      "Batch 20/50, loss: 0.6922236800193786 (0.009s), train acc: 0.495\n",
      "Batch 30/50, loss: 0.6933767616748809 (0.009s), train acc: 0.496\n",
      "Batch 40/50, loss: 0.6933115959167481 (0.009s), train acc: 0.496\n",
      "Batch 50/50, loss: 0.6930647611618042 (0.010s), train acc: 0.498\n",
      "\n",
      "-----Epoch 26/500-----\n",
      "Batch 10/50, loss: 0.6927313208580017 (0.009s), train acc: 0.497\n",
      "Batch 20/50, loss: 0.6929128170013428 (0.009s), train acc: 0.495\n",
      "Batch 30/50, loss: 0.6929208040237427 (0.010s), train acc: 0.496\n",
      "Batch 40/50, loss: 0.6917651116847991 (0.010s), train acc: 0.515\n",
      "Batch 50/50, loss: 0.6941657841205597 (0.010s), train acc: 0.510\n",
      "\n",
      "-----Epoch 27/500-----\n",
      "Batch 10/50, loss: 0.6936552226543427 (0.009s), train acc: 0.494\n",
      "Batch 20/50, loss: 0.6931409537792206 (0.009s), train acc: 0.502\n",
      "Batch 30/50, loss: 0.6930065870285034 (0.009s), train acc: 0.502\n",
      "Batch 40/50, loss: 0.692983740568161 (0.010s), train acc: 0.499\n",
      "Batch 50/50, loss: 0.6939105570316315 (0.010s), train acc: 0.492\n",
      "\n",
      "-----Epoch 28/500-----\n",
      "Batch 10/50, loss: 0.6929554164409637 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6929889917373657 (0.009s), train acc: 0.523\n",
      "Batch 30/50, loss: 0.6931057870388031 (0.009s), train acc: 0.512\n",
      "Batch 40/50, loss: 0.6929096698760986 (0.009s), train acc: 0.510\n",
      "Batch 50/50, loss: 0.692113846540451 (0.010s), train acc: 0.515\n",
      "\n",
      "-----Epoch 29/500-----\n",
      "Batch 10/50, loss: 0.6924301981925964 (0.009s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.6923700571060181 (0.009s), train acc: 0.511\n",
      "Batch 30/50, loss: 0.6948831975460052 (0.009s), train acc: 0.497\n",
      "Batch 40/50, loss: 0.6930466771125794 (0.010s), train acc: 0.497\n",
      "Batch 50/50, loss: 0.6936191856861115 (0.010s), train acc: 0.494\n",
      "\n",
      "-----Epoch 30/500-----\n",
      "Batch 10/50, loss: 0.6920459806919098 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6925627708435058 (0.010s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6928748846054077 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6931232273578644 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6943412601947785 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 31/500-----\n",
      "Batch 10/50, loss: 0.6926879227161408 (0.009s), train acc: 0.475\n",
      "Batch 20/50, loss: 0.6932156383991241 (0.009s), train acc: 0.486\n",
      "Batch 30/50, loss: 0.6932015478610992 (0.009s), train acc: 0.490\n",
      "Batch 40/50, loss: 0.693728232383728 (0.010s), train acc: 0.486\n",
      "Batch 50/50, loss: 0.6930530726909637 (0.010s), train acc: 0.484\n",
      "\n",
      "-----Epoch 32/500-----\n",
      "Batch 10/50, loss: 0.6924358427524566 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6919708490371704 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.692786705493927 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6915005087852478 (0.009s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6963952839374542 (0.010s), train acc: 0.513\n",
      "\n",
      "-----Epoch 33/500-----\n",
      "Batch 10/50, loss: 0.6941141247749328 (0.009s), train acc: 0.481\n",
      "Batch 20/50, loss: 0.6932094156742096 (0.009s), train acc: 0.487\n",
      "Batch 30/50, loss: 0.6937254071235657 (0.009s), train acc: 0.481\n",
      "Batch 40/50, loss: 0.6907879054546356 (0.010s), train acc: 0.499\n",
      "Batch 50/50, loss: 0.6956641018390656 (0.010s), train acc: 0.491\n",
      "\n",
      "-----Epoch 34/500-----\n",
      "Batch 10/50, loss: 0.6925286233425141 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6923213005065918 (0.009s), train acc: 0.508\n",
      "Batch 30/50, loss: 0.6928697884082794 (0.010s), train acc: 0.503\n",
      "Batch 40/50, loss: 0.6934574246406555 (0.010s), train acc: 0.501\n",
      "Batch 50/50, loss: 0.6937714159488678 (0.010s), train acc: 0.497\n",
      "\n",
      "-----Epoch 35/500-----\n",
      "Batch 10/50, loss: 0.6912518680095673 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6925314664840698 (0.009s), train acc: 0.498\n",
      "Batch 30/50, loss: 0.6930349349975586 (0.009s), train acc: 0.503\n",
      "Batch 40/50, loss: 0.692670214176178 (0.009s), train acc: 0.497\n",
      "Batch 50/50, loss: 0.6941322565078736 (0.010s), train acc: 0.492\n",
      "\n",
      "-----Epoch 36/500-----\n",
      "Batch 10/50, loss: 0.6926491498947144 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6917108118534088 (0.009s), train acc: 0.503\n",
      "Batch 30/50, loss: 0.6931548774242401 (0.009s), train acc: 0.496\n",
      "Batch 40/50, loss: 0.6942043840885163 (0.010s), train acc: 0.493\n",
      "Batch 50/50, loss: 0.6921461284160614 (0.010s), train acc: 0.497\n",
      "\n",
      "-----Epoch 37/500-----\n",
      "Batch 10/50, loss: 0.6928724229335785 (0.009s), train acc: 0.491\n",
      "Batch 20/50, loss: 0.6932493805885315 (0.009s), train acc: 0.483\n",
      "Batch 30/50, loss: 0.6920127093791961 (0.010s), train acc: 0.502\n",
      "Batch 40/50, loss: 0.6920501470565796 (0.010s), train acc: 0.506\n",
      "Batch 50/50, loss: 0.6927609443664551 (0.010s), train acc: 0.505\n",
      "\n",
      "-----Epoch 38/500-----\n",
      "Batch 10/50, loss: 0.6931268095970153 (0.009s), train acc: 0.497\n",
      "Batch 20/50, loss: 0.6936540186405182 (0.009s), train acc: 0.502\n",
      "Batch 30/50, loss: 0.6915511071681977 (0.009s), train acc: 0.514\n",
      "Batch 40/50, loss: 0.6927749156951905 (0.010s), train acc: 0.506\n",
      "Batch 50/50, loss: 0.6922220289707184 (0.010s), train acc: 0.517\n",
      "\n",
      "-----Epoch 39/500-----\n",
      "Batch 10/50, loss: 0.6915905475616455 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6923734009265899 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.692232072353363 (0.009s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.69294975399971 (0.010s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6943807601928711 (0.010s), train acc: 0.518\n",
      "\n",
      "-----Epoch 40/500-----\n",
      "Batch 10/50, loss: 0.6926925718784332 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6935286164283753 (0.009s), train acc: 0.506\n",
      "Batch 30/50, loss: 0.6952591121196747 (0.009s), train acc: 0.491\n",
      "Batch 40/50, loss: 0.6922208070755005 (0.010s), train acc: 0.492\n",
      "Batch 50/50, loss: 0.6922361671924591 (0.010s), train acc: 0.492\n",
      "\n",
      "-----Epoch 41/500-----\n",
      "Batch 10/50, loss: 0.6929415404796601 (0.009s), train acc: 0.487\n",
      "Batch 20/50, loss: 0.6921151757240296 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6920453786849976 (0.010s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6925945103168487 (0.010s), train acc: 0.511\n",
      "Batch 50/50, loss: 0.6925290524959564 (0.010s), train acc: 0.513\n",
      "\n",
      "-----Epoch 42/500-----\n",
      "Batch 10/50, loss: 0.6919531106948853 (0.009s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.6920317530632019 (0.009s), train acc: 0.520\n",
      "Batch 30/50, loss: 0.6944449067115783 (0.010s), train acc: 0.509\n",
      "Batch 40/50, loss: 0.6919315814971924 (0.010s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.6925542831420899 (0.010s), train acc: 0.509\n",
      "\n",
      "-----Epoch 43/500-----\n",
      "Batch 10/50, loss: 0.6911137640476227 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6952113151550293 (0.009s), train acc: 0.520\n",
      "Batch 30/50, loss: 0.6932005882263184 (0.010s), train acc: 0.511\n",
      "Batch 40/50, loss: 0.6925980448722839 (0.010s), train acc: 0.507\n",
      "Batch 50/50, loss: 0.6897962868213654 (0.010s), train acc: 0.522\n",
      "\n",
      "-----Epoch 44/500-----\n",
      "Batch 10/50, loss: 0.6915926694869995 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6923048555850982 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6921183347702027 (0.009s), train acc: 0.512\n",
      "Batch 40/50, loss: 0.692688262462616 (0.010s), train acc: 0.510\n",
      "Batch 50/50, loss: 0.6928061723709107 (0.010s), train acc: 0.512\n",
      "\n",
      "-----Epoch 45/500-----\n",
      "Batch 10/50, loss: 0.6915283858776092 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6920142412185669 (0.009s), train acc: 0.505\n",
      "Batch 30/50, loss: 0.6938892841339112 (0.010s), train acc: 0.505\n",
      "Batch 40/50, loss: 0.6918648183345795 (0.010s), train acc: 0.502\n",
      "Batch 50/50, loss: 0.6925472438335418 (0.010s), train acc: 0.499\n",
      "\n",
      "-----Epoch 46/500-----\n",
      "Batch 10/50, loss: 0.6924314737319947 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6923800945281983 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6928044497966767 (0.010s), train acc: 0.518\n",
      "Batch 40/50, loss: 0.6930113792419433 (0.009s), train acc: 0.515\n",
      "Batch 50/50, loss: 0.6934419870376587 (0.010s), train acc: 0.504\n",
      "\n",
      "-----Epoch 47/500-----\n",
      "Batch 10/50, loss: 0.6917161285877228 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6937552273273468 (0.010s), train acc: 0.506\n",
      "Batch 30/50, loss: 0.6918676316738128 (0.010s), train acc: 0.515\n",
      "Batch 40/50, loss: 0.6909118950366974 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.693462073802948 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 48/500-----\n",
      "Batch 10/50, loss: 0.6925530552864074 (0.009s), train acc: 0.519\n",
      "Batch 20/50, loss: 0.6931680917739869 (0.009s), train acc: 0.514\n",
      "Batch 30/50, loss: 0.6935868620872497 (0.010s), train acc: 0.503\n",
      "Batch 40/50, loss: 0.6907653987407685 (0.009s), train acc: 0.506\n",
      "Batch 50/50, loss: 0.6923340141773224 (0.010s), train acc: 0.508\n",
      "\n",
      "-----Epoch 49/500-----\n",
      "Batch 10/50, loss: 0.6914377987384797 (0.009s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.6915498793125152 (0.009s), train acc: 0.525\n",
      "Batch 30/50, loss: 0.6926568925380707 (0.010s), train acc: 0.524\n",
      "Batch 40/50, loss: 0.6935283124446869 (0.010s), train acc: 0.517\n",
      "Batch 50/50, loss: 0.6941050529479981 (0.010s), train acc: 0.513\n",
      "\n",
      "-----Epoch 50/500-----\n",
      "Batch 10/50, loss: 0.691685575246811 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6920934915542603 (0.009s), train acc: 0.516\n",
      "Batch 30/50, loss: 0.6934916973114014 (0.010s), train acc: 0.505\n",
      "Batch 40/50, loss: 0.6909678339958191 (0.010s), train acc: 0.510\n",
      "Batch 50/50, loss: 0.6932000339031219 (0.010s), train acc: 0.509\n",
      "\n",
      "-----Epoch 51/500-----\n",
      "Batch 10/50, loss: 0.6900149822235108 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6945135831832886 (0.010s), train acc: 0.517\n",
      "Batch 30/50, loss: 0.6929087996482849 (0.010s), train acc: 0.514\n",
      "Batch 40/50, loss: 0.6920509159564971 (0.010s), train acc: 0.515\n",
      "Batch 50/50, loss: 0.6918538451194763 (0.010s), train acc: 0.518\n",
      "\n",
      "-----Epoch 52/500-----\n",
      "Batch 10/50, loss: 0.693587851524353 (0.009s), train acc: 0.469\n",
      "Batch 20/50, loss: 0.6921925365924835 (0.009s), train acc: 0.487\n",
      "Batch 30/50, loss: 0.6916455805301667 (0.010s), train acc: 0.489\n",
      "Batch 40/50, loss: 0.692098593711853 (0.010s), train acc: 0.491\n",
      "Batch 50/50, loss: 0.6922218263149261 (0.010s), train acc: 0.497\n",
      "\n",
      "-----Epoch 53/500-----\n",
      "Batch 10/50, loss: 0.6897652745246887 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6932456135749817 (0.009s), train acc: 0.522\n",
      "Batch 30/50, loss: 0.691414600610733 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6930222272872925 (0.010s), train acc: 0.511\n",
      "Batch 50/50, loss: 0.6934775233268737 (0.010s), train acc: 0.509\n",
      "\n",
      "-----Epoch 54/500-----\n",
      "Batch 10/50, loss: 0.6919698119163513 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6904497563838958 (0.009s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6911811113357544 (0.009s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6922078907489777 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6947666943073273 (0.010s), train acc: 0.524\n",
      "\n",
      "-----Epoch 55/500-----\n",
      "Batch 10/50, loss: 0.6917498052120209 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6926210045814514 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6940650463104248 (0.010s), train acc: 0.520\n",
      "Batch 40/50, loss: 0.6895433902740479 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6918402493000031 (0.010s), train acc: 0.526\n",
      "\n",
      "-----Epoch 56/500-----\n",
      "Batch 10/50, loss: 0.6903197646141053 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6929820716381073 (0.009s), train acc: 0.506\n",
      "Batch 30/50, loss: 0.6939800083637238 (0.010s), train acc: 0.502\n",
      "Batch 40/50, loss: 0.6915019631385804 (0.010s), train acc: 0.507\n",
      "Batch 50/50, loss: 0.6901205360889435 (0.010s), train acc: 0.516\n",
      "\n",
      "-----Epoch 57/500-----\n",
      "Batch 10/50, loss: 0.6922236919403076 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.689431095123291 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6926843583583832 (0.010s), train acc: 0.515\n",
      "Batch 40/50, loss: 0.6940865993499756 (0.010s), train acc: 0.504\n",
      "Batch 50/50, loss: 0.691006988286972 (0.010s), train acc: 0.508\n",
      "\n",
      "-----Epoch 58/500-----\n",
      "Batch 10/50, loss: 0.6931233823299408 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6910008609294891 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6915529429912567 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6939695179462433 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.690740430355072 (0.010s), train acc: 0.522\n",
      "\n",
      "-----Epoch 59/500-----\n",
      "Batch 10/50, loss: 0.6926917970180512 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.6927667021751404 (0.009s), train acc: 0.508\n",
      "Batch 30/50, loss: 0.6918261349201202 (0.010s), train acc: 0.504\n",
      "Batch 40/50, loss: 0.689795869588852 (0.010s), train acc: 0.506\n",
      "Batch 50/50, loss: 0.6921875894069671 (0.010s), train acc: 0.509\n",
      "\n",
      "-----Epoch 60/500-----\n",
      "Batch 10/50, loss: 0.6874765276908874 (0.009s), train acc: 0.584\n",
      "Batch 20/50, loss: 0.6919814467430114 (0.009s), train acc: 0.556\n",
      "Batch 30/50, loss: 0.6898513615131379 (0.010s), train acc: 0.552\n",
      "Batch 40/50, loss: 0.6925540089607238 (0.009s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6978477835655212 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 61/500-----\n",
      "Batch 10/50, loss: 0.692545884847641 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6910718023777008 (0.009s), train acc: 0.520\n",
      "Batch 30/50, loss: 0.6919866740703583 (0.010s), train acc: 0.516\n",
      "Batch 40/50, loss: 0.6923099279403686 (0.010s), train acc: 0.511\n",
      "Batch 50/50, loss: 0.6910074532032013 (0.010s), train acc: 0.512\n",
      "\n",
      "-----Epoch 62/500-----\n",
      "Batch 10/50, loss: 0.6907022297382355 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6905223369598389 (0.009s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6950007081031799 (0.009s), train acc: 0.508\n",
      "Batch 40/50, loss: 0.6938179194927215 (0.009s), train acc: 0.504\n",
      "Batch 50/50, loss: 0.6878960072994232 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 63/500-----\n",
      "Batch 10/50, loss: 0.6935619592666626 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6916946291923523 (0.009s), train acc: 0.505\n",
      "Batch 30/50, loss: 0.6905063509941101 (0.010s), train acc: 0.515\n",
      "Batch 40/50, loss: 0.6915762841701507 (0.010s), train acc: 0.518\n",
      "Batch 50/50, loss: 0.6934267163276673 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 64/500-----\n",
      "Batch 10/50, loss: 0.6898317396640777 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6937252819538117 (0.009s), train acc: 0.516\n",
      "Batch 30/50, loss: 0.6876691818237305 (0.009s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.691965788602829 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6955433189868927 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 65/500-----\n",
      "Batch 10/50, loss: 0.6900554776191712 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6918801367282867 (0.009s), train acc: 0.523\n",
      "Batch 30/50, loss: 0.6919975936412811 (0.009s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6938308775424957 (0.010s), train acc: 0.511\n",
      "Batch 50/50, loss: 0.6899089395999909 (0.010s), train acc: 0.514\n",
      "\n",
      "-----Epoch 66/500-----\n",
      "Batch 10/50, loss: 0.6940052270889282 (0.009s), train acc: 0.500\n",
      "Batch 20/50, loss: 0.6884300887584687 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6925460398197174 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6910332500934601 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6920287370681762 (0.010s), train acc: 0.521\n",
      "\n",
      "-----Epoch 67/500-----\n",
      "Batch 10/50, loss: 0.6904045045375824 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6893760621547699 (0.009s), train acc: 0.509\n",
      "Batch 30/50, loss: 0.692960649728775 (0.009s), train acc: 0.507\n",
      "Batch 40/50, loss: 0.6920782446861267 (0.010s), train acc: 0.503\n",
      "Batch 50/50, loss: 0.6916097521781921 (0.010s), train acc: 0.508\n",
      "\n",
      "-----Epoch 68/500-----\n",
      "Batch 10/50, loss: 0.6919487357139588 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6898343563079834 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6880460500717163 (0.010s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.6959943175315857 (0.010s), train acc: 0.525\n",
      "Batch 50/50, loss: 0.6918825924396514 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 69/500-----\n",
      "Batch 10/50, loss: 0.688565468788147 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6910306274890899 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6921098291873932 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6942993521690368 (0.010s), train acc: 0.521\n",
      "Batch 50/50, loss: 0.6912984073162078 (0.010s), train acc: 0.521\n",
      "\n",
      "-----Epoch 70/500-----\n",
      "Batch 10/50, loss: 0.689202469587326 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.69464151263237 (0.009s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.6903359293937683 (0.009s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6912935972213745 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6956894993782043 (0.010s), train acc: 0.518\n",
      "\n",
      "-----Epoch 71/500-----\n",
      "Batch 10/50, loss: 0.6901104867458343 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6920440495014191 (0.009s), train acc: 0.514\n",
      "Batch 30/50, loss: 0.6923834443092346 (0.009s), train acc: 0.512\n",
      "Batch 40/50, loss: 0.6929110169410706 (0.010s), train acc: 0.502\n",
      "Batch 50/50, loss: 0.688546335697174 (0.010s), train acc: 0.515\n",
      "\n",
      "-----Epoch 72/500-----\n",
      "Batch 10/50, loss: 0.6914983749389648 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6899455785751343 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6936138987541198 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6911052823066711 (0.009s), train acc: 0.528\n",
      "Batch 50/50, loss: 0.6923573672771454 (0.010s), train acc: 0.521\n",
      "\n",
      "-----Epoch 73/500-----\n",
      "Batch 10/50, loss: 0.6906229615211487 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6929192662239074 (0.009s), train acc: 0.511\n",
      "Batch 30/50, loss: 0.6887034177780151 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6915053606033326 (0.009s), train acc: 0.526\n",
      "Batch 50/50, loss: 0.693156361579895 (0.010s), train acc: 0.518\n",
      "\n",
      "-----Epoch 74/500-----\n",
      "Batch 10/50, loss: 0.6942050635814667 (0.009s), train acc: 0.475\n",
      "Batch 20/50, loss: 0.6913000345230103 (0.009s), train acc: 0.480\n",
      "Batch 30/50, loss: 0.6908552706241607 (0.010s), train acc: 0.492\n",
      "Batch 40/50, loss: 0.6884653329849243 (0.010s), train acc: 0.503\n",
      "Batch 50/50, loss: 0.6916887760162354 (0.010s), train acc: 0.499\n",
      "\n",
      "-----Epoch 75/500-----\n",
      "Batch 10/50, loss: 0.6923754870891571 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6916819274425506 (0.009s), train acc: 0.522\n",
      "Batch 30/50, loss: 0.6919208645820618 (0.010s), train acc: 0.520\n",
      "Batch 40/50, loss: 0.6858625829219818 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6940921366214752 (0.010s), train acc: 0.525\n",
      "\n",
      "-----Epoch 76/500-----\n",
      "Batch 10/50, loss: 0.6886307179927826 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6952255010604859 (0.009s), train acc: 0.514\n",
      "Batch 30/50, loss: 0.689229816198349 (0.010s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6903083503246308 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6915976345539093 (0.010s), train acc: 0.515\n",
      "\n",
      "-----Epoch 77/500-----\n",
      "Batch 10/50, loss: 0.6905030727386474 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6914244651794433 (0.009s), train acc: 0.512\n",
      "Batch 30/50, loss: 0.6940102994441986 (0.010s), train acc: 0.509\n",
      "Batch 40/50, loss: 0.6904836297035217 (0.010s), train acc: 0.510\n",
      "Batch 50/50, loss: 0.6891602218151093 (0.010s), train acc: 0.518\n",
      "\n",
      "-----Epoch 78/500-----\n",
      "Batch 10/50, loss: 0.6881024301052093 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6935161888599396 (0.010s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6885150849819184 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6949579238891601 (0.010s), train acc: 0.522\n",
      "Batch 50/50, loss: 0.6896588623523712 (0.010s), train acc: 0.525\n",
      "\n",
      "-----Epoch 79/500-----\n",
      "Batch 10/50, loss: 0.6888145208358765 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6929238021373749 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6933880031108857 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6903259336948395 (0.010s), train acc: 0.519\n",
      "Batch 50/50, loss: 0.6903350234031678 (0.010s), train acc: 0.521\n",
      "\n",
      "-----Epoch 80/500-----\n",
      "Batch 10/50, loss: 0.6927592396736145 (0.009s), train acc: 0.481\n",
      "Batch 20/50, loss: 0.6905028223991394 (0.009s), train acc: 0.483\n",
      "Batch 30/50, loss: 0.6920984029769898 (0.010s), train acc: 0.495\n",
      "Batch 40/50, loss: 0.6927221715450287 (0.010s), train acc: 0.500\n",
      "Batch 50/50, loss: 0.6867839992046356 (0.010s), train acc: 0.504\n",
      "\n",
      "-----Epoch 81/500-----\n",
      "Batch 10/50, loss: 0.6891605436801911 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6963156819343567 (0.009s), train acc: 0.486\n",
      "Batch 30/50, loss: 0.6906764805316925 (0.010s), train acc: 0.501\n",
      "Batch 40/50, loss: 0.6903277099132538 (0.010s), train acc: 0.505\n",
      "Batch 50/50, loss: 0.6927037715911866 (0.010s), train acc: 0.509\n",
      "\n",
      "-----Epoch 82/500-----\n",
      "Batch 10/50, loss: 0.6922396898269654 (0.009s), train acc: 0.484\n",
      "Batch 20/50, loss: 0.6952307760715485 (0.009s), train acc: 0.484\n",
      "Batch 30/50, loss: 0.6913167655467987 (0.010s), train acc: 0.491\n",
      "Batch 40/50, loss: 0.6862150967121124 (0.010s), train acc: 0.505\n",
      "Batch 50/50, loss: 0.6921877443790436 (0.010s), train acc: 0.509\n",
      "\n",
      "-----Epoch 83/500-----\n",
      "Batch 10/50, loss: 0.692728590965271 (0.009s), train acc: 0.500\n",
      "Batch 20/50, loss: 0.688877409696579 (0.009s), train acc: 0.514\n",
      "Batch 30/50, loss: 0.6927544176578522 (0.009s), train acc: 0.514\n",
      "Batch 40/50, loss: 0.6927862882614135 (0.010s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.6881970763206482 (0.010s), train acc: 0.516\n",
      "\n",
      "-----Epoch 84/500-----\n",
      "Batch 10/50, loss: 0.691199517250061 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6905198872089386 (0.009s), train acc: 0.508\n",
      "Batch 30/50, loss: 0.6919126152992249 (0.010s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.6925474643707276 (0.010s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.6884643912315369 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 85/500-----\n",
      "Batch 10/50, loss: 0.6881208777427673 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.689018714427948 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6954801321029663 (0.009s), train acc: 0.523\n",
      "Batch 40/50, loss: 0.6887479603290558 (0.010s), train acc: 0.525\n",
      "Batch 50/50, loss: 0.6925407171249389 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 86/500-----\n",
      "Batch 10/50, loss: 0.6936604917049408 (0.009s), train acc: 0.475\n",
      "Batch 20/50, loss: 0.6933892250061036 (0.009s), train acc: 0.489\n",
      "Batch 30/50, loss: 0.6873361170291901 (0.009s), train acc: 0.504\n",
      "Batch 40/50, loss: 0.6913308203220367 (0.010s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.6875196158885956 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 87/500-----\n",
      "Batch 10/50, loss: 0.6887773096561431 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6935680449008942 (0.010s), train acc: 0.520\n",
      "Batch 30/50, loss: 0.6932181537151336 (0.010s), train acc: 0.511\n",
      "Batch 40/50, loss: 0.685122936964035 (0.010s), train acc: 0.514\n",
      "Batch 50/50, loss: 0.6920406281948089 (0.010s), train acc: 0.512\n",
      "\n",
      "-----Epoch 88/500-----\n",
      "Batch 10/50, loss: 0.6946619391441345 (0.009s), train acc: 0.491\n",
      "Batch 20/50, loss: 0.6917694211006165 (0.009s), train acc: 0.502\n",
      "Batch 30/50, loss: 0.6889283955097198 (0.010s), train acc: 0.515\n",
      "Batch 40/50, loss: 0.6895946085453033 (0.010s), train acc: 0.520\n",
      "Batch 50/50, loss: 0.6912220418453217 (0.010s), train acc: 0.519\n",
      "\n",
      "-----Epoch 89/500-----\n",
      "Batch 10/50, loss: 0.6860486149787903 (0.009s), train acc: 0.575\n",
      "Batch 20/50, loss: 0.6909432768821716 (0.010s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6908550918102264 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6887039482593537 (0.009s), train acc: 0.524\n",
      "Batch 50/50, loss: 0.6955822885036469 (0.010s), train acc: 0.521\n",
      "\n",
      "-----Epoch 90/500-----\n",
      "Batch 10/50, loss: 0.6877954006195068 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6881520807743072 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6919278979301453 (0.009s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6895713567733764 (0.010s), train acc: 0.524\n",
      "Batch 50/50, loss: 0.6944468319416046 (0.010s), train acc: 0.512\n",
      "\n",
      "-----Epoch 91/500-----\n",
      "Batch 10/50, loss: 0.6909536302089692 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6894531309604645 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6936352968215942 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6920197129249572 (0.010s), train acc: 0.524\n",
      "Batch 50/50, loss: 0.6858581900596619 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 92/500-----\n",
      "Batch 10/50, loss: 0.6927550613880158 (0.009s), train acc: 0.481\n",
      "Batch 20/50, loss: 0.6928128838539124 (0.009s), train acc: 0.484\n",
      "Batch 30/50, loss: 0.689357978105545 (0.009s), train acc: 0.512\n",
      "Batch 40/50, loss: 0.6895499348640441 (0.010s), train acc: 0.517\n",
      "Batch 50/50, loss: 0.6904083907604217 (0.010s), train acc: 0.518\n",
      "\n",
      "-----Epoch 93/500-----\n",
      "Batch 10/50, loss: 0.6905624389648437 (0.009s), train acc: 0.500\n",
      "Batch 20/50, loss: 0.6916436791419983 (0.009s), train acc: 0.514\n",
      "Batch 30/50, loss: 0.6918546319007873 (0.010s), train acc: 0.511\n",
      "Batch 40/50, loss: 0.6899840116500855 (0.010s), train acc: 0.513\n",
      "Batch 50/50, loss: 0.6910567581653595 (0.010s), train acc: 0.515\n",
      "\n",
      "-----Epoch 94/500-----\n",
      "Batch 10/50, loss: 0.6938716351985932 (0.009s), train acc: 0.491\n",
      "Batch 20/50, loss: 0.6878123104572296 (0.009s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.691982102394104 (0.010s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6870295643806458 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.691880875825882 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 95/500-----\n",
      "Batch 10/50, loss: 0.6943825900554657 (0.009s), train acc: 0.491\n",
      "Batch 20/50, loss: 0.6909731209278107 (0.009s), train acc: 0.511\n",
      "Batch 30/50, loss: 0.6933647274971009 (0.010s), train acc: 0.506\n",
      "Batch 40/50, loss: 0.6884447753429412 (0.010s), train acc: 0.516\n",
      "Batch 50/50, loss: 0.6845914781093597 (0.010s), train acc: 0.526\n",
      "\n",
      "-----Epoch 96/500-----\n",
      "Batch 10/50, loss: 0.6892451107501983 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6879102885723114 (0.009s), train acc: 0.547\n",
      "Batch 30/50, loss: 0.6931875050067902 (0.009s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6974230229854583 (0.010s), train acc: 0.507\n",
      "Batch 50/50, loss: 0.6866592228412628 (0.010s), train acc: 0.514\n",
      "\n",
      "-----Epoch 97/500-----\n",
      "Batch 10/50, loss: 0.6913650333881378 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6918520152568817 (0.009s), train acc: 0.522\n",
      "Batch 30/50, loss: 0.6886822760105134 (0.009s), train acc: 0.528\n",
      "Batch 40/50, loss: 0.6870471119880677 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6940451800823212 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 98/500-----\n",
      "Batch 10/50, loss: 0.6862493515014648 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6882860541343689 (0.009s), train acc: 0.547\n",
      "Batch 30/50, loss: 0.6951921701431274 (0.010s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6920974671840667 (0.010s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6916647672653198 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 99/500-----\n",
      "Batch 10/50, loss: 0.6897149324417114 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6877914011478424 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6942044913768768 (0.009s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.694381308555603 (0.010s), train acc: 0.517\n",
      "Batch 50/50, loss: 0.6859054028987884 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 100/500-----\n",
      "Batch 10/50, loss: 0.6870928883552552 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6897934675216675 (0.009s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6880157887935638 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6940986394882203 (0.010s), train acc: 0.519\n",
      "Batch 50/50, loss: 0.6926370799541474 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 101/500-----\n",
      "Batch 10/50, loss: 0.6888314187526703 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6883547186851502 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6930351853370667 (0.009s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6921333253383637 (0.010s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6884208977222442 (0.010s), train acc: 0.529\n",
      "\n",
      "-----Epoch 102/500-----\n",
      "Batch 10/50, loss: 0.6856141984462738 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.692822152376175 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6913308918476104 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6941010057926178 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6873546004295349 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 103/500-----\n",
      "Batch 10/50, loss: 0.6897844493389129 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.6878340482711792 (0.009s), train acc: 0.509\n",
      "Batch 30/50, loss: 0.6934446692466736 (0.010s), train acc: 0.515\n",
      "Batch 40/50, loss: 0.689486825466156 (0.010s), train acc: 0.522\n",
      "Batch 50/50, loss: 0.6894251465797424 (0.010s), train acc: 0.518\n",
      "\n",
      "-----Epoch 104/500-----\n",
      "Batch 10/50, loss: 0.6937554359436036 (0.009s), train acc: 0.469\n",
      "Batch 20/50, loss: 0.6885886073112488 (0.009s), train acc: 0.500\n",
      "Batch 30/50, loss: 0.6897978782653809 (0.010s), train acc: 0.506\n",
      "Batch 40/50, loss: 0.690051943063736 (0.010s), train acc: 0.515\n",
      "Batch 50/50, loss: 0.6886929571628571 (0.010s), train acc: 0.516\n",
      "\n",
      "-----Epoch 105/500-----\n",
      "Batch 10/50, loss: 0.6849410891532898 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6936955213546753 (0.009s), train acc: 0.511\n",
      "Batch 30/50, loss: 0.689383327960968 (0.010s), train acc: 0.517\n",
      "Batch 40/50, loss: 0.6923196673393249 (0.010s), train acc: 0.520\n",
      "Batch 50/50, loss: 0.6927034199237824 (0.010s), train acc: 0.515\n",
      "\n",
      "-----Epoch 106/500-----\n",
      "Batch 10/50, loss: 0.6960633516311645 (0.009s), train acc: 0.491\n",
      "Batch 20/50, loss: 0.6896335065364838 (0.010s), train acc: 0.516\n",
      "Batch 30/50, loss: 0.6868918597698211 (0.010s), train acc: 0.523\n",
      "Batch 40/50, loss: 0.6894583106040955 (0.010s), train acc: 0.516\n",
      "Batch 50/50, loss: 0.6896681785583496 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 107/500-----\n",
      "Batch 10/50, loss: 0.6838513493537903 (0.009s), train acc: 0.594\n",
      "Batch 20/50, loss: 0.6882293522357941 (0.009s), train acc: 0.566\n",
      "Batch 30/50, loss: 0.6992802619934082 (0.010s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6923227488994599 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6898211419582367 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 108/500-----\n",
      "Batch 10/50, loss: 0.6933892846107483 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6869765937328338 (0.009s), train acc: 0.511\n",
      "Batch 30/50, loss: 0.6872283816337585 (0.010s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.6947435855865478 (0.010s), train acc: 0.511\n",
      "Batch 50/50, loss: 0.6866962134838104 (0.010s), train acc: 0.516\n",
      "\n",
      "-----Epoch 109/500-----\n",
      "Batch 10/50, loss: 0.6856900453567505 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6903711080551147 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6884197771549225 (0.010s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6955572962760925 (0.010s), train acc: 0.519\n",
      "Batch 50/50, loss: 0.6886819899082184 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 110/500-----\n",
      "Batch 10/50, loss: 0.6900857925415039 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6895169138908386 (0.010s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6881931304931641 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6915931582450867 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6889537811279297 (0.010s), train acc: 0.529\n",
      "\n",
      "-----Epoch 111/500-----\n",
      "Batch 10/50, loss: 0.6830074906349182 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6935838878154754 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6897805869579315 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6931365907192231 (0.010s), train acc: 0.525\n",
      "Batch 50/50, loss: 0.6891738951206208 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 112/500-----\n",
      "Batch 10/50, loss: 0.6876177549362182 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6853212893009186 (0.009s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6873714685440063 (0.010s), train acc: 0.556\n",
      "Batch 40/50, loss: 0.6930525422096252 (0.010s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.6968696713447571 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 113/500-----\n",
      "Batch 10/50, loss: 0.6917426407337188 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6839434206485748 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6903465628623963 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6936755478382111 (0.010s), train acc: 0.516\n",
      "Batch 50/50, loss: 0.6914259850978851 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 114/500-----\n",
      "Batch 10/50, loss: 0.6906697452068329 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6887871563434601 (0.010s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6908260881900787 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.694189703464508 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6840946912765503 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 115/500-----\n",
      "Batch 10/50, loss: 0.685393339395523 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6935331523418427 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6889578700065613 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6930798768997193 (0.010s), train acc: 0.517\n",
      "Batch 50/50, loss: 0.6871771037578582 (0.010s), train acc: 0.517\n",
      "\n",
      "-----Epoch 116/500-----\n",
      "Batch 10/50, loss: 0.6904486238956451 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.6944008350372315 (0.009s), train acc: 0.503\n",
      "Batch 30/50, loss: 0.683305150270462 (0.009s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6910071671009064 (0.010s), train acc: 0.520\n",
      "Batch 50/50, loss: 0.6916687548160553 (0.010s), train acc: 0.524\n",
      "\n",
      "-----Epoch 117/500-----\n",
      "Batch 10/50, loss: 0.6921985685825348 (0.009s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.6892064988613129 (0.009s), train acc: 0.506\n",
      "Batch 30/50, loss: 0.6895486414432526 (0.010s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6908970773220062 (0.010s), train acc: 0.518\n",
      "Batch 50/50, loss: 0.6907816529273987 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 118/500-----\n",
      "Batch 10/50, loss: 0.6868406891822815 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6882392466068268 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6884079992771148 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.693218332529068 (0.010s), train acc: 0.536\n",
      "Batch 50/50, loss: 0.6944815039634704 (0.010s), train acc: 0.529\n",
      "\n",
      "-----Epoch 119/500-----\n",
      "Batch 10/50, loss: 0.691771000623703 (0.009s), train acc: 0.519\n",
      "Batch 20/50, loss: 0.6884242415428161 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6893725335597992 (0.010s), train acc: 0.517\n",
      "Batch 40/50, loss: 0.689276534318924 (0.010s), train acc: 0.515\n",
      "Batch 50/50, loss: 0.6913142561912536 (0.010s), train acc: 0.515\n",
      "\n",
      "-----Epoch 120/500-----\n",
      "Batch 10/50, loss: 0.6906360805034637 (0.009s), train acc: 0.497\n",
      "Batch 20/50, loss: 0.6888719201087952 (0.009s), train acc: 0.502\n",
      "Batch 30/50, loss: 0.6879450023174286 (0.010s), train acc: 0.508\n",
      "Batch 40/50, loss: 0.688116830587387 (0.010s), train acc: 0.515\n",
      "Batch 50/50, loss: 0.6916697800159455 (0.010s), train acc: 0.512\n",
      "\n",
      "-----Epoch 121/500-----\n",
      "Batch 10/50, loss: 0.6838506519794464 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6843686699867249 (0.009s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6925540924072265 (0.010s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6899357318878174 (0.010s), train acc: 0.522\n",
      "Batch 50/50, loss: 0.7010930001735687 (0.010s), train acc: 0.514\n",
      "\n",
      "-----Epoch 122/500-----\n",
      "Batch 10/50, loss: 0.6956188440322876 (0.009s), train acc: 0.497\n",
      "Batch 20/50, loss: 0.6944252669811248 (0.010s), train acc: 0.505\n",
      "Batch 30/50, loss: 0.6817909955978394 (0.010s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.6864630162715912 (0.010s), train acc: 0.531\n",
      "Batch 50/50, loss: 0.6919815897941589 (0.010s), train acc: 0.531\n",
      "\n",
      "-----Epoch 123/500-----\n",
      "Batch 10/50, loss: 0.6871760010719299 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6941124200820923 (0.009s), train acc: 0.520\n",
      "Batch 30/50, loss: 0.6880356848239899 (0.010s), train acc: 0.521\n",
      "Batch 40/50, loss: 0.6863619685173035 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6924762666225434 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 124/500-----\n",
      "Batch 10/50, loss: 0.6860159337520599 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.686192512512207 (0.009s), train acc: 0.555\n",
      "Batch 30/50, loss: 0.6912395358085632 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6914511084556579 (0.010s), train acc: 0.536\n",
      "Batch 50/50, loss: 0.6922508835792541 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 125/500-----\n",
      "Batch 10/50, loss: 0.6840292572975158 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6887277722358703 (0.009s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.6911973357200623 (0.010s), train acc: 0.544\n",
      "Batch 40/50, loss: 0.6931112170219421 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6931433796882629 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 126/500-----\n",
      "Batch 10/50, loss: 0.6935527741909027 (0.009s), train acc: 0.487\n",
      "Batch 20/50, loss: 0.6899358332157135 (0.009s), train acc: 0.503\n",
      "Batch 30/50, loss: 0.6892343401908875 (0.009s), train acc: 0.517\n",
      "Batch 40/50, loss: 0.692610889673233 (0.009s), train acc: 0.516\n",
      "Batch 50/50, loss: 0.6822516024112701 (0.010s), train acc: 0.529\n",
      "\n",
      "-----Epoch 127/500-----\n",
      "Batch 10/50, loss: 0.6883643925189972 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6867423176765441 (0.009s), train acc: 0.555\n",
      "Batch 30/50, loss: 0.686016035079956 (0.010s), train acc: 0.558\n",
      "Batch 40/50, loss: 0.6924605011940003 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6940352141857147 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 128/500-----\n",
      "Batch 10/50, loss: 0.6864549100399018 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.690748143196106 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6892084240913391 (0.009s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6923995196819306 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6896446168422699 (0.010s), train acc: 0.526\n",
      "\n",
      "-----Epoch 129/500-----\n",
      "Batch 10/50, loss: 0.6846738815307617 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6850332617759705 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6930071234703064 (0.009s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6925547242164611 (0.009s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6904699444770813 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 130/500-----\n",
      "Batch 10/50, loss: 0.6900287806987763 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6887782454490662 (0.009s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6827462792396546 (0.010s), train acc: 0.542\n",
      "Batch 40/50, loss: 0.6948330223560333 (0.010s), train acc: 0.531\n",
      "Batch 50/50, loss: 0.6907297909259796 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 131/500-----\n",
      "Batch 10/50, loss: 0.6922839283943176 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6871870577335357 (0.010s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6857894361019135 (0.009s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6872292160987854 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.693435400724411 (0.010s), train acc: 0.521\n",
      "\n",
      "-----Epoch 132/500-----\n",
      "Batch 10/50, loss: 0.6961895823478699 (0.009s), train acc: 0.478\n",
      "Batch 20/50, loss: 0.6829307973384857 (0.009s), train acc: 0.516\n",
      "Batch 30/50, loss: 0.6962313950061798 (0.010s), train acc: 0.503\n",
      "Batch 40/50, loss: 0.6879494726657868 (0.010s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.6833562016487121 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 133/500-----\n",
      "Batch 10/50, loss: 0.6894887745380401 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6895294845104217 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.691165953874588 (0.010s), train acc: 0.532\n",
      "Batch 40/50, loss: 0.6881619334220886 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.6872328639030456 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 134/500-----\n",
      "Batch 10/50, loss: 0.6852809131145478 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6878582656383514 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6917858302593232 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6949013710021973 (0.010s), train acc: 0.531\n",
      "Batch 50/50, loss: 0.6864004313945771 (0.010s), train acc: 0.525\n",
      "\n",
      "-----Epoch 135/500-----\n",
      "Batch 10/50, loss: 0.6864807188510895 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6904857516288757 (0.010s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6821328163146972 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6939620614051819 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6926331281661987 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 136/500-----\n",
      "Batch 10/50, loss: 0.6876671314239502 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6913642704486846 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6860578060150146 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.687834483385086 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6914214015007019 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 137/500-----\n",
      "Batch 10/50, loss: 0.6876519739627838 (0.009s), train acc: 0.491\n",
      "Batch 20/50, loss: 0.6913689374923706 (0.009s), train acc: 0.505\n",
      "Batch 30/50, loss: 0.6891779959201813 (0.010s), train acc: 0.517\n",
      "Batch 40/50, loss: 0.6934925079345703 (0.010s), train acc: 0.508\n",
      "Batch 50/50, loss: 0.6851887762546539 (0.010s), train acc: 0.514\n",
      "\n",
      "-----Epoch 138/500-----\n",
      "Batch 10/50, loss: 0.6885727941989899 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6916354596614838 (0.009s), train acc: 0.522\n",
      "Batch 30/50, loss: 0.6853352069854737 (0.010s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6909258186817169 (0.010s), train acc: 0.516\n",
      "Batch 50/50, loss: 0.689813357591629 (0.010s), train acc: 0.518\n",
      "\n",
      "-----Epoch 139/500-----\n",
      "Batch 10/50, loss: 0.687559974193573 (0.009s), train acc: 0.519\n",
      "Batch 20/50, loss: 0.6904367864131927 (0.009s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.6870811283588409 (0.009s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6896137773990632 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6883382022380828 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 140/500-----\n",
      "Batch 10/50, loss: 0.6925163388252258 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6812324702739716 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6892815470695496 (0.010s), train acc: 0.532\n",
      "Batch 40/50, loss: 0.6895687878131866 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6918121814727783 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 141/500-----\n",
      "Batch 10/50, loss: 0.6871015012264252 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6848500311374665 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6964014291763305 (0.010s), train acc: 0.518\n",
      "Batch 40/50, loss: 0.6945447504520417 (0.009s), train acc: 0.516\n",
      "Batch 50/50, loss: 0.6820911347866059 (0.010s), train acc: 0.518\n",
      "\n",
      "-----Epoch 142/500-----\n",
      "Batch 10/50, loss: 0.6898540019989013 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6901764392852783 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6930881917476654 (0.010s), train acc: 0.521\n",
      "Batch 40/50, loss: 0.6835779666900634 (0.010s), train acc: 0.526\n",
      "Batch 50/50, loss: 0.6878382325172424 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 143/500-----\n",
      "Batch 10/50, loss: 0.68670254945755 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6911347687244416 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6858954787254333 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6906491279602051 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.691884595155716 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 144/500-----\n",
      "Batch 10/50, loss: 0.6871300756931304 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.687666779756546 (0.009s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6905041337013245 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6892902910709381 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6898971140384674 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 145/500-----\n",
      "Batch 10/50, loss: 0.6878049731254577 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6936622619628906 (0.009s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.6839899480342865 (0.010s), train acc: 0.532\n",
      "Batch 40/50, loss: 0.686793464422226 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.69112269282341 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 146/500-----\n",
      "Batch 10/50, loss: 0.6911158561706543 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6857001960277558 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6933492064476013 (0.010s), train acc: 0.518\n",
      "Batch 40/50, loss: 0.6875435709953308 (0.009s), train acc: 0.515\n",
      "Batch 50/50, loss: 0.6838617086410522 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 147/500-----\n",
      "Batch 10/50, loss: 0.6855902731418609 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6874467372894287 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6967570424079895 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6866668045520783 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6871279895305633 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 148/500-----\n",
      "Batch 10/50, loss: 0.6873645663261414 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6846694767475128 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6904113471508027 (0.009s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6839345574378968 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.6977175414562226 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 149/500-----\n",
      "Batch 10/50, loss: 0.6841968297958374 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6903534770011902 (0.009s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.6882891654968262 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6904413878917695 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.68873051404953 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 150/500-----\n",
      "Batch 10/50, loss: 0.680835074186325 (0.009s), train acc: 0.613\n",
      "Batch 20/50, loss: 0.6894566059112549 (0.009s), train acc: 0.562\n",
      "Batch 30/50, loss: 0.6948610126972199 (0.009s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6893743634223938 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6894719481468201 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 151/500-----\n",
      "Batch 10/50, loss: 0.6893821001052857 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6884679675102234 (0.009s), train acc: 0.523\n",
      "Batch 30/50, loss: 0.6892825365066528 (0.009s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6887280762195587 (0.010s), train acc: 0.528\n",
      "Batch 50/50, loss: 0.6904853880405426 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 152/500-----\n",
      "Batch 10/50, loss: 0.6885345101356506 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6849344491958618 (0.010s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6887097418308258 (0.009s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.6909067213535309 (0.009s), train acc: 0.525\n",
      "Batch 50/50, loss: 0.6898561775684356 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 153/500-----\n",
      "Batch 10/50, loss: 0.6830508589744568 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6946316361427307 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6838129580020904 (0.010s), train acc: 0.544\n",
      "Batch 40/50, loss: 0.6964563012123108 (0.010s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6826467156410218 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 154/500-----\n",
      "Batch 10/50, loss: 0.6889892816543579 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6861869156360626 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6915424287319183 (0.009s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6875661969184875 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6904904842376709 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 155/500-----\n",
      "Batch 10/50, loss: 0.689222228527069 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.686830997467041 (0.009s), train acc: 0.520\n",
      "Batch 30/50, loss: 0.6874029695987701 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6927495181560517 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6870879054069519 (0.010s), train acc: 0.521\n",
      "\n",
      "-----Epoch 156/500-----\n",
      "Batch 10/50, loss: 0.6876855254173279 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6875942468643188 (0.010s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6902493417263031 (0.010s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6911617815494537 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6862336099147797 (0.010s), train acc: 0.529\n",
      "\n",
      "-----Epoch 157/500-----\n",
      "Batch 10/50, loss: 0.6805475890636444 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.6955559611320495 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6858027219772339 (0.009s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6906818807125091 (0.010s), train acc: 0.525\n",
      "Batch 50/50, loss: 0.6904936790466308 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 158/500-----\n",
      "Batch 10/50, loss: 0.6935128808021546 (0.009s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.6879246532917023 (0.009s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.6881600797176362 (0.010s), train acc: 0.520\n",
      "Batch 40/50, loss: 0.6842803537845612 (0.010s), train acc: 0.528\n",
      "Batch 50/50, loss: 0.691618287563324 (0.010s), train acc: 0.529\n",
      "\n",
      "-----Epoch 159/500-----\n",
      "Batch 10/50, loss: 0.6831420719623565 (0.009s), train acc: 0.578\n",
      "Batch 20/50, loss: 0.6961930692195892 (0.009s), train acc: 0.517\n",
      "Batch 30/50, loss: 0.6891060471534729 (0.009s), train acc: 0.516\n",
      "Batch 40/50, loss: 0.6949868321418762 (0.010s), train acc: 0.505\n",
      "Batch 50/50, loss: 0.6782512843608857 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 160/500-----\n",
      "Batch 10/50, loss: 0.6880943000316619 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6908561885356903 (0.009s), train acc: 0.525\n",
      "Batch 30/50, loss: 0.681561815738678 (0.010s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6933194100856781 (0.010s), train acc: 0.519\n",
      "Batch 50/50, loss: 0.6894647181034088 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 161/500-----\n",
      "Batch 10/50, loss: 0.6921920955181122 (0.009s), train acc: 0.494\n",
      "Batch 20/50, loss: 0.6960250794887543 (0.009s), train acc: 0.491\n",
      "Batch 30/50, loss: 0.6834963500499726 (0.009s), train acc: 0.516\n",
      "Batch 40/50, loss: 0.6831449329853058 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.684946870803833 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 162/500-----\n",
      "Batch 10/50, loss: 0.6865444660186768 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6842552244663238 (0.009s), train acc: 0.555\n",
      "Batch 30/50, loss: 0.6871369838714599 (0.009s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6913610696792603 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.692214161157608 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 163/500-----\n",
      "Batch 10/50, loss: 0.6958947479724884 (0.009s), train acc: 0.481\n",
      "Batch 20/50, loss: 0.6846364796161651 (0.009s), train acc: 0.506\n",
      "Batch 30/50, loss: 0.6862211048603057 (0.009s), train acc: 0.517\n",
      "Batch 40/50, loss: 0.6799552321434021 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.692717319726944 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 164/500-----\n",
      "Batch 10/50, loss: 0.6917050004005432 (0.009s), train acc: 0.519\n",
      "Batch 20/50, loss: 0.6930152952671051 (0.009s), train acc: 0.509\n",
      "Batch 30/50, loss: 0.6827768504619598 (0.009s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6873301565647125 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.6853390514850617 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 165/500-----\n",
      "Batch 10/50, loss: 0.6872842609882355 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.6892425715923309 (0.009s), train acc: 0.525\n",
      "Batch 30/50, loss: 0.6881380081176758 (0.009s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6870373725891114 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6900099217891693 (0.010s), train acc: 0.531\n",
      "\n",
      "-----Epoch 166/500-----\n",
      "Batch 10/50, loss: 0.682322335243225 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6877591609954834 (0.009s), train acc: 0.522\n",
      "Batch 30/50, loss: 0.6888702869415283 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6878986179828643 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6960149049758911 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 167/500-----\n",
      "Batch 10/50, loss: 0.6912068784236908 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6861204862594604 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6906598448753357 (0.010s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6834488987922669 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.689633309841156 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 168/500-----\n",
      "Batch 10/50, loss: 0.6868997573852539 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6842042803764343 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6878776848316193 (0.009s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6921897947788238 (0.010s), train acc: 0.531\n",
      "Batch 50/50, loss: 0.6908687055110931 (0.010s), train acc: 0.531\n",
      "\n",
      "-----Epoch 169/500-----\n",
      "Batch 10/50, loss: 0.6900154650211334 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6809393465518951 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6851266860961914 (0.009s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6933248281478882 (0.009s), train acc: 0.536\n",
      "Batch 50/50, loss: 0.6944457352161407 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 170/500-----\n",
      "Batch 10/50, loss: 0.6906560122966766 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.6902099430561066 (0.009s), train acc: 0.523\n",
      "Batch 30/50, loss: 0.6854370296001434 (0.010s), train acc: 0.523\n",
      "Batch 40/50, loss: 0.6897981464862823 (0.009s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.687102448940277 (0.010s), train acc: 0.525\n",
      "\n",
      "-----Epoch 171/500-----\n",
      "Batch 10/50, loss: 0.6883040487766265 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6896630883216858 (0.009s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.6850791871547699 (0.009s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6842669010162353 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6924985766410827 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 172/500-----\n",
      "Batch 10/50, loss: 0.6868346273899079 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.689726835489273 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6891785860061646 (0.010s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6860343217849731 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6881781101226807 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 173/500-----\n",
      "Batch 10/50, loss: 0.6827532231807709 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.686425119638443 (0.009s), train acc: 0.547\n",
      "Batch 30/50, loss: 0.6899268388748169 (0.009s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6927429676055908 (0.009s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6882389783859253 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 174/500-----\n",
      "Batch 10/50, loss: 0.687078857421875 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6876844227313995 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.683018034696579 (0.009s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.686448872089386 (0.009s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6967600524425507 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 175/500-----\n",
      "Batch 10/50, loss: 0.6895149111747741 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6838885009288788 (0.009s), train acc: 0.523\n",
      "Batch 30/50, loss: 0.6817009031772614 (0.009s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.694061678647995 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.690898209810257 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 176/500-----\n",
      "Batch 10/50, loss: 0.6802956461906433 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.687414926290512 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6889674842357636 (0.009s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6959584295749665 (0.009s), train acc: 0.518\n",
      "Batch 50/50, loss: 0.6880932688713074 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 177/500-----\n",
      "Batch 10/50, loss: 0.6891030430793762 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6818204462528229 (0.009s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.680267608165741 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.6989729642868042 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.690136331319809 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 178/500-----\n",
      "Batch 10/50, loss: 0.6835718095302582 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6888786494731903 (0.010s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.6772535681724549 (0.010s), train acc: 0.542\n",
      "Batch 40/50, loss: 0.6888933658599854 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.7016224026679992 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 179/500-----\n",
      "Batch 10/50, loss: 0.6835474610328675 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.695503544807434 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.686268424987793 (0.009s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.6853768110275269 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6893912613391876 (0.010s), train acc: 0.525\n",
      "\n",
      "-----Epoch 180/500-----\n",
      "Batch 10/50, loss: 0.6870492577552796 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6877090334892273 (0.009s), train acc: 0.523\n",
      "Batch 30/50, loss: 0.6813011527061462 (0.009s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6932300090789795 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6887959837913513 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 181/500-----\n",
      "Batch 10/50, loss: 0.6878836631774903 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6843492329120636 (0.009s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6861155390739441 (0.010s), train acc: 0.542\n",
      "Batch 40/50, loss: 0.6838077247142792 (0.010s), train acc: 0.543\n",
      "Batch 50/50, loss: 0.693829995393753 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 182/500-----\n",
      "Batch 10/50, loss: 0.6885608732700348 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6834256172180175 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.691419529914856 (0.009s), train acc: 0.514\n",
      "Batch 40/50, loss: 0.6859129309654236 (0.010s), train acc: 0.525\n",
      "Batch 50/50, loss: 0.6928728640079498 (0.010s), train acc: 0.521\n",
      "\n",
      "-----Epoch 183/500-----\n",
      "Batch 10/50, loss: 0.6923517704010009 (0.009s), train acc: 0.500\n",
      "Batch 20/50, loss: 0.6810828745365143 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6841311514377594 (0.009s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6872892022132874 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6925802588462829 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 184/500-----\n",
      "Batch 10/50, loss: 0.690013724565506 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6917980074882507 (0.009s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6820941686630249 (0.009s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6865936160087586 (0.010s), train acc: 0.536\n",
      "Batch 50/50, loss: 0.6860718131065369 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 185/500-----\n",
      "Batch 10/50, loss: 0.6872773826122284 (0.009s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.6860180675983429 (0.010s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6827012896537781 (0.009s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6911854147911072 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6921291887760163 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 186/500-----\n",
      "Batch 10/50, loss: 0.6876279175281524 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6852654039859771 (0.009s), train acc: 0.522\n",
      "Batch 30/50, loss: 0.6910453498363495 (0.009s), train acc: 0.528\n",
      "Batch 40/50, loss: 0.6906132161617279 (0.010s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6847902417182923 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 187/500-----\n",
      "Batch 10/50, loss: 0.6858592450618743 (0.009s), train acc: 0.566\n",
      "Batch 20/50, loss: 0.6838002383708954 (0.009s), train acc: 0.556\n",
      "Batch 30/50, loss: 0.6915858328342438 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.687728762626648 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6903525173664093 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 188/500-----\n",
      "Batch 10/50, loss: 0.685394424200058 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6858177304267883 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6853853404521942 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6849091172218322 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6963835477828979 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 189/500-----\n",
      "Batch 10/50, loss: 0.6903586089611053 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6832739591598511 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6927734613418579 (0.009s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6825306296348572 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6853381514549255 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 190/500-----\n",
      "Batch 10/50, loss: 0.6867497563362122 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6897198140621186 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6833220839500427 (0.009s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6917872667312622 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6853541553020477 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 191/500-----\n",
      "Batch 10/50, loss: 0.6811492919921875 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6826153516769409 (0.009s), train acc: 0.569\n",
      "Batch 30/50, loss: 0.6960290968418121 (0.009s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6917697429656983 (0.009s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.6890854001045227 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 192/500-----\n",
      "Batch 10/50, loss: 0.6851429045200348 (0.009s), train acc: 0.575\n",
      "Batch 20/50, loss: 0.6902047276496888 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6875577807426453 (0.009s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6867366135120392 (0.009s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.688105046749115 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 193/500-----\n",
      "Batch 10/50, loss: 0.6824703216552734 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6875035345554352 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.690045952796936 (0.009s), train acc: 0.519\n",
      "Batch 40/50, loss: 0.6886057436466217 (0.009s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6861523151397705 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 194/500-----\n",
      "Batch 10/50, loss: 0.6871076166629791 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6922666013240815 (0.010s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6823443472385406 (0.010s), train acc: 0.532\n",
      "Batch 40/50, loss: 0.686542946100235 (0.009s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6909031808376312 (0.010s), train acc: 0.531\n",
      "\n",
      "-----Epoch 195/500-----\n",
      "Batch 10/50, loss: 0.6853683829307556 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6832474887371063 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.692411744594574 (0.009s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6826010525226593 (0.009s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6960062980651855 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 196/500-----\n",
      "Batch 10/50, loss: 0.6864884972572327 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6913699507713318 (0.009s), train acc: 0.523\n",
      "Batch 30/50, loss: 0.6855325281620026 (0.010s), train acc: 0.520\n",
      "Batch 40/50, loss: 0.6902852416038513 (0.010s), train acc: 0.518\n",
      "Batch 50/50, loss: 0.6866862058639527 (0.010s), train acc: 0.524\n",
      "\n",
      "-----Epoch 197/500-----\n",
      "Batch 10/50, loss: 0.6858215272426605 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6895027339458466 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6854461014270783 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.690635597705841 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6872555673122406 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 198/500-----\n",
      "Batch 10/50, loss: 0.6803421020507813 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6825815200805664 (0.009s), train acc: 0.561\n",
      "Batch 30/50, loss: 0.6928861379623413 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.6866162419319153 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6940850257873535 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 199/500-----\n",
      "Batch 10/50, loss: 0.690187132358551 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6829475820064544 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6865046262741089 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.689620703458786 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.6925498962402343 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 200/500-----\n",
      "Batch 10/50, loss: 0.6832416713237762 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6894424796104431 (0.010s), train acc: 0.525\n",
      "Batch 30/50, loss: 0.6886529982089996 (0.010s), train acc: 0.520\n",
      "Batch 40/50, loss: 0.6854895949363708 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6876284599304199 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 201/500-----\n",
      "Batch 10/50, loss: 0.6864035844802856 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6894801735877991 (0.009s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6881292760372162 (0.010s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.6864034116268158 (0.010s), train acc: 0.531\n",
      "Batch 50/50, loss: 0.6855061113834381 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 202/500-----\n",
      "Batch 10/50, loss: 0.6856700301170349 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6889516115188599 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6868710219860077 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6871213912963867 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.6856327831745148 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 203/500-----\n",
      "Batch 10/50, loss: 0.6871532380580903 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6844105780124664 (0.009s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.6940475881099701 (0.010s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6860964953899383 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.683277302980423 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 204/500-----\n",
      "Batch 10/50, loss: 0.6783304333686828 (0.009s), train acc: 0.613\n",
      "Batch 20/50, loss: 0.6883287727832794 (0.009s), train acc: 0.573\n",
      "Batch 30/50, loss: 0.6941142559051514 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.686862689256668 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6898746371269227 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 205/500-----\n",
      "Batch 10/50, loss: 0.6831230640411377 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6851118266582489 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6902964949607849 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6903060615062714 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.684157520532608 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 206/500-----\n",
      "Batch 10/50, loss: 0.6770360171794891 (0.009s), train acc: 0.581\n",
      "Batch 20/50, loss: 0.690628981590271 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6864327192306519 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.6946111500263215 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6886991143226624 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 207/500-----\n",
      "Batch 10/50, loss: 0.6832809805870056 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6871856391429901 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6895538628101349 (0.010s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6841730773448944 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6909945428371429 (0.010s), train acc: 0.531\n",
      "\n",
      "-----Epoch 208/500-----\n",
      "Batch 10/50, loss: 0.6880517959594726 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6853127181529999 (0.009s), train acc: 0.514\n",
      "Batch 30/50, loss: 0.6907079458236695 (0.010s), train acc: 0.518\n",
      "Batch 40/50, loss: 0.6866919219493866 (0.010s), train acc: 0.528\n",
      "Batch 50/50, loss: 0.6846754014492035 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 209/500-----\n",
      "Batch 10/50, loss: 0.6896074175834656 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.6780186891555786 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6889599025249481 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6916421473026275 (0.010s), train acc: 0.524\n",
      "Batch 50/50, loss: 0.68766148686409 (0.010s), train acc: 0.528\n",
      "\n",
      "-----Epoch 210/500-----\n",
      "Batch 10/50, loss: 0.6866646111011505 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6878134906291962 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6824373126029968 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6814116895198822 (0.009s), train acc: 0.549\n",
      "Batch 50/50, loss: 0.6950507283210754 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 211/500-----\n",
      "Batch 10/50, loss: 0.686026793718338 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6848044157028198 (0.009s), train acc: 0.525\n",
      "Batch 30/50, loss: 0.6916054129600525 (0.010s), train acc: 0.521\n",
      "Batch 40/50, loss: 0.6843179702758789 (0.009s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6907212555408477 (0.010s), train acc: 0.529\n",
      "\n",
      "-----Epoch 212/500-----\n",
      "Batch 10/50, loss: 0.6835374653339386 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6818589746952057 (0.009s), train acc: 0.573\n",
      "Batch 30/50, loss: 0.6911164224147797 (0.010s), train acc: 0.550\n",
      "Batch 40/50, loss: 0.6898506045341491 (0.009s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6906542658805848 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 213/500-----\n",
      "Batch 10/50, loss: 0.68461874127388 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6791886329650879 (0.009s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6866023421287537 (0.009s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6879093706607818 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.7015747427940369 (0.009s), train acc: 0.530\n",
      "\n",
      "-----Epoch 214/500-----\n",
      "Batch 10/50, loss: 0.6875551342964172 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6867228925228119 (0.009s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6885553300380707 (0.009s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6825024783611298 (0.009s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6870940685272217 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 215/500-----\n",
      "Batch 10/50, loss: 0.677736347913742 (0.009s), train acc: 0.600\n",
      "Batch 20/50, loss: 0.6820055544376373 (0.010s), train acc: 0.583\n",
      "Batch 30/50, loss: 0.692589282989502 (0.009s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.688526576757431 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6930779814720154 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 216/500-----\n",
      "Batch 10/50, loss: 0.6803594470024109 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6877774477005005 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6878006875514984 (0.009s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6929681539535523 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.6839790165424346 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 217/500-----\n",
      "Batch 10/50, loss: 0.6830889105796814 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6935424745082855 (0.009s), train acc: 0.525\n",
      "Batch 30/50, loss: 0.6839057207107544 (0.009s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6840644359588623 (0.009s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6906512618064881 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 218/500-----\n",
      "Batch 10/50, loss: 0.6935906529426574 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6894935727119446 (0.009s), train acc: 0.523\n",
      "Batch 30/50, loss: 0.6838971316814423 (0.009s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6761876463890075 (0.009s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6913425862789154 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 219/500-----\n",
      "Batch 10/50, loss: 0.6824970960617065 (0.009s), train acc: 0.575\n",
      "Batch 20/50, loss: 0.6911820352077485 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6856435596942901 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6853714287281036 (0.009s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6904625236988068 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 220/500-----\n",
      "Batch 10/50, loss: 0.6854931831359863 (0.009s), train acc: 0.566\n",
      "Batch 20/50, loss: 0.6824716329574585 (0.009s), train acc: 0.559\n",
      "Batch 30/50, loss: 0.6916377604007721 (0.009s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.6853581428527832 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6881496965885162 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 221/500-----\n",
      "Batch 10/50, loss: 0.6858486831188202 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6876700043678283 (0.009s), train acc: 0.561\n",
      "Batch 30/50, loss: 0.6888124585151673 (0.009s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.6848955631256104 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.6895988941192627 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 222/500-----\n",
      "Batch 10/50, loss: 0.6838550806045532 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.6807172358036041 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6938489317893982 (0.009s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6908398032188415 (0.009s), train acc: 0.522\n",
      "Batch 50/50, loss: 0.6863465189933777 (0.009s), train acc: 0.534\n",
      "\n",
      "-----Epoch 223/500-----\n",
      "Batch 10/50, loss: 0.6819417297840118 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6816099047660827 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6872673571109772 (0.009s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.6948627173900604 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6883660972118377 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 224/500-----\n",
      "Batch 10/50, loss: 0.6897213757038116 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6776027321815491 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6852928221225738 (0.009s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6882600665092469 (0.009s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6898345828056336 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 225/500-----\n",
      "Batch 10/50, loss: 0.6818420052528381 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6913560450077056 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6867994964122772 (0.009s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6825363457202911 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6904632747173309 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 226/500-----\n",
      "Batch 10/50, loss: 0.6828188836574555 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6803497493267059 (0.009s), train acc: 0.567\n",
      "Batch 30/50, loss: 0.683322685956955 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6892737209796905 (0.010s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6966639220714569 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 227/500-----\n",
      "Batch 10/50, loss: 0.6802050173282623 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6812708139419555 (0.009s), train acc: 0.564\n",
      "Batch 30/50, loss: 0.6836678266525269 (0.009s), train acc: 0.565\n",
      "Batch 40/50, loss: 0.6960740506649017 (0.009s), train acc: 0.555\n",
      "Batch 50/50, loss: 0.6917050123214722 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 228/500-----\n",
      "Batch 10/50, loss: 0.6901063323020935 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6837256491184235 (0.009s), train acc: 0.520\n",
      "Batch 30/50, loss: 0.6886918127536774 (0.009s), train acc: 0.518\n",
      "Batch 40/50, loss: 0.6852728366851807 (0.009s), train acc: 0.521\n",
      "Batch 50/50, loss: 0.6836297512054443 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 229/500-----\n",
      "Batch 10/50, loss: 0.6947893023490905 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.696442848443985 (0.009s), train acc: 0.514\n",
      "Batch 30/50, loss: 0.6861560106277466 (0.009s), train acc: 0.514\n",
      "Batch 40/50, loss: 0.679244589805603 (0.009s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6734598994255065 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 230/500-----\n",
      "Batch 10/50, loss: 0.689774215221405 (0.008s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6874299824237824 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.683289498090744 (0.009s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.6929104447364807 (0.009s), train acc: 0.531\n",
      "Batch 50/50, loss: 0.6794629395008087 (0.009s), train acc: 0.537\n",
      "\n",
      "-----Epoch 231/500-----\n",
      "Batch 10/50, loss: 0.6930925905704498 (0.009s), train acc: 0.503\n",
      "Batch 20/50, loss: 0.6838408231735229 (0.009s), train acc: 0.508\n",
      "Batch 30/50, loss: 0.6802212715148925 (0.009s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.6818328320980072 (0.009s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6943273425102234 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 232/500-----\n",
      "Batch 10/50, loss: 0.6887680768966675 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.678549987077713 (0.009s), train acc: 0.566\n",
      "Batch 30/50, loss: 0.6853187263011933 (0.009s), train acc: 0.559\n",
      "Batch 40/50, loss: 0.6936481773853302 (0.009s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.688692569732666 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 233/500-----\n",
      "Batch 10/50, loss: 0.683169150352478 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6939155280590057 (0.009s), train acc: 0.516\n",
      "Batch 30/50, loss: 0.6782782018184662 (0.009s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6897700190544128 (0.009s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6928710162639617 (0.010s), train acc: 0.543\n",
      "\n",
      "-----Epoch 234/500-----\n",
      "Batch 10/50, loss: 0.6861367464065552 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6848819196224213 (0.009s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6860703945159912 (0.009s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6880903720855713 (0.009s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6853725671768188 (0.009s), train acc: 0.540\n",
      "\n",
      "-----Epoch 235/500-----\n",
      "Batch 10/50, loss: 0.6870878636837006 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.686465710401535 (0.009s), train acc: 0.520\n",
      "Batch 30/50, loss: 0.6868055939674378 (0.009s), train acc: 0.532\n",
      "Batch 40/50, loss: 0.6877467930316925 (0.009s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6863647282123566 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 236/500-----\n",
      "Batch 10/50, loss: 0.6821928143501281 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.6865298986434937 (0.010s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6851559519767761 (0.009s), train acc: 0.552\n",
      "Batch 40/50, loss: 0.6845938205718994 (0.009s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.6923992216587067 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 237/500-----\n",
      "Batch 10/50, loss: 0.6832253217697144 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6873867511749268 (0.009s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6869391441345215 (0.009s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.6872316181659699 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.683021855354309 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 238/500-----\n",
      "Batch 10/50, loss: 0.6914824485778809 (0.009s), train acc: 0.500\n",
      "Batch 20/50, loss: 0.6762618005275727 (0.009s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6903707802295684 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6801148414611816 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6959036886692047 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 239/500-----\n",
      "Batch 10/50, loss: 0.6831382393836976 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6893807232379914 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6914283692836761 (0.009s), train acc: 0.524\n",
      "Batch 40/50, loss: 0.6784651100635528 (0.009s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6894995629787445 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 240/500-----\n",
      "Batch 10/50, loss: 0.6880288302898407 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.684889817237854 (0.009s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.6805930376052857 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6913367450237274 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6858852624893188 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 241/500-----\n",
      "Batch 10/50, loss: 0.6924149990081787 (0.009s), train acc: 0.475\n",
      "Batch 20/50, loss: 0.6816395580768585 (0.010s), train acc: 0.506\n",
      "Batch 30/50, loss: 0.683846515417099 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6866266369819641 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.683758533000946 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 242/500-----\n",
      "Batch 10/50, loss: 0.6861581444740296 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6850735485553742 (0.010s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6839228451251984 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6929989159107208 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.683097916841507 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 243/500-----\n",
      "Batch 10/50, loss: 0.6902887940406799 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6823233306407929 (0.010s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6872777760028839 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6879931628704071 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6831857562065125 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 244/500-----\n",
      "Batch 10/50, loss: 0.6870232164859772 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.689197838306427 (0.010s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6843902766704559 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6818385899066925 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6866550326347352 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 245/500-----\n",
      "Batch 10/50, loss: 0.6861284255981446 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.686386090517044 (0.010s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6840098917484283 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6917293250560761 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.680553138256073 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 246/500-----\n",
      "Batch 10/50, loss: 0.6873610854148865 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6805168569087983 (0.010s), train acc: 0.561\n",
      "Batch 30/50, loss: 0.6847853899002075 (0.010s), train acc: 0.552\n",
      "Batch 40/50, loss: 0.6910103321075439 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6879875838756562 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 247/500-----\n",
      "Batch 10/50, loss: 0.6829678952693939 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6872410655021668 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6853097379207611 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6879130780696869 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6853784799575806 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 248/500-----\n",
      "Batch 10/50, loss: 0.6920472025871277 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.689281564950943 (0.010s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6848822414875031 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6857472777366638 (0.010s), train acc: 0.536\n",
      "Batch 50/50, loss: 0.6804526507854461 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 249/500-----\n",
      "Batch 10/50, loss: 0.6879689991474152 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6917550444602967 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6852722227573395 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6838906049728394 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.6838564932346344 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 250/500-----\n",
      "Batch 10/50, loss: 0.6747277736663818 (0.009s), train acc: 0.588\n",
      "Batch 20/50, loss: 0.6865047514438629 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6821612358093262 (0.010s), train acc: 0.555\n",
      "Batch 40/50, loss: 0.6958487927913666 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6916230022907257 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 251/500-----\n",
      "Batch 10/50, loss: 0.6859704673290252 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6842392206192016 (0.010s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6876410782337189 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.685312682390213 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6861689269542695 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 252/500-----\n",
      "Batch 10/50, loss: 0.6824637413024902 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6797716856002808 (0.009s), train acc: 0.561\n",
      "Batch 30/50, loss: 0.690011614561081 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6884430229663849 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6897237598896027 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 253/500-----\n",
      "Batch 10/50, loss: 0.6862080097198486 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6822706520557403 (0.010s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6863984942436219 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6853253662586212 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6873943746089936 (0.010s), train acc: 0.545\n",
      "\n",
      "-----Epoch 254/500-----\n",
      "Batch 10/50, loss: 0.6805180013179779 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6915847182273864 (0.010s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6818402707576752 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6848010838031768 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6953469038009643 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 255/500-----\n",
      "Batch 10/50, loss: 0.6867854773998261 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6776135981082916 (0.010s), train acc: 0.566\n",
      "Batch 30/50, loss: 0.6858225464820862 (0.010s), train acc: 0.554\n",
      "Batch 40/50, loss: 0.6882279872894287 (0.010s), train acc: 0.555\n",
      "Batch 50/50, loss: 0.6892680048942565 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 256/500-----\n",
      "Batch 10/50, loss: 0.6953617393970489 (0.009s), train acc: 0.463\n",
      "Batch 20/50, loss: 0.6812195599079132 (0.010s), train acc: 0.487\n",
      "Batch 30/50, loss: 0.6861780464649201 (0.010s), train acc: 0.496\n",
      "Batch 40/50, loss: 0.6830014228820801 (0.010s), train acc: 0.512\n",
      "Batch 50/50, loss: 0.6862509191036225 (0.010s), train acc: 0.519\n",
      "\n",
      "-----Epoch 257/500-----\n",
      "Batch 10/50, loss: 0.679650628566742 (0.009s), train acc: 0.584\n",
      "Batch 20/50, loss: 0.6894801557064056 (0.010s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6884288847446441 (0.010s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6965658485889434 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6744951903820038 (0.010s), train acc: 0.531\n",
      "\n",
      "-----Epoch 258/500-----\n",
      "Batch 10/50, loss: 0.6795848727226257 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6835899412631988 (0.010s), train acc: 0.556\n",
      "Batch 30/50, loss: 0.69098259806633 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6911591649055481 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6823997855186462 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 259/500-----\n",
      "Batch 10/50, loss: 0.684203565120697 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6914010524749756 (0.010s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6760906159877778 (0.010s), train acc: 0.555\n",
      "Batch 40/50, loss: 0.6942531108856201 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6874806702136993 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 260/500-----\n",
      "Batch 10/50, loss: 0.6863105416297912 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6894931316375732 (0.010s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6831244051456451 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6804859101772308 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6911250472068786 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 261/500-----\n",
      "Batch 10/50, loss: 0.6806599736213684 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.7003385365009308 (0.010s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6836050570011138 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6764818012714386 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6878454744815826 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 262/500-----\n",
      "Batch 10/50, loss: 0.6850398480892181 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6922314405441284 (0.010s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6812880277633667 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.688204562664032 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.683373087644577 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 263/500-----\n",
      "Batch 10/50, loss: 0.687013179063797 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6889314174652099 (0.009s), train acc: 0.520\n",
      "Batch 30/50, loss: 0.6799452424049377 (0.010s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6855393052101135 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6868275582790375 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 264/500-----\n",
      "Batch 10/50, loss: 0.6825058996677399 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6871660351753235 (0.010s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6835873305797577 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6904299795627594 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6857322573661804 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 265/500-----\n",
      "Batch 10/50, loss: 0.6873556315898895 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.680382126569748 (0.010s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.689152467250824 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6822247862815857 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.6871661305427551 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 266/500-----\n",
      "Batch 10/50, loss: 0.6840725362300872 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6849451124668121 (0.010s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6859670519828797 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.6827857434749603 (0.010s), train acc: 0.553\n",
      "Batch 50/50, loss: 0.6907482326030732 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 267/500-----\n",
      "Batch 10/50, loss: 0.6822592914104462 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6927617967128754 (0.009s), train acc: 0.555\n",
      "Batch 30/50, loss: 0.6896533787250518 (0.010s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6820517003536224 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6768702030181885 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 268/500-----\n",
      "Batch 10/50, loss: 0.6841231703758239 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6796034395694732 (0.010s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.688011759519577 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6795444905757904 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6958861172199249 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 269/500-----\n",
      "Batch 10/50, loss: 0.6831436157226562 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6936092674732208 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6855850636959075 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6842840611934662 (0.010s), train acc: 0.529\n",
      "Batch 50/50, loss: 0.6799559414386749 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 270/500-----\n",
      "Batch 10/50, loss: 0.6858527660369873 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6921321451663971 (0.009s), train acc: 0.514\n",
      "Batch 30/50, loss: 0.6710118770599365 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6946310043334961 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6829464614391327 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 271/500-----\n",
      "Batch 10/50, loss: 0.6870117604732513 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6825782835483551 (0.009s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.6889949083328247 (0.010s), train acc: 0.542\n",
      "Batch 40/50, loss: 0.6862809956073761 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6795604348182678 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 272/500-----\n",
      "Batch 10/50, loss: 0.6812264800071717 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6799037218093872 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6843692183494567 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6990783929824829 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6842914283275604 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 273/500-----\n",
      "Batch 10/50, loss: 0.6855208396911621 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6842066943645477 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6808368384838104 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6926755428314209 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.6790702402591705 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 274/500-----\n",
      "Batch 10/50, loss: 0.68258917927742 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6778230667114258 (0.009s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.685050231218338 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6950294852256775 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6853223204612732 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 275/500-----\n",
      "Batch 10/50, loss: 0.6894054293632508 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6877953052520752 (0.010s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.684062796831131 (0.010s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6753553152084351 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6909654617309571 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 276/500-----\n",
      "Batch 10/50, loss: 0.6869987785816193 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6866615355014801 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6846482455730438 (0.010s), train acc: 0.519\n",
      "Batch 40/50, loss: 0.6786028802394867 (0.010s), train acc: 0.528\n",
      "Batch 50/50, loss: 0.6890644490718841 (0.010s), train acc: 0.529\n",
      "\n",
      "-----Epoch 277/500-----\n",
      "Batch 10/50, loss: 0.6936796724796295 (0.009s), train acc: 0.500\n",
      "Batch 20/50, loss: 0.6800210893154144 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6742798984050751 (0.010s), train acc: 0.550\n",
      "Batch 40/50, loss: 0.6963813483715058 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6798405945301056 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 278/500-----\n",
      "Batch 10/50, loss: 0.6884474396705628 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.6874364793300629 (0.010s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6791794717311859 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6823571801185608 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6880076944828033 (0.010s), train acc: 0.545\n",
      "\n",
      "-----Epoch 279/500-----\n",
      "Batch 10/50, loss: 0.6866073966026306 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6820101499557495 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6844423830509185 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.6859237372875213 (0.010s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6847608268260956 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 280/500-----\n",
      "Batch 10/50, loss: 0.6767645061016083 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6833523690700531 (0.010s), train acc: 0.566\n",
      "Batch 30/50, loss: 0.6908586025238037 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6828661739826203 (0.010s), train acc: 0.543\n",
      "Batch 50/50, loss: 0.6967560768127441 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 281/500-----\n",
      "Batch 10/50, loss: 0.685022896528244 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.683706670999527 (0.009s), train acc: 0.566\n",
      "Batch 30/50, loss: 0.6843667268753052 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.6935558199882508 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6790887773036957 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 282/500-----\n",
      "Batch 10/50, loss: 0.6858094036579132 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6782928764820099 (0.010s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6834524989128112 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6895838141441345 (0.010s), train acc: 0.524\n",
      "Batch 50/50, loss: 0.6875231325626373 (0.010s), train acc: 0.525\n",
      "\n",
      "-----Epoch 283/500-----\n",
      "Batch 10/50, loss: 0.6862603783607483 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6903883695602417 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6824186742305756 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6831545472145081 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6845123410224915 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 284/500-----\n",
      "Batch 10/50, loss: 0.6896751940250396 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6883786976337433 (0.009s), train acc: 0.509\n",
      "Batch 30/50, loss: 0.684624183177948 (0.010s), train acc: 0.516\n",
      "Batch 40/50, loss: 0.6825827896595001 (0.010s), train acc: 0.523\n",
      "Batch 50/50, loss: 0.6790100038051605 (0.010s), train acc: 0.527\n",
      "\n",
      "-----Epoch 285/500-----\n",
      "Batch 10/50, loss: 0.6861279606819153 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6811444461345673 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6857474386692047 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6844889044761657 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6892131745815278 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 286/500-----\n",
      "Batch 10/50, loss: 0.6781837224960328 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6926866292953491 (0.010s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6777429461479187 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.6839366376399993 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6957741916179657 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 287/500-----\n",
      "Batch 10/50, loss: 0.682565051317215 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.6822117328643799 (0.010s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6894795358181 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.6867354869842529 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6855536162853241 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 288/500-----\n",
      "Batch 10/50, loss: 0.6788332402706146 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6869889438152313 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.679990291595459 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6891546726226807 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6891939043998718 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 289/500-----\n",
      "Batch 10/50, loss: 0.6774772822856903 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6834101021289826 (0.010s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6837404191493988 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6830748617649078 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6967596709728241 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 290/500-----\n",
      "Batch 10/50, loss: 0.6798400223255158 (0.009s), train acc: 0.597\n",
      "Batch 20/50, loss: 0.6872342228889465 (0.009s), train acc: 0.564\n",
      "Batch 30/50, loss: 0.6843597650527954 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6888175010681152 (0.010s), train acc: 0.543\n",
      "Batch 50/50, loss: 0.6897071719169616 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 291/500-----\n",
      "Batch 10/50, loss: 0.6876792728900909 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6869538128376007 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6823976993560791 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6793339669704437 (0.010s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.688686853647232 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 292/500-----\n",
      "Batch 10/50, loss: 0.680293220281601 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.687196946144104 (0.009s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.6832355916500091 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.6869712352752686 (0.010s), train acc: 0.549\n",
      "Batch 50/50, loss: 0.6876606225967408 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 293/500-----\n",
      "Batch 10/50, loss: 0.6817690193653106 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6787343919277191 (0.009s), train acc: 0.583\n",
      "Batch 30/50, loss: 0.6902612626552582 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.6884806513786316 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6858894109725953 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 294/500-----\n",
      "Batch 10/50, loss: 0.6889962911605835 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6857582330703735 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.687404215335846 (0.010s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.6798199594020844 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.684110140800476 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 295/500-----\n",
      "Batch 10/50, loss: 0.6855725705623626 (0.009s), train acc: 0.497\n",
      "Batch 20/50, loss: 0.6818747639656066 (0.010s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6813618540763855 (0.010s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.6942664861679078 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6797423601150513 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 296/500-----\n",
      "Batch 10/50, loss: 0.6774073243141174 (0.009s), train acc: 0.594\n",
      "Batch 20/50, loss: 0.6822543680667877 (0.009s), train acc: 0.573\n",
      "Batch 30/50, loss: 0.7013882994651794 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6852385342121124 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6823515295982361 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 297/500-----\n",
      "Batch 10/50, loss: 0.6918272018432617 (0.009s), train acc: 0.487\n",
      "Batch 20/50, loss: 0.6849314391613006 (0.009s), train acc: 0.512\n",
      "Batch 30/50, loss: 0.6825262129306793 (0.010s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6876883566379547 (0.010s), train acc: 0.528\n",
      "Batch 50/50, loss: 0.6760813951492309 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 298/500-----\n",
      "Batch 10/50, loss: 0.6825463891029357 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6819024801254272 (0.009s), train acc: 0.555\n",
      "Batch 30/50, loss: 0.6833770275115967 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6934697031974792 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6841563999652862 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 299/500-----\n",
      "Batch 10/50, loss: 0.689845985174179 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6844967544078827 (0.010s), train acc: 0.580\n",
      "Batch 30/50, loss: 0.679572194814682 (0.010s), train acc: 0.553\n",
      "Batch 40/50, loss: 0.678465735912323 (0.010s), train acc: 0.553\n",
      "Batch 50/50, loss: 0.6890317380428315 (0.010s), train acc: 0.545\n",
      "\n",
      "-----Epoch 300/500-----\n",
      "Batch 10/50, loss: 0.6847964704036713 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6771230280399323 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6876489520072937 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6882507979869843 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6852005779743194 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 301/500-----\n",
      "Batch 10/50, loss: 0.6811611235141755 (0.009s), train acc: 0.575\n",
      "Batch 20/50, loss: 0.6867619752883911 (0.010s), train acc: 0.556\n",
      "Batch 30/50, loss: 0.6794438242912293 (0.010s), train acc: 0.558\n",
      "Batch 40/50, loss: 0.6830274403095246 (0.010s), train acc: 0.551\n",
      "Batch 50/50, loss: 0.6953530192375184 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 302/500-----\n",
      "Batch 10/50, loss: 0.6852326810359954 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6834555089473724 (0.009s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6873924970626831 (0.010s), train acc: 0.542\n",
      "Batch 40/50, loss: 0.6811149179935455 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6850269675254822 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 303/500-----\n",
      "Batch 10/50, loss: 0.6845557570457459 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6834456562995911 (0.010s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.682540363073349 (0.010s), train acc: 0.553\n",
      "Batch 40/50, loss: 0.6925946712493897 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6799687564373016 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 304/500-----\n",
      "Batch 10/50, loss: 0.6886356830596924 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6789466679096222 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6800058960914612 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6868189096450805 (0.010s), train acc: 0.543\n",
      "Batch 50/50, loss: 0.6886542975902558 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 305/500-----\n",
      "Batch 10/50, loss: 0.6914910316467285 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6804587304592132 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6912647187709808 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6763421058654785 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6844510793685913 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 306/500-----\n",
      "Batch 10/50, loss: 0.6815683364868164 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6902937293052673 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6794480919837952 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6932163536548615 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.677294647693634 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 307/500-----\n",
      "Batch 10/50, loss: 0.6819662511348724 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6754473209381103 (0.009s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6838923335075379 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.6967567563056946 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6827851712703705 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 308/500-----\n",
      "Batch 10/50, loss: 0.686540710926056 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6797376155853272 (0.009s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6841022551059723 (0.010s), train acc: 0.554\n",
      "Batch 40/50, loss: 0.6842055082321167 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6894048810005188 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 309/500-----\n",
      "Batch 10/50, loss: 0.6850890934467315 (0.009s), train acc: 0.494\n",
      "Batch 20/50, loss: 0.6871405184268952 (0.009s), train acc: 0.511\n",
      "Batch 30/50, loss: 0.6797540009021759 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6817752420902252 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6879560887813568 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 310/500-----\n",
      "Batch 10/50, loss: 0.6832625269889832 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6931481719017029 (0.009s), train acc: 0.522\n",
      "Batch 30/50, loss: 0.6847743093967438 (0.010s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6853553175926208 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6766837179660797 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 311/500-----\n",
      "Batch 10/50, loss: 0.6878943860530853 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6842233300209045 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6781577646732331 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.6810975134372711 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.690237694978714 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 312/500-----\n",
      "Batch 10/50, loss: 0.681354683637619 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6814212679862977 (0.009s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6901569724082947 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6956718325614929 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6700508236885071 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 313/500-----\n",
      "Batch 10/50, loss: 0.6904501974582672 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6861016273498535 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6890350639820099 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6837539613246918 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6737403035163879 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 314/500-----\n",
      "Batch 10/50, loss: 0.6866913914680481 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6873895049095153 (0.010s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.684629637002945 (0.010s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.683756297826767 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6802354216575622 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 315/500-----\n",
      "Batch 10/50, loss: 0.6810663998126983 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6898750543594361 (0.009s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6856470167636871 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6819035053253174 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6819848954677582 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 316/500-----\n",
      "Batch 10/50, loss: 0.6849671006202698 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6796754419803619 (0.010s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.6794628620147705 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6894711136817933 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.684757000207901 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 317/500-----\n",
      "Batch 10/50, loss: 0.6905826270580292 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6791554391384125 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6937022030353546 (0.010s), train acc: 0.544\n",
      "Batch 40/50, loss: 0.6828818678855896 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6789250850677491 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 318/500-----\n",
      "Batch 10/50, loss: 0.6821710884571075 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6862627506256104 (0.010s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6813119232654572 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6968655586242676 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6747577667236329 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 319/500-----\n",
      "Batch 10/50, loss: 0.6883427441120148 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6860412299633026 (0.009s), train acc: 0.556\n",
      "Batch 30/50, loss: 0.680908715724945 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6814395487308502 (0.010s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.6836785793304443 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 320/500-----\n",
      "Batch 10/50, loss: 0.688476151227951 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6892880737781525 (0.010s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.674345076084137 (0.010s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.695008271932602 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.6739229202270508 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 321/500-----\n",
      "Batch 10/50, loss: 0.6810147285461425 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6964833974838257 (0.010s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6806350290775299 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6881710052490234 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6725059628486634 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 322/500-----\n",
      "Batch 10/50, loss: 0.6850876688957215 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6757901132106781 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6872524082660675 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6920582115650177 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.6797424554824829 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 323/500-----\n",
      "Batch 10/50, loss: 0.6887647628784179 (0.009s), train acc: 0.519\n",
      "Batch 20/50, loss: 0.6804307281970978 (0.010s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.678982138633728 (0.010s), train acc: 0.560\n",
      "Batch 40/50, loss: 0.6856291711330413 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6948897242546082 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 324/500-----\n",
      "Batch 10/50, loss: 0.6881257474422455 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.693639725446701 (0.010s), train acc: 0.516\n",
      "Batch 30/50, loss: 0.6801030158996582 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6800476849079132 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6817745268344879 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 325/500-----\n",
      "Batch 10/50, loss: 0.6842103362083435 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6927866280078888 (0.010s), train acc: 0.517\n",
      "Batch 30/50, loss: 0.6898497939109802 (0.010s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6709514617919922 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.6841005742549896 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 326/500-----\n",
      "Batch 10/50, loss: 0.6836408376693726 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6767742872238159 (0.010s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6830769658088685 (0.010s), train acc: 0.564\n",
      "Batch 40/50, loss: 0.6881114184856415 (0.010s), train acc: 0.549\n",
      "Batch 50/50, loss: 0.6874823093414306 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 327/500-----\n",
      "Batch 10/50, loss: 0.6805169582366943 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.685195928812027 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6861536383628846 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6849507749080658 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6854852199554443 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 328/500-----\n",
      "Batch 10/50, loss: 0.6882272124290466 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6832907378673554 (0.009s), train acc: 0.523\n",
      "Batch 30/50, loss: 0.6837348401546478 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.680305689573288 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.689790666103363 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 329/500-----\n",
      "Batch 10/50, loss: 0.6866146266460419 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.685276597738266 (0.010s), train acc: 0.516\n",
      "Batch 30/50, loss: 0.6742083966732025 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6932094812393188 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6837129831314087 (0.010s), train acc: 0.526\n",
      "\n",
      "-----Epoch 330/500-----\n",
      "Batch 10/50, loss: 0.676623409986496 (0.009s), train acc: 0.594\n",
      "Batch 20/50, loss: 0.6872965276241303 (0.010s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6855669319629669 (0.010s), train acc: 0.556\n",
      "Batch 40/50, loss: 0.6903069317340851 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6808418273925781 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 331/500-----\n",
      "Batch 10/50, loss: 0.689093416929245 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6853213727474212 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6791711628437043 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.682312422990799 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.681968092918396 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 332/500-----\n",
      "Batch 10/50, loss: 0.6853326857089996 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6800181329250335 (0.010s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6863887131214141 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6829047977924347 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6873394548892975 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 333/500-----\n",
      "Batch 10/50, loss: 0.6802266418933869 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6753843724727631 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6785785913467407 (0.010s), train acc: 0.561\n",
      "Batch 40/50, loss: 0.6954379320144654 (0.010s), train acc: 0.551\n",
      "Batch 50/50, loss: 0.687976747751236 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 334/500-----\n",
      "Batch 10/50, loss: 0.6847510397434234 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6830092430114746 (0.010s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6770793080329895 (0.010s), train acc: 0.550\n",
      "Batch 40/50, loss: 0.6815760374069214 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6912709891796112 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 335/500-----\n",
      "Batch 10/50, loss: 0.6684284925460815 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.6905488193035125 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6848114132881165 (0.010s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6983956277370453 (0.010s), train acc: 0.519\n",
      "Batch 50/50, loss: 0.680542653799057 (0.010s), train acc: 0.523\n",
      "\n",
      "-----Epoch 336/500-----\n",
      "Batch 10/50, loss: 0.6822927892208099 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6815365314483642 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6891044974327087 (0.010s), train acc: 0.544\n",
      "Batch 40/50, loss: 0.6796594202518463 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6857517242431641 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 337/500-----\n",
      "Batch 10/50, loss: 0.6777481019496918 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.6898930847644806 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6859131395816803 (0.010s), train acc: 0.544\n",
      "Batch 40/50, loss: 0.6871499359607697 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6808465719223022 (0.010s), train acc: 0.543\n",
      "\n",
      "-----Epoch 338/500-----\n",
      "Batch 10/50, loss: 0.688028484582901 (0.009s), train acc: 0.475\n",
      "Batch 20/50, loss: 0.68320552110672 (0.010s), train acc: 0.517\n",
      "Batch 30/50, loss: 0.6824613809585571 (0.010s), train acc: 0.518\n",
      "Batch 40/50, loss: 0.6765209317207337 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6893460333347321 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 339/500-----\n",
      "Batch 10/50, loss: 0.6857284784317017 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6797338843345642 (0.009s), train acc: 0.555\n",
      "Batch 30/50, loss: 0.6835162580013275 (0.010s), train acc: 0.561\n",
      "Batch 40/50, loss: 0.6898218750953674 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6785236954689026 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 340/500-----\n",
      "Batch 10/50, loss: 0.6888634443283081 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6895748555660248 (0.010s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6875679731369019 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6847207248210907 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6703281462192535 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 341/500-----\n",
      "Batch 10/50, loss: 0.6737589120864869 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6849549531936645 (0.010s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6861182987689972 (0.010s), train acc: 0.552\n",
      "Batch 40/50, loss: 0.6869061708450317 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6870841264724732 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 342/500-----\n",
      "Batch 10/50, loss: 0.6800406754016877 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6869262278079986 (0.010s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6838088393211365 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.689597874879837 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6777618050575256 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 343/500-----\n",
      "Batch 10/50, loss: 0.6862835764884949 (0.009s), train acc: 0.575\n",
      "Batch 20/50, loss: 0.6873954057693481 (0.010s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6813546061515808 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6821465730667114 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6799919545650482 (0.010s), train acc: 0.550\n",
      "\n",
      "-----Epoch 344/500-----\n",
      "Batch 10/50, loss: 0.6835144698619843 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.682930338382721 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6909370303153992 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6896360039710998 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6729089498519898 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 345/500-----\n",
      "Batch 10/50, loss: 0.6799511849880219 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6856601297855377 (0.010s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6769936740398407 (0.010s), train acc: 0.556\n",
      "Batch 40/50, loss: 0.6901360929012299 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6907305538654327 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 346/500-----\n",
      "Batch 10/50, loss: 0.6739018976688385 (0.009s), train acc: 0.591\n",
      "Batch 20/50, loss: 0.6799502849578858 (0.009s), train acc: 0.566\n",
      "Batch 30/50, loss: 0.6856191158294678 (0.009s), train acc: 0.555\n",
      "Batch 40/50, loss: 0.7030286669731141 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.683043223619461 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 347/500-----\n",
      "Batch 10/50, loss: 0.6871218919754029 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6831393122673035 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6758444488048554 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6862770259380341 (0.010s), train acc: 0.526\n",
      "Batch 50/50, loss: 0.6844709098339081 (0.010s), train acc: 0.519\n",
      "\n",
      "-----Epoch 348/500-----\n",
      "Batch 10/50, loss: 0.6886698126792907 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6753658473491668 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6846854388713837 (0.010s), train acc: 0.520\n",
      "Batch 40/50, loss: 0.6822208404541016 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6882486462593078 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 349/500-----\n",
      "Batch 10/50, loss: 0.6832331538200378 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6764853537082672 (0.009s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.687051385641098 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6878197669982911 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6843527972698211 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 350/500-----\n",
      "Batch 10/50, loss: 0.6896353960037231 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6786168813705444 (0.010s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6878813087940217 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.6752696394920349 (0.010s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6886176466941833 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 351/500-----\n",
      "Batch 10/50, loss: 0.6736640810966492 (0.009s), train acc: 0.581\n",
      "Batch 20/50, loss: 0.6859571099281311 (0.010s), train acc: 0.562\n",
      "Batch 30/50, loss: 0.6772956073284149 (0.010s), train acc: 0.558\n",
      "Batch 40/50, loss: 0.6895585894584656 (0.010s), train acc: 0.553\n",
      "Batch 50/50, loss: 0.6879412949085235 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 352/500-----\n",
      "Batch 10/50, loss: 0.6861725568771362 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6836082100868225 (0.009s), train acc: 0.525\n",
      "Batch 30/50, loss: 0.6805523276329041 (0.010s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6863048255443573 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6817663550376892 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 353/500-----\n",
      "Batch 10/50, loss: 0.6799775958061218 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6792364239692688 (0.010s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6803322076797486 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.6882998824119568 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.688984501361847 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 354/500-----\n",
      "Batch 10/50, loss: 0.6716587662696838 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6772400736808777 (0.010s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6855098485946656 (0.010s), train acc: 0.559\n",
      "Batch 40/50, loss: 0.685030597448349 (0.010s), train acc: 0.550\n",
      "Batch 50/50, loss: 0.6990098834037781 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 355/500-----\n",
      "Batch 10/50, loss: 0.6791794002056122 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6872983276844025 (0.009s), train acc: 0.547\n",
      "Batch 30/50, loss: 0.6884561479091644 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6727313578128815 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6878244280815125 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 356/500-----\n",
      "Batch 10/50, loss: 0.6810480177402496 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6792598783969879 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6935341358184814 (0.010s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.6801778852939606 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6830516457557678 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 357/500-----\n",
      "Batch 10/50, loss: 0.6752940654754639 (0.009s), train acc: 0.566\n",
      "Batch 20/50, loss: 0.6769094347953797 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6797587454319001 (0.010s), train acc: 0.559\n",
      "Batch 40/50, loss: 0.6942636966705322 (0.010s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6882815301418305 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 358/500-----\n",
      "Batch 10/50, loss: 0.6800592422485352 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6878816187381744 (0.010s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6872875690460205 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6774543583393097 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.681958132982254 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 359/500-----\n",
      "Batch 10/50, loss: 0.681655752658844 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6796460509300232 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6887274384498596 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6885080873966217 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6770939528942108 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 360/500-----\n",
      "Batch 10/50, loss: 0.6767795324325562 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.6886212408542634 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6857973515987397 (0.010s), train acc: 0.552\n",
      "Batch 40/50, loss: 0.6820829451084137 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6831396281719208 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 361/500-----\n",
      "Batch 10/50, loss: 0.6811858892440796 (0.009s), train acc: 0.578\n",
      "Batch 20/50, loss: 0.690477055311203 (0.010s), train acc: 0.547\n",
      "Batch 30/50, loss: 0.6815814197063446 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6791251063346863 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6896462678909302 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 362/500-----\n",
      "Batch 10/50, loss: 0.6861925899982453 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6818050444126129 (0.010s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6775698959827423 (0.010s), train acc: 0.554\n",
      "Batch 40/50, loss: 0.6781878471374512 (0.010s), train acc: 0.555\n",
      "Batch 50/50, loss: 0.6929083883762359 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 363/500-----\n",
      "Batch 10/50, loss: 0.6822714686393738 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6736112475395203 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6820748746395111 (0.010s), train acc: 0.553\n",
      "Batch 40/50, loss: 0.6901054382324219 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6900200724601746 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 364/500-----\n",
      "Batch 10/50, loss: 0.6848522067070008 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6843078255653381 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6788219034671783 (0.010s), train acc: 0.554\n",
      "Batch 40/50, loss: 0.6874068021774292 (0.010s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6865528166294098 (0.010s), train acc: 0.548\n",
      "\n",
      "-----Epoch 365/500-----\n",
      "Batch 10/50, loss: 0.6770856022834778 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6820147573947907 (0.010s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6833186984062195 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6819002330303192 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6912687838077545 (0.010s), train acc: 0.543\n",
      "\n",
      "-----Epoch 366/500-----\n",
      "Batch 10/50, loss: 0.6862381339073181 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.687399023771286 (0.010s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.686245459318161 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6882865488529205 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6687294483184815 (0.010s), train acc: 0.545\n",
      "\n",
      "-----Epoch 367/500-----\n",
      "Batch 10/50, loss: 0.6767997920513154 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.68289235830307 (0.009s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6817815244197846 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.6944406747817993 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6808038532733918 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 368/500-----\n",
      "Batch 10/50, loss: 0.6798176646232605 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6823428213596344 (0.010s), train acc: 0.555\n",
      "Batch 30/50, loss: 0.6902668237686157 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6831760466098785 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6827955305576324 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 369/500-----\n",
      "Batch 10/50, loss: 0.6819965422153473 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6790718972682953 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6807231962680816 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.6803670287132263 (0.010s), train acc: 0.555\n",
      "Batch 50/50, loss: 0.6966312229633331 (0.010s), train acc: 0.545\n",
      "\n",
      "-----Epoch 370/500-----\n",
      "Batch 10/50, loss: 0.6812484204769135 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6840630888938903 (0.010s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6918070793151856 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.677127069234848 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6796036899089813 (0.010s), train acc: 0.541\n",
      "\n",
      "-----Epoch 371/500-----\n",
      "Batch 10/50, loss: 0.6835987865924835 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6796548962593079 (0.009s), train acc: 0.555\n",
      "Batch 30/50, loss: 0.6852133929729461 (0.010s), train acc: 0.552\n",
      "Batch 40/50, loss: 0.6860225260257721 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6817996740341187 (0.010s), train acc: 0.545\n",
      "\n",
      "-----Epoch 372/500-----\n",
      "Batch 10/50, loss: 0.6810955762863159 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6785018563270568 (0.010s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6803795397281647 (0.010s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.6914068341255188 (0.010s), train acc: 0.519\n",
      "Batch 50/50, loss: 0.683747547864914 (0.010s), train acc: 0.520\n",
      "\n",
      "-----Epoch 373/500-----\n",
      "Batch 10/50, loss: 0.6859348595142365 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6762004017829895 (0.009s), train acc: 0.567\n",
      "Batch 30/50, loss: 0.6993902146816253 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6777207553386688 (0.010s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.6776345312595368 (0.010s), train acc: 0.545\n",
      "\n",
      "-----Epoch 374/500-----\n",
      "Batch 10/50, loss: 0.6828818619251251 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6732197046279907 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6763217926025391 (0.010s), train acc: 0.553\n",
      "Batch 40/50, loss: 0.6825392961502075 (0.010s), train acc: 0.558\n",
      "Batch 50/50, loss: 0.69884472489357 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 375/500-----\n",
      "Batch 10/50, loss: 0.6790282964706421 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6816977262496948 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6928324460983276 (0.010s), train acc: 0.514\n",
      "Batch 40/50, loss: 0.6878996074199677 (0.010s), train acc: 0.516\n",
      "Batch 50/50, loss: 0.6714925169944763 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 376/500-----\n",
      "Batch 10/50, loss: 0.6665538609027862 (0.009s), train acc: 0.584\n",
      "Batch 20/50, loss: 0.6851308405399322 (0.009s), train acc: 0.559\n",
      "Batch 30/50, loss: 0.6955556035041809 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6782803893089294 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6861576735973358 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 377/500-----\n",
      "Batch 10/50, loss: 0.683149665594101 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6863524377346039 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6865840971469879 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6751205563545227 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6886066198348999 (0.010s), train acc: 0.548\n",
      "\n",
      "-----Epoch 378/500-----\n",
      "Batch 10/50, loss: 0.6857121646404266 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6884498357772827 (0.009s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6818550884723663 (0.010s), train acc: 0.531\n",
      "Batch 40/50, loss: 0.6752175092697144 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6831772804260254 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 379/500-----\n",
      "Batch 10/50, loss: 0.673151034116745 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6865827202796936 (0.010s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6850427091121674 (0.010s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.6922108769416809 (0.010s), train acc: 0.536\n",
      "Batch 50/50, loss: 0.6752819001674653 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 380/500-----\n",
      "Batch 10/50, loss: 0.683496642112732 (0.009s), train acc: 0.566\n",
      "Batch 20/50, loss: 0.6826321244239807 (0.009s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6722314715385437 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6938661158084869 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6843930840492248 (0.010s), train acc: 0.532\n",
      "\n",
      "-----Epoch 381/500-----\n",
      "Batch 10/50, loss: 0.6850382924079895 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6832098066806793 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6802380800247192 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.6840535879135132 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6797478914260864 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 382/500-----\n",
      "Batch 10/50, loss: 0.6754924535751343 (0.009s), train acc: 0.578\n",
      "Batch 20/50, loss: 0.6900529325008392 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6781095206737519 (0.010s), train acc: 0.558\n",
      "Batch 40/50, loss: 0.6820443212985993 (0.010s), train acc: 0.557\n",
      "Batch 50/50, loss: 0.68816437125206 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 383/500-----\n",
      "Batch 10/50, loss: 0.6834623634815216 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.687772536277771 (0.010s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6814465224742889 (0.010s), train acc: 0.550\n",
      "Batch 40/50, loss: 0.6794910669326782 (0.010s), train acc: 0.551\n",
      "Batch 50/50, loss: 0.6816184461116791 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 384/500-----\n",
      "Batch 10/50, loss: 0.6766071557998657 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6754372596740723 (0.009s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6941109001636505 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6777260184288025 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6906037092208862 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 385/500-----\n",
      "Batch 10/50, loss: 0.6885514378547668 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.6758115887641907 (0.009s), train acc: 0.517\n",
      "Batch 30/50, loss: 0.6802148699760437 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6873624205589295 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.682486218214035 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 386/500-----\n",
      "Batch 10/50, loss: 0.6774482011795044 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6811514437198639 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6828213572502136 (0.010s), train acc: 0.550\n",
      "Batch 40/50, loss: 0.6849975228309632 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6872001707553863 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 387/500-----\n",
      "Batch 10/50, loss: 0.6871341049671174 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6740412414073944 (0.010s), train acc: 0.559\n",
      "Batch 30/50, loss: 0.6849536001682281 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6821010410785675 (0.010s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6826380133628845 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 388/500-----\n",
      "Batch 10/50, loss: 0.6788317441940308 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6750059008598328 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6940976798534393 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6788916051387787 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6913037300109863 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 389/500-----\n",
      "Batch 10/50, loss: 0.6876778185367585 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6820501148700714 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6869460642337799 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.672844797372818 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6833446025848389 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 390/500-----\n",
      "Batch 10/50, loss: 0.6842904567718506 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6868416965007782 (0.010s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6800291240215302 (0.010s), train acc: 0.544\n",
      "Batch 40/50, loss: 0.6845016479492188 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6736543297767639 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 391/500-----\n",
      "Batch 10/50, loss: 0.6822959005832672 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6742611944675445 (0.009s), train acc: 0.555\n",
      "Batch 30/50, loss: 0.6801829040050507 (0.010s), train acc: 0.557\n",
      "Batch 40/50, loss: 0.6865081429481507 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6854857623577117 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 392/500-----\n",
      "Batch 10/50, loss: 0.6860310733318329 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6788510739803314 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6916023373603821 (0.010s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6808586597442627 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6767985820770264 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 393/500-----\n",
      "Batch 10/50, loss: 0.6756027102470398 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.6803200244903564 (0.009s), train acc: 0.547\n",
      "Batch 30/50, loss: 0.6861476361751556 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6853843212127686 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6852442800998688 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 394/500-----\n",
      "Batch 10/50, loss: 0.6833163022994995 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6851440250873566 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6829171657562256 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6830312490463257 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.680024528503418 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 395/500-----\n",
      "Batch 10/50, loss: 0.6830951035022735 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6880508720874786 (0.009s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6816588580608368 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.680001175403595 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6811080574989319 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 396/500-----\n",
      "Batch 10/50, loss: 0.6830992639064789 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6769565165042877 (0.009s), train acc: 0.567\n",
      "Batch 30/50, loss: 0.6888161420822143 (0.010s), train acc: 0.550\n",
      "Batch 40/50, loss: 0.6752043187618255 (0.010s), train acc: 0.559\n",
      "Batch 50/50, loss: 0.6956466853618621 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 397/500-----\n",
      "Batch 10/50, loss: 0.6863539159297943 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6739461362361908 (0.009s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6784236907958985 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.692746502161026 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6812335133552552 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 398/500-----\n",
      "Batch 10/50, loss: 0.6804264724254608 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6773493766784668 (0.009s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6906162679195404 (0.010s), train acc: 0.517\n",
      "Batch 40/50, loss: 0.6820957064628601 (0.010s), train acc: 0.526\n",
      "Batch 50/50, loss: 0.6808566153049469 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 399/500-----\n",
      "Batch 10/50, loss: 0.6760262846946716 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6893348574638367 (0.009s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.688098281621933 (0.010s), train acc: 0.532\n",
      "Batch 40/50, loss: 0.6847088396549225 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6752805352210999 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 400/500-----\n",
      "Batch 10/50, loss: 0.6817619562149048 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6759770214557648 (0.009s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6922680616378785 (0.010s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6747139990329742 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.686611944437027 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 401/500-----\n",
      "Batch 10/50, loss: 0.681702709197998 (0.009s), train acc: 0.578\n",
      "Batch 20/50, loss: 0.6793976306915284 (0.009s), train acc: 0.559\n",
      "Batch 30/50, loss: 0.6847030639648437 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.6914825081825257 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6771130084991455 (0.010s), train acc: 0.548\n",
      "\n",
      "-----Epoch 402/500-----\n",
      "Batch 10/50, loss: 0.6841460287570953 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6774106025695801 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6868100225925445 (0.010s), train acc: 0.548\n",
      "Batch 40/50, loss: 0.6808039486408234 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.682853227853775 (0.010s), train acc: 0.545\n",
      "\n",
      "-----Epoch 403/500-----\n",
      "Batch 10/50, loss: 0.6774989128112793 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6768255412578583 (0.009s), train acc: 0.569\n",
      "Batch 30/50, loss: 0.6857179045677185 (0.010s), train acc: 0.566\n",
      "Batch 40/50, loss: 0.682691079378128 (0.010s), train acc: 0.559\n",
      "Batch 50/50, loss: 0.688886570930481 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 404/500-----\n",
      "Batch 10/50, loss: 0.6857149124145507 (0.009s), train acc: 0.481\n",
      "Batch 20/50, loss: 0.6830293297767639 (0.010s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.6862136662006378 (0.010s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.6769256114959716 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6795166552066803 (0.010s), train acc: 0.533\n",
      "\n",
      "-----Epoch 405/500-----\n",
      "Batch 10/50, loss: 0.6849037647247315 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6897272288799285 (0.009s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6864610314369202 (0.010s), train acc: 0.518\n",
      "Batch 40/50, loss: 0.676262891292572 (0.010s), train acc: 0.531\n",
      "Batch 50/50, loss: 0.6743555426597595 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 406/500-----\n",
      "Batch 10/50, loss: 0.6760262668132782 (0.009s), train acc: 0.588\n",
      "Batch 20/50, loss: 0.6784056842327117 (0.010s), train acc: 0.564\n",
      "Batch 30/50, loss: 0.687336903810501 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6839754700660705 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6874739706516266 (0.010s), train acc: 0.543\n",
      "\n",
      "-----Epoch 407/500-----\n",
      "Batch 10/50, loss: 0.6805603563785553 (0.009s), train acc: 0.575\n",
      "Batch 20/50, loss: 0.6826633810997009 (0.009s), train acc: 0.559\n",
      "Batch 30/50, loss: 0.6942817568778992 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6882848083972931 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6689598739147187 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 408/500-----\n",
      "Batch 10/50, loss: 0.6745575428009033 (0.009s), train acc: 0.584\n",
      "Batch 20/50, loss: 0.6918639242649078 (0.010s), train acc: 0.577\n",
      "Batch 30/50, loss: 0.6851715385913849 (0.010s), train acc: 0.554\n",
      "Batch 40/50, loss: 0.6837973117828369 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6805875897407532 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 409/500-----\n",
      "Batch 10/50, loss: 0.6792321562767029 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6891942918300629 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6765106856822968 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6864757359027862 (0.010s), train acc: 0.531\n",
      "Batch 50/50, loss: 0.6820343494415283 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 410/500-----\n",
      "Batch 10/50, loss: 0.696860957145691 (0.009s), train acc: 0.478\n",
      "Batch 20/50, loss: 0.6797530174255371 (0.009s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.6757647871971131 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.682027131319046 (0.010s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6777234077453613 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 411/500-----\n",
      "Batch 10/50, loss: 0.6906329393386841 (0.009s), train acc: 0.509\n",
      "Batch 20/50, loss: 0.686331033706665 (0.009s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6799963414669037 (0.010s), train acc: 0.533\n",
      "Batch 40/50, loss: 0.6740923821926117 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6796909928321838 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 412/500-----\n",
      "Batch 10/50, loss: 0.6785674393177032 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6850168764591217 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6894689261913299 (0.010s), train acc: 0.535\n",
      "Batch 40/50, loss: 0.6835832536220551 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.676572448015213 (0.010s), train acc: 0.543\n",
      "\n",
      "-----Epoch 413/500-----\n",
      "Batch 10/50, loss: 0.6899032592773438 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6746882021427154 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.686572527885437 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6721229493618012 (0.010s), train acc: 0.555\n",
      "Batch 50/50, loss: 0.688282573223114 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 414/500-----\n",
      "Batch 10/50, loss: 0.6770473718643188 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6842489123344422 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6765990495681763 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6891910433769226 (0.010s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6822708249092102 (0.010s), train acc: 0.552\n",
      "\n",
      "-----Epoch 415/500-----\n",
      "Batch 10/50, loss: 0.6715661704540252 (0.009s), train acc: 0.581\n",
      "Batch 20/50, loss: 0.6932568848133087 (0.009s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6818587124347687 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.6846336305141449 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6789739966392517 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 416/500-----\n",
      "Batch 10/50, loss: 0.6819226801395416 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6789278864860535 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6864808857440948 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6819772183895111 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6841651797294617 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 417/500-----\n",
      "Batch 10/50, loss: 0.6889112234115601 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6791445970535278 (0.010s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6808570384979248 (0.010s), train acc: 0.542\n",
      "Batch 40/50, loss: 0.6790449500083924 (0.010s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.6864887952804566 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 418/500-----\n",
      "Batch 10/50, loss: 0.69055455327034 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6867126941680908 (0.009s), train acc: 0.550\n",
      "Batch 30/50, loss: 0.6796123147010803 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6779957234859466 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6777920663356781 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 419/500-----\n",
      "Batch 10/50, loss: 0.6847979068756104 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6734001636505127 (0.009s), train acc: 0.531\n",
      "Batch 30/50, loss: 0.6794913828372955 (0.010s), train acc: 0.526\n",
      "Batch 40/50, loss: 0.6836912870407105 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6922606885433197 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 420/500-----\n",
      "Batch 10/50, loss: 0.6858769953250885 (0.009s), train acc: 0.578\n",
      "Batch 20/50, loss: 0.6727378845214844 (0.009s), train acc: 0.580\n",
      "Batch 30/50, loss: 0.6789265155792237 (0.010s), train acc: 0.571\n",
      "Batch 40/50, loss: 0.6864025235176087 (0.010s), train acc: 0.560\n",
      "Batch 50/50, loss: 0.6892218649387359 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 421/500-----\n",
      "Batch 10/50, loss: 0.6767991304397583 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6786655366420746 (0.009s), train acc: 0.561\n",
      "Batch 30/50, loss: 0.6802458822727203 (0.010s), train acc: 0.556\n",
      "Batch 40/50, loss: 0.6832707107067109 (0.010s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.6911783218383789 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 422/500-----\n",
      "Batch 10/50, loss: 0.6833385288715362 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6830270886421204 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6834919154644012 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.687465351819992 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6728375196456909 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 423/500-----\n",
      "Batch 10/50, loss: 0.6805947244167327 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6721563100814819 (0.009s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6843833267688751 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6831711590290069 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6884693384170533 (0.010s), train acc: 0.548\n",
      "\n",
      "-----Epoch 424/500-----\n",
      "Batch 10/50, loss: 0.6840067625045776 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6822450518608093 (0.010s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6789279162883759 (0.010s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6791672110557556 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.685848867893219 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 425/500-----\n",
      "Batch 10/50, loss: 0.6812625467777252 (0.009s), train acc: 0.575\n",
      "Batch 20/50, loss: 0.6772102594375611 (0.010s), train acc: 0.575\n",
      "Batch 30/50, loss: 0.6863764584064483 (0.010s), train acc: 0.557\n",
      "Batch 40/50, loss: 0.6879816591739655 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6785779774188996 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 426/500-----\n",
      "Batch 10/50, loss: 0.6775440514087677 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6808014214038849 (0.009s), train acc: 0.547\n",
      "Batch 30/50, loss: 0.680764663219452 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6881890237331391 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6886758148670197 (0.010s), train acc: 0.531\n",
      "\n",
      "-----Epoch 427/500-----\n",
      "Batch 10/50, loss: 0.6823421359062195 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6721377968788147 (0.010s), train acc: 0.562\n",
      "Batch 30/50, loss: 0.6852656364440918 (0.010s), train acc: 0.555\n",
      "Batch 40/50, loss: 0.6916116893291473 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6793746232986451 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 428/500-----\n",
      "Batch 10/50, loss: 0.686905300617218 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6779945909976959 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6840943574905396 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6751816749572754 (0.010s), train acc: 0.543\n",
      "Batch 50/50, loss: 0.683368855714798 (0.010s), train acc: 0.543\n",
      "\n",
      "-----Epoch 429/500-----\n",
      "Batch 10/50, loss: 0.6777901411056518 (0.009s), train acc: 0.575\n",
      "Batch 20/50, loss: 0.6863404989242554 (0.010s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6798633575439453 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6842952251434327 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.6813766777515411 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 430/500-----\n",
      "Batch 10/50, loss: 0.6797683238983154 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6921438574790955 (0.009s), train acc: 0.519\n",
      "Batch 30/50, loss: 0.6768237233161927 (0.010s), train acc: 0.525\n",
      "Batch 40/50, loss: 0.687445604801178 (0.010s), train acc: 0.532\n",
      "Batch 50/50, loss: 0.6743780612945557 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 431/500-----\n",
      "Batch 10/50, loss: 0.6756324768066406 (0.009s), train acc: 0.566\n",
      "Batch 20/50, loss: 0.6785289347171783 (0.009s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.687075161933899 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6815949022769928 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.686147290468216 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 432/500-----\n",
      "Batch 10/50, loss: 0.6833062887191772 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6818368136882782 (0.009s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6756263971328735 (0.010s), train acc: 0.562\n",
      "Batch 40/50, loss: 0.6892181634902954 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6841787755489349 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 433/500-----\n",
      "Batch 10/50, loss: 0.6851360917091369 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6821416556835175 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6807631373405456 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6775150179862977 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6861642301082611 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 434/500-----\n",
      "Batch 10/50, loss: 0.6857443153858185 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6854457914829254 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6769512534141541 (0.010s), train acc: 0.550\n",
      "Batch 40/50, loss: 0.6772445738315582 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6851240932941437 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 435/500-----\n",
      "Batch 10/50, loss: 0.6804122745990753 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6845157444477081 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6726054787635803 (0.010s), train acc: 0.556\n",
      "Batch 40/50, loss: 0.6821251094341279 (0.010s), train acc: 0.555\n",
      "Batch 50/50, loss: 0.6948702096939087 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 436/500-----\n",
      "Batch 10/50, loss: 0.6715133428573609 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6731349289417267 (0.009s), train acc: 0.575\n",
      "Batch 30/50, loss: 0.6861866474151611 (0.010s), train acc: 0.570\n",
      "Batch 40/50, loss: 0.6898913264274598 (0.010s), train acc: 0.559\n",
      "Batch 50/50, loss: 0.6906671226024628 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 437/500-----\n",
      "Batch 10/50, loss: 0.6772338211536407 (0.009s), train acc: 0.578\n",
      "Batch 20/50, loss: 0.6814827382564544 (0.009s), train acc: 0.562\n",
      "Batch 30/50, loss: 0.6812228202819824 (0.010s), train acc: 0.554\n",
      "Batch 40/50, loss: 0.687332546710968 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6895400524139405 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 438/500-----\n",
      "Batch 10/50, loss: 0.6830457031726838 (0.009s), train acc: 0.516\n",
      "Batch 20/50, loss: 0.6758990526199341 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6816412687301636 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6827447533607482 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6903703927993774 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 439/500-----\n",
      "Batch 10/50, loss: 0.6686555206775665 (0.009s), train acc: 0.578\n",
      "Batch 20/50, loss: 0.6956955850124359 (0.009s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6892299473285675 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6808741748332977 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6775428652763367 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 440/500-----\n",
      "Batch 10/50, loss: 0.6880493223667145 (0.009s), train acc: 0.522\n",
      "Batch 20/50, loss: 0.6835806608200073 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6729488730430603 (0.010s), train acc: 0.544\n",
      "Batch 40/50, loss: 0.6848580718040467 (0.010s), train acc: 0.539\n",
      "Batch 50/50, loss: 0.684611588716507 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 441/500-----\n",
      "Batch 10/50, loss: 0.6805158734321595 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6953435361385345 (0.009s), train acc: 0.511\n",
      "Batch 30/50, loss: 0.681714141368866 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6749260365962982 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6786231279373169 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 442/500-----\n",
      "Batch 10/50, loss: 0.6802584767341614 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6839502274990081 (0.009s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.674041497707367 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6914134860038758 (0.010s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.6780395627021789 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 443/500-----\n",
      "Batch 10/50, loss: 0.6873006045818328 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6854080140590668 (0.009s), train acc: 0.517\n",
      "Batch 30/50, loss: 0.6792964398860931 (0.009s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6773579180240631 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.683556979894638 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 444/500-----\n",
      "Batch 10/50, loss: 0.6829430282115936 (0.009s), train acc: 0.556\n",
      "Batch 20/50, loss: 0.6778020799160004 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.681126457452774 (0.010s), train acc: 0.530\n",
      "Batch 40/50, loss: 0.6866744160652161 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6843923389911651 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 445/500-----\n",
      "Batch 10/50, loss: 0.6797788441181183 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6844388544559479 (0.009s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6808078050613403 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.6893760621547699 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6716984808444977 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 446/500-----\n",
      "Batch 10/50, loss: 0.6843865394592286 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6695093870162964 (0.010s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6810778081417084 (0.010s), train acc: 0.545\n",
      "Batch 40/50, loss: 0.6884490191936493 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.684593391418457 (0.010s), train acc: 0.543\n",
      "\n",
      "-----Epoch 447/500-----\n",
      "Batch 10/50, loss: 0.6814965307712555 (0.009s), train acc: 0.597\n",
      "Batch 20/50, loss: 0.6914482116699219 (0.010s), train acc: 0.562\n",
      "Batch 30/50, loss: 0.6767194330692291 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.6831851184368134 (0.010s), train acc: 0.538\n",
      "Batch 50/50, loss: 0.6795726120471954 (0.010s), train acc: 0.545\n",
      "\n",
      "-----Epoch 448/500-----\n",
      "Batch 10/50, loss: 0.6819301605224609 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6819900631904602 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.683357173204422 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.674717229604721 (0.010s), train acc: 0.555\n",
      "Batch 50/50, loss: 0.6840996205806732 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 449/500-----\n",
      "Batch 10/50, loss: 0.6854767620563507 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6791851282119751 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6879025340080261 (0.010s), train acc: 0.523\n",
      "Batch 40/50, loss: 0.6763391733169556 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6808087170124054 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 450/500-----\n",
      "Batch 10/50, loss: 0.6830235719680786 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6871862649917603 (0.010s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6847098350524903 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6765127658843995 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6739222705364227 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 451/500-----\n",
      "Batch 10/50, loss: 0.6783626675605774 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6781223893165589 (0.009s), train acc: 0.575\n",
      "Batch 30/50, loss: 0.6815709233283996 (0.010s), train acc: 0.571\n",
      "Batch 40/50, loss: 0.680594664812088 (0.010s), train acc: 0.566\n",
      "Batch 50/50, loss: 0.691884332895279 (0.010s), train acc: 0.553\n",
      "\n",
      "-----Epoch 452/500-----\n",
      "Batch 10/50, loss: 0.6919905722141266 (0.009s), train acc: 0.519\n",
      "Batch 20/50, loss: 0.6790822863578796 (0.009s), train acc: 0.528\n",
      "Batch 30/50, loss: 0.6876249492168427 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6750378966331482 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.6762716054916382 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 453/500-----\n",
      "Batch 10/50, loss: 0.674202048778534 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.6941897988319397 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6892582476139069 (0.009s), train acc: 0.520\n",
      "Batch 40/50, loss: 0.6837786853313446 (0.010s), train acc: 0.526\n",
      "Batch 50/50, loss: 0.6748336553573608 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 454/500-----\n",
      "Batch 10/50, loss: 0.6778661847114563 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6844089269638062 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6781335711479187 (0.010s), train acc: 0.537\n",
      "Batch 40/50, loss: 0.6816884279251099 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6856532275676728 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 455/500-----\n",
      "Batch 10/50, loss: 0.6771114468574524 (0.009s), train acc: 0.591\n",
      "Batch 20/50, loss: 0.6829850912094116 (0.009s), train acc: 0.564\n",
      "Batch 30/50, loss: 0.688044798374176 (0.010s), train acc: 0.552\n",
      "Batch 40/50, loss: 0.6798562705516815 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6782738566398621 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 456/500-----\n",
      "Batch 10/50, loss: 0.6848608255386353 (0.009s), train acc: 0.578\n",
      "Batch 20/50, loss: 0.6827036619186402 (0.009s), train acc: 0.566\n",
      "Batch 30/50, loss: 0.6861527442932129 (0.010s), train acc: 0.562\n",
      "Batch 40/50, loss: 0.6627526044845581 (0.010s), train acc: 0.571\n",
      "Batch 50/50, loss: 0.6917285740375518 (0.010s), train acc: 0.553\n",
      "\n",
      "-----Epoch 457/500-----\n",
      "Batch 10/50, loss: 0.6844779193401337 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.674825781583786 (0.009s), train acc: 0.570\n",
      "Batch 30/50, loss: 0.685929673910141 (0.010s), train acc: 0.556\n",
      "Batch 40/50, loss: 0.6852704584598541 (0.010s), train acc: 0.546\n",
      "Batch 50/50, loss: 0.6740968942642211 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 458/500-----\n",
      "Batch 10/50, loss: 0.6839755535125732 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.6748677551746368 (0.009s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.6889431953430176 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6813502609729767 (0.010s), train acc: 0.543\n",
      "Batch 50/50, loss: 0.6819928169250489 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 459/500-----\n",
      "Batch 10/50, loss: 0.67768674492836 (0.009s), train acc: 0.575\n",
      "Batch 20/50, loss: 0.6782495260238648 (0.009s), train acc: 0.558\n",
      "Batch 30/50, loss: 0.6795992374420166 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6887912690639496 (0.010s), train acc: 0.543\n",
      "Batch 50/50, loss: 0.6856607377529145 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 460/500-----\n",
      "Batch 10/50, loss: 0.6817772507667541 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6778424024581909 (0.009s), train acc: 0.552\n",
      "Batch 30/50, loss: 0.6834364414215088 (0.010s), train acc: 0.557\n",
      "Batch 40/50, loss: 0.680731201171875 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.683515465259552 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 461/500-----\n",
      "Batch 10/50, loss: 0.6755465269088745 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6833783268928528 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.6718929767608642 (0.009s), train acc: 0.568\n",
      "Batch 40/50, loss: 0.6900814533233642 (0.010s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.6867207050323486 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 462/500-----\n",
      "Batch 10/50, loss: 0.6800902783870697 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6862600386142731 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.6756738483905792 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6864237844944 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6854249596595764 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 463/500-----\n",
      "Batch 10/50, loss: 0.6832243084907532 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6749511063098907 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.692116892337799 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6779212296009064 (0.010s), train acc: 0.536\n",
      "Batch 50/50, loss: 0.67917320728302 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 464/500-----\n",
      "Batch 10/50, loss: 0.683441948890686 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6809013545513153 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6803484976291656 (0.010s), train acc: 0.549\n",
      "Batch 40/50, loss: 0.6892379701137543 (0.010s), train acc: 0.542\n",
      "Batch 50/50, loss: 0.6747012078762055 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 465/500-----\n",
      "Batch 10/50, loss: 0.6766940832138062 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6788834869861603 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6849575102329254 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.687622320652008 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6808446943759918 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 466/500-----\n",
      "Batch 10/50, loss: 0.6783899009227753 (0.009s), train acc: 0.581\n",
      "Batch 20/50, loss: 0.6851471602916718 (0.009s), train acc: 0.566\n",
      "Batch 30/50, loss: 0.6728818476200104 (0.010s), train acc: 0.569\n",
      "Batch 40/50, loss: 0.6834947526454925 (0.010s), train acc: 0.564\n",
      "Batch 50/50, loss: 0.6900538563728332 (0.010s), train acc: 0.551\n",
      "\n",
      "-----Epoch 467/500-----\n",
      "Batch 10/50, loss: 0.6824524939060211 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6635023593902588 (0.010s), train acc: 0.575\n",
      "Batch 30/50, loss: 0.6841822385787963 (0.010s), train acc: 0.554\n",
      "Batch 40/50, loss: 0.691743153333664 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6838995397090912 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 468/500-----\n",
      "Batch 10/50, loss: 0.6848451614379882 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6658609271049499 (0.009s), train acc: 0.584\n",
      "Batch 30/50, loss: 0.6860694944858551 (0.010s), train acc: 0.567\n",
      "Batch 40/50, loss: 0.6828384935855866 (0.010s), train acc: 0.562\n",
      "Batch 50/50, loss: 0.6857844591140747 (0.010s), train acc: 0.552\n",
      "\n",
      "-----Epoch 469/500-----\n",
      "Batch 10/50, loss: 0.676921135187149 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6891615271568299 (0.009s), train acc: 0.534\n",
      "Batch 30/50, loss: 0.6886678040027618 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.6705446600914001 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6847617149353027 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 470/500-----\n",
      "Batch 10/50, loss: 0.68103586435318 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6798418223857879 (0.010s), train acc: 0.556\n",
      "Batch 30/50, loss: 0.6774196267127991 (0.010s), train acc: 0.558\n",
      "Batch 40/50, loss: 0.6779157042503356 (0.010s), train acc: 0.549\n",
      "Batch 50/50, loss: 0.6885506689548493 (0.010s), train acc: 0.553\n",
      "\n",
      "-----Epoch 471/500-----\n",
      "Batch 10/50, loss: 0.687995970249176 (0.009s), train acc: 0.500\n",
      "Batch 20/50, loss: 0.6816115319728852 (0.010s), train acc: 0.527\n",
      "Batch 30/50, loss: 0.6764502704143525 (0.010s), train acc: 0.528\n",
      "Batch 40/50, loss: 0.6744270980358124 (0.010s), train acc: 0.540\n",
      "Batch 50/50, loss: 0.6879776358604431 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 472/500-----\n",
      "Batch 10/50, loss: 0.6670413851737976 (0.009s), train acc: 0.572\n",
      "Batch 20/50, loss: 0.6850482940673828 (0.009s), train acc: 0.559\n",
      "Batch 30/50, loss: 0.6844980418682098 (0.010s), train acc: 0.551\n",
      "Batch 40/50, loss: 0.6873589396476746 (0.010s), train acc: 0.543\n",
      "Batch 50/50, loss: 0.6801870048046113 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 473/500-----\n",
      "Batch 10/50, loss: 0.6848529577255249 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6864605903625488 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6812282204627991 (0.010s), train acc: 0.558\n",
      "Batch 40/50, loss: 0.6801711022853851 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6765372037887574 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 474/500-----\n",
      "Batch 10/50, loss: 0.684770542383194 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6897353589534759 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6889348685741424 (0.010s), train acc: 0.519\n",
      "Batch 40/50, loss: 0.6712454259395599 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6709405243396759 (0.010s), train acc: 0.537\n",
      "\n",
      "-----Epoch 475/500-----\n",
      "Batch 10/50, loss: 0.680490642786026 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6841484546661377 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6819223642349244 (0.010s), train acc: 0.542\n",
      "Batch 40/50, loss: 0.6799605667591095 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.678756719827652 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 476/500-----\n",
      "Batch 10/50, loss: 0.6820856094360351 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6743118643760682 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6799198746681213 (0.010s), train acc: 0.536\n",
      "Batch 40/50, loss: 0.6841498434543609 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.6866636455059052 (0.010s), train acc: 0.536\n",
      "\n",
      "-----Epoch 477/500-----\n",
      "Batch 10/50, loss: 0.6813826501369477 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.680820745229721 (0.009s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6781214952468873 (0.010s), train acc: 0.562\n",
      "Batch 40/50, loss: 0.6746363818645478 (0.010s), train acc: 0.562\n",
      "Batch 50/50, loss: 0.6945428311824798 (0.010s), train acc: 0.550\n",
      "\n",
      "-----Epoch 478/500-----\n",
      "Batch 10/50, loss: 0.6847992062568664 (0.009s), train acc: 0.541\n",
      "Batch 20/50, loss: 0.6768932163715362 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.6838607072830201 (0.010s), train acc: 0.542\n",
      "Batch 40/50, loss: 0.6907601296901703 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.6682230472564697 (0.010s), train acc: 0.543\n",
      "\n",
      "-----Epoch 479/500-----\n",
      "Batch 10/50, loss: 0.6811631560325623 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6856736719608307 (0.009s), train acc: 0.533\n",
      "Batch 30/50, loss: 0.671534925699234 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6823947012424469 (0.010s), train acc: 0.544\n",
      "Batch 50/50, loss: 0.6900710105895996 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 480/500-----\n",
      "Batch 10/50, loss: 0.675210976600647 (0.009s), train acc: 0.562\n",
      "Batch 20/50, loss: 0.676830404996872 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6770530641078949 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6885667026042939 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6865193784236908 (0.010s), train acc: 0.548\n",
      "\n",
      "-----Epoch 481/500-----\n",
      "Batch 10/50, loss: 0.6776543378829956 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6843579590320588 (0.010s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6764317691326142 (0.010s), train acc: 0.552\n",
      "Batch 40/50, loss: 0.6890560448169708 (0.010s), train acc: 0.550\n",
      "Batch 50/50, loss: 0.6795853495597839 (0.010s), train acc: 0.551\n",
      "\n",
      "-----Epoch 482/500-----\n",
      "Batch 10/50, loss: 0.6884246110916138 (0.009s), train acc: 0.512\n",
      "Batch 20/50, loss: 0.6761202096939087 (0.009s), train acc: 0.536\n",
      "Batch 30/50, loss: 0.6781803965568542 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.689135205745697 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.6746431112289428 (0.010s), train acc: 0.544\n",
      "\n",
      "-----Epoch 483/500-----\n",
      "Batch 10/50, loss: 0.6772298872470855 (0.009s), train acc: 0.581\n",
      "Batch 20/50, loss: 0.6710220515727997 (0.009s), train acc: 0.567\n",
      "Batch 30/50, loss: 0.6942075252532959 (0.010s), train acc: 0.544\n",
      "Batch 40/50, loss: 0.6868801593780518 (0.010s), train acc: 0.553\n",
      "Batch 50/50, loss: 0.6738686800003052 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 484/500-----\n",
      "Batch 10/50, loss: 0.6782068848609925 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.690384429693222 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6732496976852417 (0.010s), train acc: 0.553\n",
      "Batch 40/50, loss: 0.667218667268753 (0.010s), train acc: 0.554\n",
      "Batch 50/50, loss: 0.6966078400611877 (0.010s), train acc: 0.540\n",
      "\n",
      "-----Epoch 485/500-----\n",
      "Batch 10/50, loss: 0.677088838815689 (0.009s), train acc: 0.537\n",
      "Batch 20/50, loss: 0.6799788236618042 (0.010s), train acc: 0.544\n",
      "Batch 30/50, loss: 0.6766902923583984 (0.010s), train acc: 0.552\n",
      "Batch 40/50, loss: 0.6903697431087494 (0.010s), train acc: 0.533\n",
      "Batch 50/50, loss: 0.6791251599788666 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 486/500-----\n",
      "Batch 10/50, loss: 0.6733193218708038 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6801810324192047 (0.009s), train acc: 0.537\n",
      "Batch 30/50, loss: 0.684786319732666 (0.010s), train acc: 0.534\n",
      "Batch 40/50, loss: 0.6862864434719086 (0.010s), train acc: 0.537\n",
      "Batch 50/50, loss: 0.678820139169693 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 487/500-----\n",
      "Batch 10/50, loss: 0.6827023565769196 (0.009s), train acc: 0.547\n",
      "Batch 20/50, loss: 0.6753439903259277 (0.009s), train acc: 0.547\n",
      "Batch 30/50, loss: 0.6843962371349335 (0.010s), train acc: 0.546\n",
      "Batch 40/50, loss: 0.6780892014503479 (0.010s), train acc: 0.552\n",
      "Batch 50/50, loss: 0.6846499800682068 (0.010s), train acc: 0.552\n",
      "\n",
      "-----Epoch 488/500-----\n",
      "Batch 10/50, loss: 0.6732282400131225 (0.009s), train acc: 0.559\n",
      "Batch 20/50, loss: 0.6824176430702209 (0.010s), train acc: 0.553\n",
      "Batch 30/50, loss: 0.6810141265392303 (0.010s), train acc: 0.544\n",
      "Batch 40/50, loss: 0.7000472843647003 (0.010s), train acc: 0.524\n",
      "Batch 50/50, loss: 0.6711134433746337 (0.010s), train acc: 0.535\n",
      "\n",
      "-----Epoch 489/500-----\n",
      "Batch 10/50, loss: 0.6807197809219361 (0.009s), train acc: 0.528\n",
      "Batch 20/50, loss: 0.6817171216011048 (0.009s), train acc: 0.541\n",
      "Batch 30/50, loss: 0.6817340731620789 (0.010s), train acc: 0.541\n",
      "Batch 40/50, loss: 0.676860511302948 (0.010s), train acc: 0.547\n",
      "Batch 50/50, loss: 0.6834794104099273 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 490/500-----\n",
      "Batch 10/50, loss: 0.6821784019470215 (0.009s), train acc: 0.569\n",
      "Batch 20/50, loss: 0.6910264611244201 (0.009s), train acc: 0.548\n",
      "Batch 30/50, loss: 0.6703959226608276 (0.010s), train acc: 0.547\n",
      "Batch 40/50, loss: 0.6740530729293823 (0.010s), train acc: 0.549\n",
      "Batch 50/50, loss: 0.6885344982147217 (0.010s), train acc: 0.549\n",
      "\n",
      "-----Epoch 491/500-----\n",
      "Batch 10/50, loss: 0.6814550757408142 (0.009s), train acc: 0.506\n",
      "Batch 20/50, loss: 0.6783898711204529 (0.009s), train acc: 0.525\n",
      "Batch 30/50, loss: 0.6813678562641143 (0.010s), train acc: 0.522\n",
      "Batch 40/50, loss: 0.6888112366199494 (0.010s), train acc: 0.521\n",
      "Batch 50/50, loss: 0.673208475112915 (0.010s), train acc: 0.530\n",
      "\n",
      "-----Epoch 492/500-----\n",
      "Batch 10/50, loss: 0.6785140752792358 (0.009s), train acc: 0.550\n",
      "Batch 20/50, loss: 0.6840934813022613 (0.010s), train acc: 0.539\n",
      "Batch 30/50, loss: 0.6822320878505707 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6740366756916046 (0.010s), train acc: 0.550\n",
      "Batch 50/50, loss: 0.6847956061363221 (0.010s), train acc: 0.547\n",
      "\n",
      "-----Epoch 493/500-----\n",
      "Batch 10/50, loss: 0.6729346871376037 (0.009s), train acc: 0.578\n",
      "Batch 20/50, loss: 0.6827844202518463 (0.009s), train acc: 0.559\n",
      "Batch 30/50, loss: 0.6812453508377075 (0.010s), train acc: 0.550\n",
      "Batch 40/50, loss: 0.6846225500106812 (0.010s), train acc: 0.541\n",
      "Batch 50/50, loss: 0.681804871559143 (0.010s), train acc: 0.542\n",
      "\n",
      "-----Epoch 494/500-----\n",
      "Batch 10/50, loss: 0.6762988269329071 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.693103575706482 (0.009s), train acc: 0.511\n",
      "Batch 30/50, loss: 0.6755375385284423 (0.010s), train acc: 0.529\n",
      "Batch 40/50, loss: 0.6777272701263428 (0.010s), train acc: 0.535\n",
      "Batch 50/50, loss: 0.6846207022666931 (0.010s), train acc: 0.539\n",
      "\n",
      "-----Epoch 495/500-----\n",
      "Batch 10/50, loss: 0.6760669469833374 (0.009s), train acc: 0.553\n",
      "Batch 20/50, loss: 0.6823458909988404 (0.009s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.680184680223465 (0.010s), train acc: 0.542\n",
      "Batch 40/50, loss: 0.6862368822097779 (0.010s), train acc: 0.548\n",
      "Batch 50/50, loss: 0.6801924288272858 (0.010s), train acc: 0.551\n",
      "\n",
      "-----Epoch 496/500-----\n",
      "Batch 10/50, loss: 0.6886730968952179 (0.009s), train acc: 0.534\n",
      "Batch 20/50, loss: 0.6756342113018036 (0.009s), train acc: 0.542\n",
      "Batch 30/50, loss: 0.6884333968162537 (0.010s), train acc: 0.527\n",
      "Batch 40/50, loss: 0.6790780127048492 (0.010s), train acc: 0.534\n",
      "Batch 50/50, loss: 0.681418639421463 (0.010s), train acc: 0.546\n",
      "\n",
      "-----Epoch 497/500-----\n",
      "Batch 10/50, loss: 0.6789431214332581 (0.009s), train acc: 0.531\n",
      "Batch 20/50, loss: 0.6649842977523803 (0.009s), train acc: 0.566\n",
      "Batch 30/50, loss: 0.6878478944301605 (0.010s), train acc: 0.559\n",
      "Batch 40/50, loss: 0.6898227512836457 (0.010s), train acc: 0.545\n",
      "Batch 50/50, loss: 0.6887376606464386 (0.010s), train acc: 0.538\n",
      "\n",
      "-----Epoch 498/500-----\n",
      "Batch 10/50, loss: 0.7001775920391082 (0.009s), train acc: 0.478\n",
      "Batch 20/50, loss: 0.6750246167182923 (0.009s), train acc: 0.530\n",
      "Batch 30/50, loss: 0.671457850933075 (0.010s), train acc: 0.543\n",
      "Batch 40/50, loss: 0.6947664201259613 (0.010s), train acc: 0.530\n",
      "Batch 50/50, loss: 0.6705423414707183 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 499/500-----\n",
      "Batch 10/50, loss: 0.6819505214691162 (0.009s), train acc: 0.544\n",
      "Batch 20/50, loss: 0.6739316821098328 (0.009s), train acc: 0.545\n",
      "Batch 30/50, loss: 0.6932657063007355 (0.010s), train acc: 0.539\n",
      "Batch 40/50, loss: 0.6883745610713958 (0.010s), train acc: 0.527\n",
      "Batch 50/50, loss: 0.6769145607948304 (0.010s), train acc: 0.534\n",
      "\n",
      "-----Epoch 500/500-----\n",
      "Batch 10/50, loss: 0.6794848203659057 (0.009s), train acc: 0.525\n",
      "Batch 20/50, loss: 0.6828077137470245 (0.009s), train acc: 0.522\n",
      "Batch 30/50, loss: 0.6695893228054046 (0.010s), train acc: 0.540\n",
      "Batch 40/50, loss: 0.6878186523914337 (0.010s), train acc: 0.536\n",
      "Batch 50/50, loss: 0.6874634623527527 (0.010s), train acc: 0.540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenericAttackModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=121, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_x, attack_y = create_attack_dataset(s_model, s_data)\n",
    "att_dataset = GenericDataset(attack_x, attack_y)\n",
    "lr = 0.001\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "att_model = GenericAttackModel(num_feat=121).to(DEVICE)\n",
    "optimizer = optim.Adam(att_model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(att_model, optimizer, att_dataset, loss_fn, epochs, batch_size, device=DEVICE)\n",
    "att_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04a9fc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5041113219481341"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_model.eval()\n",
    "t_x, t_y = create_attack_dataset(t_model, t_data)\n",
    "\n",
    "pred = att_model(t_x.to(DEVICE)).detach().cpu()\n",
    "get_accuracy(pred, t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f10c7215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4675480769230769"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.where(np.argmax(t_y, axis=1) == 0)[0]\n",
    "get_accuracy(pred[idx], t_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ecf6732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7496991576413959"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.where(np.argmax(t_y, axis=1) == 1)[0]\n",
    "get_accuracy(pred[idx], t_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dd20d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d7058ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(list(range(6)), t_x[t_y.argmin(dim=1).to(bool)].mean(dim=0), alpha=0.4)\n",
    "plt.bar(list(range(6)), t_x[t_y.argmax(dim=1).to(bool)].mean(dim=0), alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97ad7d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(list(range(6)), attack_x[attack_y.argmin(dim=1).to(bool)].mean(dim=0), alpha=0.4)\n",
    "plt.bar(list(range(6)), attack_x[attack_y.argmax(dim=1).to(bool)].mean(dim=0), alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "982be959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6288, -1.5556,  0.2882,  5.3236, -1.2754,  0.3285]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_zero_hop(t_model, s_data.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90a4491e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0381, -2.7032, -1.2279,  4.5965, -1.2803, -1.0144]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_zero_hop(s_model, s_data.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "886dff7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6288, -1.5556,  0.2882,  5.3236, -1.2754,  0.3285]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_zero_hop(t_model, s_data.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0cc2e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327, 6], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9274b911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.7845e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0502, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2569, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.7717e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.8514e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.8504e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.4466e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.6592e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.6951e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.5780e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.9902e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.3584e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.1257e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.4908e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.4417e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.5056e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0130, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.5978e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1047e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.9085e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.9085e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.6682e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.2864e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0130, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.8359e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.0544e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.4705e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.5614e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5301e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.5811e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4444, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.4544e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.1410e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.6480e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9633e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9066e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.7931e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.7319e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6569, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.4404e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2306e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9521e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.6446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.3884e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.4044e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.7555e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.2382e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.5958e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9418e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.4619e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.5398e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.5845e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.2786e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.2181e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1569, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.1738e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5916e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1413, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9297e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.7093e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.4818e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.1372e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.9371e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.7057e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.8333e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1976e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.3720e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.7320e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.3573e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.3696e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9716e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1447e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8658e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.0389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.7163e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.8754e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2437e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.3962e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0230e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.2494e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.8077e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.6352e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.9273e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.0461e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.6783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.4214e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.6163e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1788, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9469e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.0923e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8857e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2914e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.5122e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.8333e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.9795e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1009e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1872e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.6460e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.3081e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.7566e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.7568e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.6685e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.0779e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.2285e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.3036e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.0544e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.6556e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.4634e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.4286e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3212e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.8776e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.1256e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.0976e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.2912e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for x in s_data.x:\n",
    "    t_pred = query_zero_hop(t_model, x).softmax(dim=1)[0].log()\n",
    "    s_pred = query_zero_hop(s_model, x).softmax(dim=1)[0].log()\n",
    "#     print(s_pred.softmax(dim=1)[0])\n",
    "#     print(s_pred)\n",
    "    print(F.kl_div(t_pred, s_pred, log_target=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec6c80b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2024,  0.2694],\n",
       "        [-0.2251,  0.2920],\n",
       "        [ 1.3049, -1.1046],\n",
       "        ...,\n",
       "        [-0.2102,  0.2770],\n",
       "        [-0.1613,  0.2264],\n",
       "        [ 0.3046, -0.1908]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_model(t_x.to(DEVICE)).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe48ace8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3306,  6.5233,  0.3790, -5.2127, -0.0700, -1.2676]]) 0\n",
      "tensor([[-4.2230, -2.8926, -2.3202, -0.6136, -1.0923,  9.0295]]) 1\n",
      "tensor([[-2.4567,  1.2455, -0.1968, -0.6522, -0.1249,  1.4294]]) 0\n",
      "tensor([[-0.1942,  6.4990,  1.1117, -1.6230, -1.8155, -2.8304]]) 1\n",
      "tensor([[-0.1924,  1.3156,  5.0560, -0.7706, -1.6254, -0.9846]]) 0\n",
      "tensor([[ 0.0217, -4.1570, -5.0231,  3.4691,  3.9772, -0.7018]]) 0\n",
      "tensor([[ 3.1671,  2.4182,  5.5301, -2.0429, -3.6690, -4.2726]]) 0\n",
      "tensor([[-0.0350,  8.1872,  1.3754, -1.5269, -2.2231, -3.7270]]) 0\n",
      "tensor([[-1.5291,  1.2116,  4.3174,  1.7053, -1.9073,  0.0722]]) 0\n",
      "tensor([[-1.4291, -0.4917,  7.3205,  2.4674, -1.2470, -2.4339]]) 1\n",
      "tensor([[-2.8103,  1.2664, -0.1583,  0.6598,  2.2892, -0.2350]]) 1\n",
      "tensor([[ 3.9910,  2.6695, -1.2225, -0.9036, -1.0643, -5.1664]]) 1\n",
      "tensor([[-2.5793, -2.8477, -2.4662, -0.0145, -0.1930,  7.0797]]) 0\n",
      "tensor([[-0.2909,  1.8908,  3.4079,  2.4875, -1.9633, -2.7896]]) 0\n",
      "tensor([[-1.2980, -0.4327, -1.5348, -0.6909,  0.2168,  2.6888]]) 0\n",
      "tensor([[-3.8837, -1.2318, -2.6317,  1.5830,  1.0158,  3.3601]]) 1\n",
      "tensor([[-0.6373,  0.6990,  9.7052, -0.8129, -1.8816, -1.3060]]) 1\n",
      "tensor([[-2.2604, -0.6313, -1.2923, -0.2364,  3.9360, -1.7078]]) 1\n",
      "tensor([[-0.6273,  0.8134,  5.8346, -1.2180, -2.5335, -0.2338]]) 0\n",
      "tensor([[ 0.3303,  4.9785, -0.2054, -3.1441, -2.1805, -0.5417]]) 0\n",
      "tensor([[-2.6696, -0.5271,  4.2547, -0.0501, -0.7906,  1.8542]]) 1\n",
      "tensor([[-0.8852,  0.2347,  2.1396,  4.1488, -1.6985, -1.7907]]) 1\n",
      "tensor([[-1.0994, -2.1530,  1.1726,  7.0415, -2.1071, -0.8076]]) 1\n",
      "tensor([[ 2.9017,  1.3137, -2.9425, -1.7335, -0.0043, -3.2534]]) 0\n",
      "tensor([[-1.6952,  6.8993,  0.6159, -0.5266, -1.7602, -1.6556]]) 1\n",
      "tensor([[-0.7846, -1.0633, -2.1648,  0.7324, -0.5326,  3.3775]]) 0\n",
      "tensor([[-0.0950,  7.3928, -0.1430, -3.3569,  0.3334, -5.2221]]) 1\n",
      "tensor([[-1.6911,  0.2099,  1.1900,  0.4809, -2.6657,  3.1879]]) 0\n",
      "tensor([[-0.8612, -0.4620,  3.1344,  3.7515, -1.5778, -1.5486]]) 0\n",
      "tensor([[-0.7403, -1.6941,  0.4563,  3.6927, -0.6292, -0.0518]]) 1\n",
      "tensor([[-2.3156, -2.6346, -0.6798,  1.6171, -0.6185,  5.5708]]) 1\n",
      "tensor([[-2.1989, -3.5558,  3.1081,  2.8446, -0.5022,  2.0143]]) 0\n",
      "tensor([[ 0.0107,  6.7639, -1.0207,  1.6292, -0.8672, -4.3077]]) 1\n",
      "tensor([[-4.2137, -1.5683, -1.2522, -1.3215,  8.1208, -1.1899]]) 1\n",
      "tensor([[ 0.1806,  1.4291,  0.5945, -0.5728, -0.1048, -1.2477]]) 0\n",
      "tensor([[ 0.4431,  1.2582, -3.4616,  0.1918,  3.0783, -5.1205]]) 0\n",
      "tensor([[-2.0245, -0.4142,  5.7696,  0.9162, -1.7589,  1.4617]]) 0\n",
      "tensor([[ 0.0508,  5.8034, -0.4082, -0.7856, -0.4031, -3.1344]]) 0\n",
      "tensor([[ 1.7559,  0.5349,  8.2260, -0.7561, -1.4046, -5.2917]]) 0\n",
      "tensor([[-1.7670,  1.2731, -2.0104, -2.0164,  4.0044, -2.5377]]) 0\n",
      "tensor([[-2.6732, -1.9034,  0.7456, -0.0426, -0.9110,  5.1618]]) 1\n",
      "tensor([[-3.5739, -2.0246, -1.6350, -1.7582, -1.0656,  7.9631]]) 0\n",
      "tensor([[-1.8899, -0.8813, -1.5519,  2.8484,  1.5213, -0.6226]]) 0\n",
      "tensor([[ 0.2736, -1.9029, -4.7009, -1.5146,  6.6882, -4.6538]]) 1\n",
      "tensor([[-1.7415, -0.9536,  5.2016, -0.4327, -0.3527,  0.8290]]) 0\n",
      "tensor([[-0.0545,  5.8631, -3.2905,  0.4355,  1.2810, -4.9242]]) 0\n",
      "tensor([[-0.0703,  6.6580, -1.8894, -1.9751, -0.1214, -2.0752]]) 0\n",
      "tensor([[ 2.6030, -0.8542, -2.0650,  0.0277, -1.6611, -0.7707]]) 1\n",
      "tensor([[ 1.1761,  4.3281, -1.6795, -3.0946, -1.4787, -3.0701]]) 0\n",
      "tensor([[-1.2649, -1.0263,  3.9617,  2.8345, -0.9543, -0.5106]]) 0\n",
      "tensor([[-0.3316,  9.4481,  0.8359, -3.2342, -1.6247, -3.8813]]) 0\n",
      "tensor([[-0.1297,  5.9283, -0.7054,  0.0287, -2.5731, -1.3810]]) 1\n",
      "tensor([[-3.2215,  0.4874, -3.9282, -1.6233,  9.2452, -5.5993]]) 0\n",
      "tensor([[-0.1200, -2.7693, -0.8849,  0.7268,  3.0312, -1.6547]]) 0\n",
      "tensor([[ 0.4374,  8.6717,  0.7442, -2.3616, -2.1306, -4.2613]]) 0\n",
      "tensor([[-0.8224,  0.9018, -2.8558, -0.6756,  5.5143, -4.4343]]) 1\n",
      "tensor([[-0.8039,  0.8702, -2.2817,  0.6762,  0.1223, -0.0878]]) 0\n",
      "tensor([[-2.3421, -0.5931, -0.6420,  6.8331, -0.3247, -1.3253]]) 1\n",
      "tensor([[ 0.5691,  0.4415, -2.4079, -0.3207,  0.7959, -1.9594]]) 0\n",
      "tensor([[-2.9590, -0.6453,  1.3317, -0.1889, -0.1236,  3.0482]]) 1\n",
      "tensor([[ 1.8339,  6.1688, -2.1288, -3.7514,  1.6347, -7.4593]]) 1\n",
      "tensor([[-0.9527, -2.1100, -0.1836,  6.2867,  1.3477, -1.5493]]) 1\n",
      "tensor([[ 1.4666,  7.6095, -1.0825, -2.5985, -0.6177, -5.4268]]) 1\n",
      "tensor([[ 2.8849,  3.1219,  0.0282, -2.5051, -1.5229, -4.1418]]) 0\n",
      "tensor([[-2.2430,  0.3824, -0.0330, -2.5138,  0.1614,  3.7515]]) 0\n",
      "tensor([[ 4.9397, -0.1395, -4.2637, -2.6855,  0.7004, -6.4958]]) 0\n",
      "tensor([[ 3.9740,  1.3748, -0.9832, -1.1041, -1.9749, -3.7306]]) 1\n",
      "tensor([[-1.4400, -2.3766,  0.0540,  7.3106, -1.3038, -0.1333]]) 1\n",
      "tensor([[ 0.0917,  5.5436,  0.3853, -0.5658, -1.8357, -4.2508]]) 0\n",
      "tensor([[-1.6139,  1.6495,  0.0100, -1.1625,  0.9127,  0.9179]]) 0\n",
      "tensor([[ 1.6749, -0.3440, -0.3437, -0.2138, -0.5022, -2.6476]]) 1\n",
      "tensor([[-1.6717, -2.4175, -2.9173,  0.9056,  4.5708, -0.7631]]) 0\n",
      "tensor([[-0.5942,  0.2816,  1.7112,  3.0799, -0.8786, -1.5599]]) 0\n",
      "tensor([[ 0.0851,  2.2810, -1.3850, -2.0399,  1.5363, -1.6501]]) 0\n",
      "tensor([[-1.0812, -0.7364,  2.5946,  4.5045, -0.9538, -1.1186]]) 0\n",
      "tensor([[ 0.3488,  1.3495,  3.9483, -0.2312, -0.3277, -3.6333]]) 0\n",
      "tensor([[-0.9444,  0.9404,  8.0284, -0.6039, -1.5251, -1.2539]]) 1\n",
      "tensor([[-0.8488, -0.1735,  4.4064, -0.4784, -2.0307,  0.9764]]) 1\n",
      "tensor([[-2.5238, -2.9308, -3.4556,  1.7518,  9.3961, -3.1157]]) 1\n",
      "tensor([[-0.0927,  0.2401, -2.1294, -0.5588,  0.0814, -0.4521]]) 0\n",
      "tensor([[-0.1671,  0.6241,  3.2148, -0.5618, -1.4749,  0.1435]]) 0\n",
      "tensor([[ 0.7437,  0.4877,  2.9577, -0.3341, -1.7065, -0.8582]]) 1\n",
      "tensor([[-1.6263, -0.8329, -2.5365,  5.3299,  0.1963, -1.6492]]) 0\n",
      "tensor([[-0.6800, -0.8389,  7.4215,  2.7648, -2.4901, -2.2456]]) 1\n",
      "tensor([[-1.7552, -1.9484, -1.3245,  3.9321,  2.0673,  0.2680]]) 0\n",
      "tensor([[-0.2074, -0.3533,  0.6698,  2.7484, -1.8430, -0.1280]]) 0\n",
      "tensor([[-0.2223, -0.9222,  1.0888,  5.1788, -2.5098, -1.0995]]) 1\n",
      "tensor([[-0.6621, -1.7804, -2.1337,  1.9293, -1.7300,  3.2278]]) 0\n",
      "tensor([[-0.6229, -3.0119, -1.8078,  7.3639, -0.8053, -0.4924]]) 0\n",
      "tensor([[-0.4808,  1.4017,  6.1977, -0.5271, -2.2307, -1.6894]]) 1\n",
      "tensor([[-1.3301, -2.7094,  0.3073,  9.5323, -1.4175, -1.5323]]) 1\n",
      "tensor([[-2.3938, -0.5878, -0.1645,  4.0281, -0.4328,  0.1827]]) 0\n",
      "tensor([[-1.0536,  0.7787,  0.0222,  0.2510,  2.1828, -2.2776]]) 0\n",
      "tensor([[-1.2399,  0.9300,  1.1874, -0.4662, -2.2337,  2.7393]]) 0\n",
      "tensor([[-0.3064,  2.6597, -2.9677, -3.2023,  4.2490, -3.2704]]) 1\n",
      "tensor([[-1.3307, -3.5708, -1.5516,  8.5820, -1.2855,  0.6356]]) 1\n",
      "tensor([[-0.2931, -1.2047, -1.6101, -0.4228,  4.8316, -2.2342]]) 1\n",
      "tensor([[-0.8428,  4.7898, -2.2293,  3.4460, -1.6015, -3.1157]]) 0\n",
      "tensor([[-2.2578, -1.2951, -1.3358,  3.2928, -1.5297,  3.3004]]) 0\n",
      "tensor([[-0.1637, -0.6171, -1.9356, -1.3573,  5.9379, -4.9853]]) 1\n",
      "tensor([[ 0.5189, -0.1540,  0.2589,  4.4675, -2.5427, -1.7857]]) 1\n",
      "tensor([[ 1.6167,  2.0078,  0.8928,  1.7542, -0.6874, -4.8998]]) 0\n",
      "tensor([[-1.4520, -3.2889, -1.8573,  6.3685, -0.0246, -0.2503]]) 1\n",
      "tensor([[-2.5320, -1.9077, -0.4569,  0.9529, -0.0326,  4.6481]]) 1\n",
      "tensor([[-1.3114, -1.7647, -2.7259,  4.5837,  0.0646,  0.1034]]) 1\n",
      "tensor([[-1.4900, -2.5489, -1.4595,  6.8173, -0.8983,  0.1202]]) 1\n",
      "tensor([[-0.8856, -0.9952,  0.0503,  1.4188,  0.2900, -0.3612]]) 0\n",
      "tensor([[-0.6164,  6.7078, -1.8010, -3.5919,  1.2823, -2.5578]]) 1\n",
      "tensor([[-4.5948, -1.9032, -0.2976, -0.2205, -1.7617,  8.4874]]) 1\n",
      "tensor([[-0.0182,  7.3252,  2.0510, -1.3763, -3.0184, -2.7646]]) 1\n",
      "tensor([[-1.7938, -3.7570, -1.7911, 10.6334, -0.3148, -1.0428]]) 1\n",
      "tensor([[ 0.3662, 10.2512, -1.8729, -3.4767,  0.3800, -4.8902]]) 1\n",
      "tensor([[-0.4683, -0.6607, -0.2086,  3.8861, -0.4099, -1.2477]]) 1\n",
      "tensor([[ 0.5052,  2.2481,  6.8207, -1.4230, -2.7160, -2.9184]]) 0\n",
      "tensor([[ 0.1638,  3.4619,  0.0295, -1.0135, -1.4789, -0.6950]]) 0\n",
      "tensor([[-1.2419, -1.3951,  2.5642,  0.3389, -0.1347,  1.2444]]) 0\n",
      "tensor([[-2.3787, -2.3471,  1.0880,  1.8274, -1.0984,  3.5267]]) 0\n",
      "tensor([[-1.7665,  0.2521, -4.1532,  0.1727,  6.0561, -3.2875]]) 1\n",
      "tensor([[-2.9219, -1.2329, -1.5210,  0.4105,  5.6761, -1.2586]]) 1\n",
      "tensor([[-1.2374, -1.8523,  5.5328,  1.4319, -2.3090,  1.3414]]) 1\n",
      "tensor([[-1.2805, -3.6086, -1.3786,  0.6637, -0.0662,  4.6413]]) 1\n",
      "tensor([[-1.9645, -2.1978,  2.0315,  7.3345, -1.1458, -1.3382]]) 1\n",
      "tensor([[-0.0246,  0.6600,  4.4856,  1.3376, -0.2481, -3.5847]]) 0\n",
      "tensor([[-0.1682,  7.5939,  0.6710,  0.3128, -2.8134, -4.4848]]) 1\n",
      "tensor([[-0.2097,  1.8942,  5.1423,  1.3826, -3.3968, -1.4830]]) 1\n",
      "tensor([[-0.8458,  1.5948, -1.3080,  4.4234, -0.3391, -2.5171]]) 0\n",
      "tensor([[-2.1895,  0.2100,  1.9140,  5.4991, -1.0457, -1.4196]]) 1\n",
      "tensor([[-0.2834,  5.0018, -1.4228, -2.6903, -0.6761, -0.9817]]) 1\n",
      "tensor([[-2.6714, -0.3824, -2.8030, -0.3697, -0.8855,  5.4720]]) 1\n",
      "tensor([[-2.5312, -1.4034, -0.4236,  0.1146,  0.0549,  4.7842]]) 0\n",
      "tensor([[-3.0134, -1.3485, -0.8867, -0.1905, -1.0220,  5.7461]]) 0\n",
      "tensor([[-5.4292, -0.0415, -2.0082,  0.0650, -2.4426,  9.0002]]) 1\n",
      "tensor([[-0.0991,  0.3009,  2.9549,  1.3286, -1.0989, -1.3752]]) 0\n",
      "tensor([[ 1.0712, -1.1194,  1.8892,  0.2425, -1.4081, -0.1982]]) 0\n",
      "tensor([[-2.6545,  2.9767,  3.6307, -1.2066, -2.3359,  2.0386]]) 0\n",
      "tensor([[ 0.1663, -1.2729, -1.4245,  5.3593, -0.4537, -1.7954]]) 1\n",
      "tensor([[ 0.5527, -0.5063, -1.1398, -0.7329, -5.2330,  4.2016]]) 0\n",
      "tensor([[-0.4082, -0.2976,  7.7866, -0.5529, -1.5856, -1.9437]]) 1\n",
      "tensor([[-2.8730, -3.1860,  3.1938,  1.5373, -0.1264,  3.1056]]) 0\n",
      "tensor([[-0.7015, -2.6828, -0.4745,  7.9474,  0.1589, -1.8788]]) 1\n",
      "tensor([[-0.4711, -1.5621, -0.1627,  5.5930, -1.2201, -0.9173]]) 0\n",
      "tensor([[-2.3172, -3.4168, -0.4784,  4.8925, -1.2795,  2.5939]]) 1\n",
      "tensor([[-0.1858,  5.8504,  0.3429, -1.3706, -0.3383, -4.0056]]) 1\n",
      "tensor([[-4.3958,  0.6542, -5.6606,  0.5806,  5.0509, -0.6972]]) 0\n",
      "tensor([[-0.7461, 10.4458, -0.8334, -1.7326, -1.5710, -5.3526]]) 0\n",
      "tensor([[-0.7846,  0.0046, -1.7849,  2.6000,  0.1275, -0.6311]]) 0\n",
      "tensor([[-1.1140,  0.4640,  0.8615,  3.6897, -2.4800, -0.0290]]) 1\n",
      "tensor([[-3.6170, -0.9498, -0.9254,  1.0249, -0.0198,  5.0291]]) 1\n",
      "tensor([[ 0.4794,  9.9505, -0.9765, -1.8385, -1.0368, -4.5829]]) 1\n",
      "tensor([[-1.7297, -2.8788, -1.4502,  6.7100,  1.0386, -0.6134]]) 0\n",
      "tensor([[-1.1127, -0.2148,  0.5985,  0.4758,  3.0910, -3.3114]]) 1\n",
      "tensor([[-0.4606,  1.9336,  4.9783,  0.3202, -3.6064, -0.8962]]) 0\n",
      "tensor([[-1.8484,  8.7984, -2.0037, -4.0834, -0.1937, -1.1969]]) 1\n",
      "tensor([[ 0.7933,  4.4057, -2.6679, -1.2283,  1.2462, -4.6127]]) 0\n",
      "tensor([[-0.3409, -0.9182,  4.9323,  2.3539, -1.4029, -1.6722]]) 0\n",
      "tensor([[-1.4349, -1.3573, -0.0336,  6.2712, -1.2126, -0.6749]]) 1\n",
      "tensor([[-0.3201, -3.0746, -1.5703,  7.3694, -1.4703, -0.5106]]) 1\n",
      "tensor([[-0.7323, -0.0924,  2.7720,  5.6705, -1.6045, -2.7370]]) 0\n",
      "tensor([[-1.5298, -1.0386,  7.4288,  3.9530, -2.0420, -1.7124]]) 0\n",
      "tensor([[-0.9720,  4.8438, -3.5196, -2.9012,  1.9666, -2.4442]]) 1\n",
      "tensor([[-3.2949, -2.1858, -0.0160,  0.5241,  4.2468,  0.3878]]) 0\n",
      "tensor([[-0.0486,  6.9460, -0.6582, -1.4387,  0.2093, -4.1808]]) 0\n",
      "tensor([[ 0.9069,  5.9808, -2.2940, -2.7744, -0.6791, -2.5783]]) 1\n",
      "tensor([[-0.7688,  1.0700,  5.0970, -0.9993, -1.2975, -0.7773]]) 1\n",
      "tensor([[-0.1017, -1.0800, -0.7512,  3.6364, -0.2926, -0.9900]]) 0\n",
      "tensor([[ 0.3383,  3.9206, -2.0117, -0.8967, -0.4432, -3.1638]]) 0\n",
      "tensor([[-3.1279, -2.7546, -0.5306,  0.1292, -0.2666,  7.4853]]) 1\n",
      "tensor([[ 1.0707,  0.3187, -3.2538, -0.4573,  2.3960, -2.4776]]) 0\n",
      "tensor([[-0.9038, -2.3042, -0.5905, -0.6510, -0.1111,  3.6509]]) 0\n",
      "tensor([[ 2.4686,  1.3461, -1.8801,  2.2643, -3.4214, -1.8514]]) 1\n",
      "tensor([[ 2.7489, -1.5740, -2.4123,  5.7206, -1.5500, -3.3177]]) 0\n",
      "tensor([[-0.7764, -2.7805, -0.9457,  6.4623, -0.4377, -0.4053]]) 1\n",
      "tensor([[ 1.9377, -0.0075, -2.6846, -2.2526, -0.9109,  1.4448]]) 1\n",
      "tensor([[-1.7464, -1.1994, -3.9763, -0.1378,  6.7976, -1.8277]]) 0\n",
      "tensor([[-2.0513, -2.3362, -1.3229,  2.2037,  0.5350,  2.1643]]) 1\n",
      "tensor([[-0.1582,  3.5158,  5.2952, -1.9257, -1.4102, -2.3644]]) 1\n",
      "tensor([[-1.8402, -0.8584, -0.6894,  0.7167,  0.5388,  1.6198]]) 1\n",
      "tensor([[ 0.1561,  0.7316,  3.2149, -0.2443, -1.4749, -1.5935]]) 0\n",
      "tensor([[ 0.0547,  0.0243,  2.9582,  2.5697, -2.1362, -1.6928]]) 0\n",
      "tensor([[-2.2331, -0.1788, -2.5412, -0.3367,  0.4900,  3.7537]]) 1\n",
      "tensor([[-2.4341, -0.3046,  1.7572,  0.0982, -0.5626,  3.1923]]) 0\n",
      "tensor([[ 0.2029, -0.5677, -1.1348,  3.7663, -0.5579, -1.9212]]) 0\n",
      "tensor([[-1.2770,  0.9017,  5.6979,  1.7098, -2.5092, -0.1263]]) 1\n",
      "tensor([[-1.0487,  8.2831, -2.5300, -1.9874,  0.1168, -2.6672]]) 1\n",
      "tensor([[-0.9158, -0.7654,  8.1220, -1.4696, -1.7418,  0.3917]]) 1\n",
      "tensor([[ 1.2509,  0.2379,  3.5983,  0.0240, -1.9105, -1.3301]]) 1\n",
      "tensor([[-2.5288,  5.2137,  1.2207, -1.8478, -0.7755, -0.1854]]) 0\n",
      "tensor([[ 1.3210,  8.4094, -0.2219, -3.5823, -1.9147, -4.4830]]) 0\n",
      "tensor([[-0.5938, -2.4163, -3.5470,  1.7614,  2.5698, -0.5537]]) 0\n",
      "tensor([[-4.2275, -0.0833, -0.9465, -1.3390, -1.2571,  7.7437]]) 1\n",
      "tensor([[ 0.3133,  4.7540, -1.2462, -1.1460,  0.5492, -3.6073]]) 0\n",
      "tensor([[-2.1637, -2.1346, -0.4363,  0.5446,  3.8002, -0.6224]]) 1\n",
      "tensor([[ 3.8896,  3.4978, -2.7342, -2.8156, -2.6699, -3.1859]]) 0\n",
      "tensor([[ 0.2262, -1.1231, -1.3523,  5.9682, -0.3568, -2.6435]]) 0\n",
      "tensor([[-1.9544, -0.7651, -1.4255, -0.6536,  0.9346,  1.6818]]) 0\n",
      "tensor([[ 5.2706, -0.8274, -1.9589, -0.8439, -2.0189, -4.3464]]) 1\n",
      "tensor([[ 0.3420, -4.4703, -1.9394, 10.0287, -1.6165, -1.6187]]) 1\n",
      "tensor([[-2.8735,  1.9560, -4.7754, -2.2189,  4.0506,  0.6359]]) 0\n",
      "tensor([[-0.6964, -0.7897,  5.3481,  1.8837, -1.8571, -0.6646]]) 1\n",
      "tensor([[ 0.1548,  1.9480,  2.4089,  1.9379, -2.3716, -1.8573]]) 0\n",
      "tensor([[ 0.2764,  6.4515, -2.1478, -0.8832,  0.0477, -4.2802]]) 0\n",
      "tensor([[-0.6804,  5.0621, -0.6182,  1.0319, -1.3957, -2.3298]]) 1\n",
      "tensor([[-0.2561,  0.9535, -3.4339, -1.3647,  4.3592, -4.4768]]) 0\n",
      "tensor([[-0.9869, -1.5609,  4.4282,  1.1621, -0.5468, -0.1394]]) 0\n",
      "tensor([[-1.2124, -1.0186, -0.4023,  4.2476,  1.0418, -1.0445]]) 1\n",
      "tensor([[-0.6583,  4.0835, -0.6350, -0.8636,  0.1494, -1.8073]]) 0\n",
      "tensor([[-3.1755, -1.4287, -1.2259, -0.3008,  4.5927,  0.5076]]) 1\n",
      "tensor([[ 0.7106,  1.2331,  7.5114,  1.3562, -3.2821, -3.0751]]) 0\n",
      "tensor([[-1.7335, -1.6361,  0.9797,  0.7174,  1.2340,  0.6374]]) 0\n",
      "tensor([[-3.5903, -0.9628,  1.5027, -1.1300,  4.1775,  0.6970]]) 1\n",
      "tensor([[ 1.4646,  0.5455,  0.1614,  0.2995, -0.6043, -3.0244]]) 0\n",
      "tensor([[ 4.2442,  2.1779, -2.4311, -0.3953, -2.4675, -4.2971]]) 1\n",
      "tensor([[ 3.1601, -3.0675, -3.1207,  6.4723, -2.3022, -3.1047]]) 0\n",
      "tensor([[-1.7229, -1.2529, -1.6927,  0.0124, -0.6612,  5.0220]]) 0\n",
      "tensor([[-2.1336,  1.0845,  1.9352,  0.3040, -0.1818, -0.3367]]) 0\n",
      "tensor([[-1.2035,  4.9157,  5.1627, -3.1344, -0.8887, -2.7389]]) 1\n",
      "tensor([[-0.9541,  4.8079,  0.3605,  1.0956, -1.9182, -1.8991]]) 0\n",
      "tensor([[-1.9903,  9.1446, -0.2077, -3.0031,  0.3774, -2.9554]]) 1\n",
      "tensor([[-0.7367, -0.5325,  5.4116,  1.8244, -2.2501, -0.2724]]) 0\n",
      "tensor([[-0.5210, -0.4037, -4.6661, -1.5733,  5.5694, -3.1898]]) 1\n",
      "tensor([[-1.7007,  0.3839, -3.7935, -2.4553,  5.3273, -1.4586]]) 1\n",
      "tensor([[-0.0702,  1.0686,  0.5188,  0.5466, -3.0834,  1.6615]]) 0\n",
      "tensor([[-1.6863,  1.3170,  5.0141,  0.7477, -0.5482, -0.9404]]) 1\n",
      "tensor([[-0.4142,  1.1939,  6.3498,  2.1109, -2.4982, -1.9790]]) 1\n",
      "tensor([[-3.6761, -0.4074, -1.0441,  1.2179, -0.8766,  5.4375]]) 1\n",
      "tensor([[-0.8967,  3.3753,  5.9047,  0.8302, -3.3517, -1.3119]]) 0\n",
      "tensor([[-1.5117,  0.0453,  8.3553,  1.5110, -2.9124, -0.1441]]) 0\n",
      "tensor([[-1.7453, -2.6511,  0.9767,  7.5634, -1.4270, -0.8181]]) 1\n",
      "tensor([[-0.1465, -0.0441,  8.7275, -0.0445, -2.5354, -1.5922]]) 0\n",
      "tensor([[-0.5314, -1.7924, -0.2322,  7.8035, -1.9183, -1.2503]]) 0\n",
      "tensor([[ 0.5194, -2.0150, -2.8343, -2.4948,  4.1099, -2.2344]]) 0\n",
      "tensor([[-1.7555,  0.4740, -1.0779,  0.6958,  0.0332,  0.9527]]) 0\n",
      "tensor([[ 3.2412, -0.6910, -2.6012, -0.9036, -1.0122, -3.3801]]) 1\n",
      "tensor([[-2.3385, -1.1738, -0.6817,  1.1117, -0.9512,  4.7996]]) 1\n",
      "tensor([[-1.3397, -1.1647, -2.4594, -1.0218,  0.2131,  3.7095]]) 0\n",
      "tensor([[-2.0022,  1.1696, -0.3048, -0.2977,  0.0234,  1.5939]]) 0\n",
      "tensor([[-1.4787, -1.3192,  0.0705,  2.2648,  2.7582, -2.4210]]) 0\n",
      "tensor([[-3.4946, -1.3572,  3.8548,  0.3885, -2.0182,  5.0304]]) 1\n",
      "tensor([[-3.3444, -2.5347, -1.3889,  0.8149, -1.0736,  6.1187]]) 1\n",
      "tensor([[-1.4581, -3.9253, -0.2851,  8.1575,  0.6476, -1.9006]]) 0\n",
      "tensor([[-1.7274,  0.0161, -4.2033,  0.2569,  7.0993, -3.4790]]) 0\n",
      "tensor([[ 2.2323,  7.4844, -1.0279, -2.4668, -2.5973, -5.1345]]) 1\n",
      "tensor([[-0.6418,  3.5700, -2.4819, -0.6229,  0.9443, -2.7031]]) 0\n",
      "tensor([[-0.9983,  4.7009, -3.8814, -2.3325,  1.5251, -2.3931]]) 0\n",
      "tensor([[-1.3629, -1.6468,  0.7822,  4.1638,  0.6756, -1.0031]]) 0\n",
      "tensor([[-3.0071,  0.3288,  6.8388,  0.4142, -0.4190,  0.4767]]) 1\n",
      "tensor([[-1.8194, -2.0477, -0.8515,  2.4609, -0.2023,  2.4683]]) 1\n",
      "tensor([[-1.8753, -0.9664,  0.2460,  0.6810, -1.6362,  3.7550]]) 1\n",
      "tensor([[-1.1138,  2.8371,  4.8067,  0.7876, -2.0477, -1.4971]]) 0\n",
      "tensor([[-2.6125, -3.1604, -2.7832,  2.1692,  0.2573,  4.7501]]) 0\n",
      "tensor([[-1.0147,  3.2070,  1.0806, -0.4934, -1.2927,  0.0306]]) 0\n",
      "tensor([[-1.7546, -2.0945,  1.2662, -0.5934, -1.7819,  5.5417]]) 1\n",
      "tensor([[ 0.0140,  5.9688,  0.0472, -0.1832, -1.6563, -3.5917]]) 1\n",
      "tensor([[ 1.2838,  4.8458, -2.4256, -1.8066, -0.9449, -3.1195]]) 1\n",
      "tensor([[-1.3192, -1.5823, -1.5630, -0.8108, -1.0688,  3.8277]]) 0\n",
      "tensor([[-2.5048, -2.8332,  1.4126,  7.1732, -0.5265, -0.4534]]) 1\n",
      "tensor([[ 0.4046,  7.6768, -2.3998, -4.6631, -0.7202, -2.5178]]) 1\n",
      "tensor([[-1.8433,  3.9554, -3.9981, -2.1077,  4.8691, -5.0143]]) 1\n",
      "tensor([[-3.2524, -0.8565, -2.8768,  0.8284,  5.5543, -1.1298]]) 1\n",
      "tensor([[-0.8049,  5.8676,  2.6550, -0.2348, -2.3855, -2.0222]]) 1\n",
      "tensor([[-1.6279,  2.9191, -0.8126,  1.0032,  0.3509, -1.5600]]) 0\n",
      "tensor([[-0.4764,  5.2612, -2.9692, -2.4552,  0.0263, -1.6863]]) 1\n",
      "tensor([[ 1.3208, -4.8490, -4.3882,  9.0826, -1.0400, -1.1382]]) 1\n",
      "tensor([[-0.7042,  3.8373, -0.3761, -1.2381, -0.2733, -0.6077]]) 0\n",
      "tensor([[-2.9694, -1.0516, -1.8624, -1.0337,  1.3790,  3.4928]]) 0\n",
      "tensor([[-0.9935, -0.0975,  5.9468, -0.6180, -0.7066, -0.0985]]) 1\n",
      "tensor([[ 0.3244,  1.4844,  4.5402, -0.1728, -0.9494, -2.8401]]) 1\n",
      "tensor([[-3.0258, -1.7815, -1.7259, -0.7627,  3.6970,  2.0825]]) 0\n",
      "tensor([[-1.1136, -0.2611,  5.4471,  0.4469, -1.3479,  0.0842]]) 1\n",
      "tensor([[ 1.7290,  2.7482, -1.3324,  1.6988, -1.3272, -4.1206]]) 0\n",
      "tensor([[-2.0214,  5.4890, -0.5434, -2.3600, -0.6508, -0.2455]]) 1\n",
      "tensor([[ 0.4415,  7.0894, -2.0902, -1.1458,  0.7744, -5.1091]]) 0\n",
      "tensor([[-3.5379, -1.5866, -0.3097,  0.4630, -1.1210,  5.5102]]) 0\n",
      "tensor([[-2.7058, -0.6433, -3.6071,  0.1693,  4.9558, -0.7466]]) 1\n",
      "tensor([[ 2.4522, -0.3737, -3.5635, -1.2617,  4.7897, -6.9694]]) 1\n",
      "tensor([[-0.0122, -0.9718, -1.3072,  0.1004,  1.7583, -2.1890]]) 0\n",
      "tensor([[-0.3337, -0.0472,  7.9652, -0.2064, -2.4160, -1.5615]]) 1\n",
      "tensor([[ 4.1119, -0.0187, -1.3422,  0.9350, -1.2330, -4.7139]]) 0\n",
      "tensor([[-2.4072, -0.9251, -1.6762, -0.3976,  1.2627,  2.3906]]) 0\n",
      "tensor([[-2.8398, -0.0626, -4.0372, -1.3859,  4.8571, -0.0875]]) 1\n",
      "tensor([[-2.9731,  0.6205,  1.9443, -1.5199, -1.5222,  4.0459]]) 1\n",
      "tensor([[-0.8089, -2.7836,  3.5784,  4.0725, -1.3051, -0.2255]]) 0\n",
      "tensor([[-0.5705,  8.4667, -1.7460, -2.0817, -0.0748, -3.4018]]) 1\n",
      "tensor([[ 4.2647, -2.9867, -1.4250,  6.5502, -3.1800, -2.3861]]) 0\n",
      "tensor([[-0.7581,  0.2450, -5.2770, -0.2860,  5.2047, -2.2858]]) 1\n",
      "tensor([[-1.8388, -2.0199, -1.5345,  8.2111, -0.6014, -0.4083]]) 1\n",
      "tensor([[-2.7548, -0.0369,  5.8102, -0.6148,  1.0511, -0.4724]]) 1\n",
      "tensor([[-0.5365, -0.3037,  4.4282, -0.0980, -0.8265, -0.7666]]) 0\n",
      "tensor([[-0.9190, -2.0154,  0.5184, -1.6599,  5.4955, -2.7719]]) 1\n",
      "tensor([[-0.7338,  1.6362,  1.7941,  2.3281, -1.7883, -1.2214]]) 0\n",
      "tensor([[-1.4505, -3.0072,  0.0803,  7.9471, -0.8598, -1.1550]]) 0\n",
      "tensor([[-1.4314, -2.1054,  5.3418,  2.5869,  1.1791, -0.9097]]) 0\n",
      "tensor([[-1.5471, -1.8319, -0.6423, -1.0375,  5.4628, -1.7566]]) 1\n",
      "tensor([[ 0.3224,  5.6404, -1.0535, -0.3377, -1.2607, -3.6660]]) 1\n",
      "tensor([[-1.4601, -1.3465, -5.4381,  0.0358,  9.0213, -5.1576]]) 0\n",
      "tensor([[-3.1455, -0.4920, -1.9998, -0.5963,  4.1836,  0.4428]]) 0\n",
      "tensor([[-1.4235,  2.6919,  2.9837, -0.8469, -1.9475, -0.1325]]) 0\n",
      "tensor([[ 0.0112, -3.1761, -5.2233,  0.3505,  5.9276, -2.6173]]) 1\n",
      "tensor([[-0.1180,  8.2403,  1.0736, -4.3162, -1.1833, -3.5708]]) 0\n",
      "tensor([[ 0.8343,  7.2233, -0.5180, -3.0312, -1.1351, -3.8191]]) 0\n",
      "tensor([[-1.0836, -0.2678,  5.7736, -0.9895, -0.8474, -0.1662]]) 0\n",
      "tensor([[-0.1447, -1.4183,  0.8883,  5.4574, -2.7986,  0.1201]]) 0\n",
      "tensor([[-1.1813, -4.3527, -0.9965,  8.2676,  0.7010, -0.5558]]) 0\n",
      "tensor([[ 0.2746,  6.9322, -1.4201, -0.0260, -1.6279, -3.6133]]) 1\n",
      "tensor([[-1.5014, -2.4300,  0.6905,  3.2991, -0.6136,  1.1978]]) 1\n",
      "tensor([[-0.1097, -0.2088, -2.1893,  2.6073,  0.8500, -2.1168]]) 0\n",
      "tensor([[-4.6604, -2.0976, -1.3452, -0.6373, -0.6578,  9.6866]]) 1\n",
      "tensor([[ 0.3242,  8.7894, -0.8021, -3.3189, -0.9706, -4.4481]]) 0\n",
      "tensor([[-0.7972, -0.7447, -3.7940, -0.5259,  6.0002, -2.7995]]) 0\n",
      "tensor([[-0.0950,  5.4933,  0.8017, -4.1150, -1.5342, -1.6320]]) 1\n",
      "tensor([[-1.8465, -2.3404,  5.1597,  1.3804, -1.2582,  1.1288]]) 1\n",
      "tensor([[-1.5352, -1.5970,  0.0806,  6.5373, -0.8203, -0.4626]]) 0\n",
      "tensor([[-3.2177, -1.7393,  4.1156,  3.3690, -1.3748,  1.7661]]) 0\n",
      "tensor([[-3.2600,  0.2254, -2.2463, -0.3089,  3.4657,  0.5538]]) 0\n",
      "tensor([[-2.0280, -0.9624, -0.5575, -1.3927, -0.6741,  4.5783]]) 0\n",
      "tensor([[ 0.1024,  4.4108, -1.4636, -1.8197, -0.5044, -3.3304]]) 1\n",
      "tensor([[-1.5985, -0.2680, -1.3724,  0.4361,  3.2945, -1.2828]]) 1\n",
      "tensor([[-1.5031, -1.5102,  9.5153,  2.1584, -2.8182, -0.3338]]) 0\n",
      "tensor([[-0.4393, -0.5491,  3.8665,  0.6414, -0.5089, -0.6318]]) 1\n",
      "tensor([[-1.1089, -2.9275,  2.2850,  6.9719, -2.5889, -0.0746]]) 0\n",
      "tensor([[ 1.2169,  1.1779,  3.9454,  1.2200, -1.7866, -4.0822]]) 0\n",
      "tensor([[-0.7811, -2.1394,  0.0301,  6.0153, -1.2098, -0.8929]]) 1\n",
      "tensor([[-1.4058,  1.0999,  1.6434,  1.0300, -2.4394,  0.4623]]) 0\n",
      "tensor([[-0.9203, -0.4298,  8.3703, -0.3581, -2.0919, -0.0132]]) 1\n",
      "tensor([[-0.0599,  1.7629,  1.2285, -1.6453,  0.8257, -2.0864]]) 0\n",
      "tensor([[-0.5561, -1.1162, -3.0847,  0.8152,  5.3760, -3.4051]]) 0\n",
      "tensor([[ 3.1710, -0.9771, -1.6910, -0.5882, -1.3759, -1.4620]]) 0\n",
      "tensor([[ 0.9478,  0.8653, -3.5707, -0.5352,  6.0713, -6.5656]]) 0\n",
      "tensor([[ 0.3126,  8.5042, -2.1261, -2.1297, -1.0237, -3.5115]]) 1\n",
      "tensor([[-2.0765,  0.7449, -3.0789, -1.4952,  6.8575, -3.0265]]) 1\n",
      "tensor([[-0.3190,  7.7831, -0.1178, -3.0766, -1.6984, -2.9676]]) 1\n",
      "tensor([[-2.6903, -0.9439,  0.0979,  0.1244, -1.3340,  5.4048]]) 1\n",
      "tensor([[-0.9093, -2.1042, -2.0641,  5.8231,  0.9550, -2.6661]]) 1\n",
      "tensor([[-0.7648, -1.8687, -1.8610,  5.2489, -0.8105, -0.0802]]) 0\n",
      "tensor([[ 0.7360,  6.4137, -2.5584, -0.8853, -1.6574, -3.1733]]) 0\n",
      "tensor([[ 0.1752,  3.5026,  6.1915, -1.2022, -2.9752, -2.1110]]) 0\n",
      "tensor([[ 0.4119,  3.8483,  0.1691, -0.2855, -1.0798, -2.8933]]) 0\n",
      "tensor([[-0.8795, -2.3878,  0.9364,  6.5585, -0.6613, -2.1329]]) 1\n",
      "tensor([[-1.6180, -2.4801, -1.2217,  8.4329, -1.0696, -0.6309]]) 1\n",
      "tensor([[-0.3303, -2.8594, -2.5013,  7.8138, -1.0177, -1.0243]]) 0\n",
      "tensor([[ 0.3185,  5.4197, -1.9275, -1.3792,  0.4503, -4.4271]]) 0\n",
      "tensor([[ 2.5586,  1.0203, -1.9370, -2.3765, -0.3565, -2.4071]]) 1\n",
      "tensor([[-2.5989, -2.7734, -1.3633,  1.2098,  5.0394,  0.0577]]) 0\n",
      "tensor([[-0.0186,  4.8537,  2.5614, -0.7710, -2.9031, -0.5540]]) 0\n",
      "tensor([[-1.4114, -3.3256,  1.1428,  6.9192, -0.6678,  0.0453]]) 1\n",
      "tensor([[-3.0884, -2.2447,  3.9006,  0.7781, -0.8666,  3.6861]]) 1\n",
      "tensor([[-1.0669, -1.2627,  5.7513,  1.5589, -0.7442, -0.9775]]) 0\n",
      "tensor([[-4.6928, -1.2765, -2.7503, -0.3779,  0.8054,  6.3225]]) 1\n",
      "tensor([[-5.4403, -2.8823, -2.2227, -0.1723, -0.2024,  9.9351]]) 0\n",
      "tensor([[ 0.2897,  3.2672,  0.8876, -0.3404, -1.1297, -2.2044]]) 1\n",
      "tensor([[-2.3999,  1.4984,  6.8795,  0.5747, -1.8784,  0.5574]]) 0\n",
      "tensor([[-2.8100, -0.4421,  3.1604,  0.6543,  1.9815, -1.8953]]) 0\n",
      "tensor([[-0.1163, -1.1001, -0.9095,  3.7111, -1.1685,  0.4327]]) 1\n",
      "tensor([[-3.2061, -1.0276, -1.6996, -1.6627, -0.3036,  5.7865]]) 1\n",
      "tensor([[-0.8074,  3.8975, -0.8661,  3.4611, -0.8770, -4.0737]]) 1\n",
      "tensor([[ 2.0060,  0.6646, -2.5963, -2.1252,  0.6643, -2.9129]]) 0\n",
      "tensor([[-1.6705,  0.8906, -4.7933, -0.2819,  7.0670, -5.1355]]) 1\n",
      "tensor([[-1.3832, -0.7980,  3.1200,  6.4383, -1.7495, -1.6179]]) 0\n",
      "tensor([[-2.7747, -1.6464,  3.1340, -1.7327, -0.8640,  4.2794]]) 1\n",
      "tensor([[ 3.1749, -0.5345, -1.5561,  0.9096, -0.5543, -3.1513]]) 1\n",
      "tensor([[ 4.0888, -2.7292, -3.8364,  4.4804, -0.4691, -3.6757]]) 1\n",
      "tensor([[ 1.2128,  9.8150, -2.4111, -1.0426, -1.6175, -5.0325]]) 1\n",
      "tensor([[ 0.3793,  5.4969,  0.4233, -1.1300, -1.0477, -4.3372]]) 1\n",
      "tensor([[ 2.8563,  2.8687, -3.0730,  2.1107, -1.5890, -4.6275]]) 1\n",
      "tensor([[ 0.0418,  2.9537, -0.8845, -0.4455,  0.5898, -2.3959]]) 0\n",
      "tensor([[ 3.0850,  2.6656, -0.7927, -0.3047, -2.1471, -3.6619]]) 1\n",
      "tensor([[-2.3561, -1.8138, -1.8087,  0.4413, -1.1296,  5.9548]]) 1\n",
      "tensor([[-1.5585, -1.4605,  6.1520,  1.1909, -1.0450, -0.5676]]) 1\n",
      "tensor([[-2.7953, -1.4221, -1.8934,  0.2901,  4.7378, -0.6220]]) 1\n",
      "tensor([[-1.7168,  1.0190, -4.5129, -1.4444,  7.0088, -4.1485]]) 1\n",
      "tensor([[-1.1395,  1.8565, -0.4405, -0.7179, -0.9442,  1.2158]]) 0\n",
      "tensor([[-1.3931, -1.4043, -1.2510, -0.6618,  0.0655,  3.4173]]) 1\n",
      "tensor([[-1.8973,  0.1040,  5.2077, -1.0689, -1.4491,  2.3752]]) 0\n",
      "tensor([[ 0.3291,  7.7344, -2.7813, -3.1964,  0.3556, -4.0279]]) 0\n",
      "tensor([[-2.7347, -1.2944, -1.2542,  5.9635, -0.0860,  0.3487]]) 1\n",
      "tensor([[-1.0238, -2.2452,  0.0465,  5.3468,  0.5662, -1.7221]]) 0\n",
      "tensor([[ 0.9371,  3.5780,  0.5812,  0.0938, -1.8127, -3.1875]]) 0\n",
      "tensor([[-1.8099, -0.9765,  2.7495,  3.4399, -1.5721,  0.8365]]) 1\n",
      "tensor([[-1.0201, -1.7543, -2.3244,  0.1368,  5.3225, -2.9647]]) 1\n",
      "tensor([[-2.5160, -1.8070, -1.6543,  0.3730,  5.0678, -0.2118]]) 1\n",
      "tensor([[-1.8080, -3.2634,  3.4333,  6.5305, -2.6570,  0.4800]]) 0\n",
      "tensor([[-1.7304,  0.5576,  4.2308, -0.6708, -1.3501,  1.4779]]) 0\n",
      "tensor([[ 0.2764, -0.3551,  3.6071, -1.2889, -0.7143, -0.1980]]) 0\n",
      "tensor([[-1.8673, -2.2116,  1.9880,  7.4746, -1.5366, -0.7838]]) 0\n",
      "tensor([[-2.8525, -1.1592,  0.3074, -2.2123,  0.9039,  4.5696]]) 1\n",
      "tensor([[-2.5013, -1.0923, -2.0854,  0.3772,  3.2866,  0.8331]]) 0\n",
      "tensor([[-0.2400,  0.0573, -2.8570, -2.1107,  6.0353, -4.4817]]) 1\n",
      "tensor([[-1.9758, -1.9965, -0.8492, -0.2243, -1.4046,  6.0471]]) 1\n",
      "tensor([[-0.3375, -2.3098, -2.4957,  5.7752,  0.6393, -2.0710]]) 1\n",
      "tensor([[-2.2443, -2.0697, -1.5612,  6.0721, -0.3999,  0.8637]]) 0\n",
      "tensor([[ 0.4086,  4.6951,  3.9042, -1.6758, -2.2537, -2.3535]]) 1\n",
      "tensor([[ 2.7782, -1.7186, -2.7284, -1.5618, -1.2888,  0.0435]]) 1\n",
      "tensor([[-2.0236, -3.6085, -0.3510,  6.5656,  0.8712, -0.3875]]) 0\n",
      "tensor([[ 0.0115,  8.2827, -1.3707, -2.9391, -1.1859, -3.4414]]) 1\n",
      "tensor([[-0.7774, -0.0634,  5.8050, -0.3678, -1.6363,  0.5555]]) 0\n",
      "tensor([[-2.1056, -1.6183, -1.2647, -1.3913,  4.8157, -0.4179]]) 0\n",
      "tensor([[ 0.0886,  7.1171, -2.8673, -3.1016,  1.5658, -4.0940]]) 0\n",
      "tensor([[-1.0129,  4.3539, -0.9916,  3.0716, -1.1139, -2.6306]]) 1\n",
      "tensor([[-3.3521, -0.1495, -0.6085, -0.4587,  7.8758, -3.6352]]) 0\n",
      "tensor([[-0.9460, -2.1420, -2.5805, -0.3182,  2.7719,  0.9806]]) 1\n",
      "tensor([[-3.4338, -1.8977, -2.2492,  1.7098, -0.4285,  6.2327]]) 1\n",
      "tensor([[ 3.2801, -0.4016, -1.8173, -1.0419, -0.2514, -3.0736]]) 1\n",
      "tensor([[ 4.5314, -0.6357, -2.1423, -1.9180, -1.4381, -3.5740]]) 1\n",
      "tensor([[ 0.4248,  4.8664,  2.5443, -1.9286, -3.3638, -1.6492]]) 0\n",
      "tensor([[-0.7477,  0.6141, -0.5569, -1.2445,  0.3973,  0.4662]]) 0\n",
      "tensor([[-1.0235,  0.9965, -6.2128, -2.1939,  7.3698, -4.7610]]) 1\n",
      "tensor([[-0.4646, -0.5062, -2.2453, -1.2124,  4.7508, -1.1945]]) 1\n",
      "tensor([[-0.8708,  6.4119, -0.1294,  0.2272, -2.1464, -2.2370]]) 1\n",
      "tensor([[-0.5649, -2.7197, -5.5168, -0.1096,  7.4537, -5.7807]]) 1\n",
      "tensor([[-3.5346, -0.1189, -2.3504,  0.0338, -0.3405,  4.5996]]) 0\n",
      "tensor([[-2.7086, -2.2276, -0.9585,  2.0475, -1.8548,  5.3754]]) 0\n",
      "tensor([[-0.9947, -1.3097,  0.5528,  5.6812, -0.5804, -1.3551]]) 1\n",
      "tensor([[-1.8552, -0.1143,  5.2334,  0.5981, -1.5612,  1.2361]]) 1\n",
      "tensor([[-1.8300,  1.2998,  5.4468,  1.9516, -1.6864, -0.7872]]) 1\n",
      "tensor([[ 0.7119, -3.0818, -3.8425,  6.6539, -0.6001, -1.9281]]) 1\n",
      "tensor([[ 0.4620, -0.4878, -1.1757,  4.9108, -1.6985, -0.8724]]) 0\n",
      "tensor([[-1.6200, -0.8878, -1.6157,  5.7646, -1.1357,  0.7054]]) 1\n",
      "tensor([[-0.9529,  4.5973,  0.7546, -2.5552, -1.4899, -0.9803]]) 0\n",
      "tensor([[-0.0522, -1.3463,  3.2090,  1.6870, -0.0778, -1.3705]]) 0\n",
      "tensor([[-1.6705,  0.0091, -3.5000, -1.2888,  4.7822, -0.7277]]) 0\n",
      "tensor([[-1.1404, -0.5037,  0.7289,  3.4653, -0.4894, -1.3689]]) 0\n",
      "tensor([[-1.3811,  1.0367,  0.7527, -0.3136, -1.1632,  1.3483]]) 0\n",
      "tensor([[-0.5519, -0.5085, -4.0492, -0.8055,  5.7394, -4.0070]]) 1\n",
      "tensor([[ 0.1459, -0.2657, -2.9979,  0.0346,  4.0388, -3.5025]]) 1\n",
      "tensor([[ 2.3763, -0.9952, -3.7074,  0.9975,  2.8029, -7.1517]]) 1\n",
      "tensor([[-1.1904, -1.9653,  1.7985,  7.2763, -1.0671, -1.3274]]) 0\n",
      "tensor([[-4.1710, -3.1609,  1.1036, -1.1787,  5.9144,  0.2940]]) 1\n",
      "tensor([[-2.6643, -1.8432, -1.0361,  5.7354, -1.0473,  1.8377]]) 1\n",
      "tensor([[-0.6619,  5.5627,  0.4794,  1.9780, -1.6788, -4.3033]]) 0\n",
      "tensor([[ 0.4602,  2.9760,  1.1313, -1.5432, -2.1509, -0.5441]]) 0\n",
      "tensor([[ 0.4513, -0.5247,  0.1033,  3.2018, -1.7497, -0.5120]]) 0\n",
      "tensor([[-1.7331,  5.5287,  1.3054, -2.0501, -1.8879,  0.0688]]) 0\n",
      "tensor([[-0.6655,  1.9505,  7.5295, -0.8172, -2.1961, -1.0399]]) 0\n",
      "tensor([[ 4.9143, -2.7494, -2.9526,  5.1572, -2.6416, -3.2668]]) 0\n",
      "tensor([[-1.8901, -0.2431,  1.7210,  0.5756, -0.4025,  1.0316]]) 1\n",
      "tensor([[-1.4876, -2.3813, -0.7673,  1.8259,  4.2610, -1.8079]]) 1\n",
      "tensor([[-0.7360, -1.9838,  2.3826,  8.4352, -2.0960, -2.7654]]) 1\n",
      "tensor([[-1.2126,  0.6880,  4.0599,  0.1688, -1.9193,  0.5966]]) 0\n",
      "tensor([[-0.2418,  1.7072, -0.4686,  0.1872,  1.5655, -2.1513]]) 0\n",
      "tensor([[-1.6849, -2.8117,  0.4208,  6.7420, -0.0229, -0.0663]]) 0\n",
      "tensor([[ 5.1321,  0.2368, -3.4861,  0.5804, -1.4244, -6.6721]]) 1\n",
      "tensor([[-2.3615, -2.6713, -0.1347,  2.3658,  1.5044,  0.5662]]) 0\n",
      "tensor([[ 0.2299,  6.3362, -3.2272, -3.9833,  1.6240, -4.0082]]) 0\n",
      "tensor([[-0.8295, -2.1185,  6.4501,  0.8412, -0.6551, -0.5469]]) 1\n",
      "tensor([[-1.5387, -1.0746, -1.1277,  4.3851, -0.1206, -0.3430]]) 1\n",
      "tensor([[-2.3420, -1.1086, -3.2252,  0.0491,  5.6848, -1.1351]]) 0\n",
      "tensor([[-1.8278, -1.4338,  8.7439,  2.3670, -3.3783,  0.4987]]) 1\n",
      "tensor([[ 3.1786, -0.7062, -2.0795, -1.5666, -1.3754, -1.4447]]) 0\n",
      "tensor([[-1.1692, -0.2804,  5.3705, -0.0461, -1.3996,  0.1820]]) 0\n",
      "tensor([[-2.2718, -0.1246, -4.6519,  0.5881,  7.1976, -3.8947]]) 1\n",
      "tensor([[-1.9566, -3.9117, -5.2498,  5.3089,  5.7918, -2.7671]]) 1\n",
      "tensor([[-2.8103,  1.0488, -0.8683, -0.6281,  4.2029, -0.6782]]) 1\n",
      "tensor([[-2.6976, -2.1372, -1.1907,  0.5721,  3.2080,  1.4301]]) 1\n",
      "tensor([[ 4.2880, -2.3284, -1.3350, -0.9432, -1.8226, -1.3126]]) 0\n",
      "tensor([[-1.9414, -0.5078, -2.3129, -0.2334,  7.8691, -4.1668]]) 1\n",
      "tensor([[ 0.4972,  3.8035, -4.0576, -4.0060,  4.8516, -4.9533]]) 0\n",
      "tensor([[ 2.8327, -0.3282,  5.2191, -1.0604, -3.8921, -2.1756]]) 1\n",
      "tensor([[-4.0161, -2.6300, -1.8326,  0.8863, -0.2178,  7.1084]]) 1\n",
      "tensor([[ 5.3304, -0.0868, -1.4942, -2.5999, -2.1993, -2.1034]]) 0\n",
      "tensor([[-0.5791,  1.0233,  0.6374,  2.6033, -0.5548, -2.0756]]) 0\n",
      "tensor([[-0.5858, -1.0606,  4.0901,  3.8770, -2.0711, -0.9738]]) 0\n",
      "tensor([[ 0.4254,  0.6549,  6.0638,  0.4706, -2.5196, -2.1090]]) 0\n",
      "tensor([[-1.0460, -0.8597, -0.9097,  5.4083,  0.1043, -1.6998]]) 0\n",
      "tensor([[-3.1630, -0.8977, -1.9012,  0.5522, -1.4928,  6.1359]]) 0\n",
      "tensor([[-3.3180, -1.1824, -1.0966, -0.4632, -1.3464,  7.1670]]) 0\n",
      "tensor([[-3.0760, -1.2340, -4.5344,  0.3463,  4.4965,  1.2119]]) 1\n",
      "tensor([[-1.6883,  1.1820,  2.0262,  0.3911, -0.7359,  0.6554]]) 0\n",
      "tensor([[ 3.7520, -0.3125, -2.6938, -2.8683, -1.2280, -1.3532]]) 1\n",
      "tensor([[-2.3852, -3.5422, -0.7545,  0.9821,  0.1031,  5.2920]]) 0\n",
      "tensor([[-2.5958, -0.1985, -0.5022, -1.6366, -1.1601,  6.1953]]) 0\n",
      "tensor([[-3.7996, -0.5427, -2.5430,  0.3730,  6.8271, -1.7615]]) 1\n",
      "tensor([[-1.0718,  6.9853, -3.2643, -3.6813,  1.7888, -2.9535]]) 1\n",
      "tensor([[-2.1322e+00, -2.0897e+00, -2.4022e+00,  7.3495e+00, -1.1471e+00,\n",
      "         -2.1913e-03]]) 1\n",
      "tensor([[ 0.0057, -1.1601,  3.1534,  1.6298, -0.9670, -1.1039]]) 0\n",
      "tensor([[ 0.7066, -3.0326,  5.2118,  2.6664, -1.3016, -1.1124]]) 0\n",
      "tensor([[-0.2432,  0.0672,  0.9004,  3.5277, -1.1201, -2.2115]]) 1\n",
      "tensor([[-1.3384,  1.8355,  4.2507, -2.0097, -0.9442, -0.8741]]) 0\n",
      "tensor([[ 1.9026,  3.6115,  0.9302, -0.4707, -2.6358, -3.6247]]) 0\n",
      "tensor([[ 0.2247, -0.9147,  5.4455,  2.0842, -3.1301, -0.4868]]) 1\n",
      "tensor([[-0.7626,  2.1687,  5.2321,  0.6939, -2.3183, -0.2833]]) 0\n",
      "tensor([[-2.4396,  0.0885, -3.7134, -0.5649,  6.6174, -2.6825]]) 0\n",
      "tensor([[-1.0288, -1.9928, -2.2509,  6.3204,  0.9899, -2.7453]]) 1\n",
      "tensor([[ 0.2953, -2.6542,  2.8679,  5.7520, -0.9018, -2.9349]]) 1\n",
      "tensor([[-2.6220, -1.5771, -1.5765,  0.0451, -0.4768,  5.9879]]) 1\n",
      "tensor([[-0.1924,  2.6999, -2.9254, -0.1558,  0.7191, -1.9830]]) 1\n",
      "tensor([[ 2.6782,  4.2241, -1.2959, -1.9964, -3.1508, -2.7725]]) 1\n",
      "tensor([[-1.5846,  0.0991,  0.7982,  2.5738, -1.0721,  0.2602]]) 0\n",
      "tensor([[ 0.5065,  0.9183,  4.4910, -1.8082, -2.4785, -0.1430]]) 1\n",
      "tensor([[-1.0750, -0.4960,  1.9235,  2.7475, -0.0845, -0.8060]]) 0\n",
      "tensor([[-0.9930,  0.2593,  6.7156, -1.2571, -1.6330,  0.2734]]) 0\n",
      "tensor([[-4.1951, -3.4523, -4.3496,  2.6504,  4.2330,  2.3607]]) 1\n",
      "tensor([[-1.6766, -4.7118, -1.5350, 10.7121, -1.1896, -0.2322]]) 1\n",
      "tensor([[-1.5662,  0.1172, -4.4666, -0.0543,  9.7949, -6.1430]]) 1\n",
      "tensor([[-2.5155,  1.3234,  6.2946, -0.9045, -1.1527,  0.2025]]) 1\n",
      "tensor([[-1.2800,  1.3821,  4.2475, -1.1651, -2.0075, -0.0195]]) 0\n",
      "tensor([[-0.8327,  0.1296,  0.9887,  2.1061,  1.0811, -1.9427]]) 0\n",
      "tensor([[-1.3415, -1.5652,  3.2182,  4.0146, -1.4885, -0.1850]]) 0\n",
      "tensor([[-0.9656,  2.6039, -0.6445,  6.6384, -1.7859, -4.3951]]) 0\n",
      "tensor([[-1.9044, -2.2484, -1.2879,  2.8207,  1.5312,  0.7082]]) 0\n",
      "tensor([[-1.1960,  0.2369,  3.4748,  0.1900, -1.4266,  1.2011]]) 0\n",
      "tensor([[-1.9537, -0.1748, -5.6752, -0.9734,  8.3749, -4.2118]]) 0\n",
      "tensor([[-1.3475,  0.5582, -1.5797, -1.7241,  4.2210, -3.4623]]) 0\n",
      "tensor([[ 0.7462,  6.1324, -1.5985, -0.2403, -1.4571, -3.2051]]) 0\n",
      "tensor([[ 1.2049,  5.8311, -1.3589, -3.2979,  1.5080, -5.0779]]) 1\n",
      "tensor([[-0.2646,  1.3764,  4.9806,  0.8363, -1.4594, -2.2648]]) 1\n",
      "tensor([[-1.3451, -3.7513, -2.2705,  3.9002,  4.1417, -0.7601]]) 1\n",
      "tensor([[-1.6529, -2.0115, -2.3558, -1.4175,  4.9942, -1.0916]]) 0\n",
      "tensor([[-3.1721, -1.4177, -0.8912, -1.5822,  0.8081,  4.5760]]) 1\n",
      "tensor([[-0.6985, -0.4600, -3.6105, -0.2911,  5.3080, -2.7461]]) 1\n",
      "tensor([[ 2.3985, -2.6172,  0.9781, -0.2821, -0.1632, -0.8977]]) 1\n",
      "tensor([[-2.6855,  0.3692, -1.9807, -3.1597,  3.9426,  0.3690]]) 0\n",
      "tensor([[-3.0340, -0.1721, -1.1479,  5.9720, -0.0955, -0.0578]]) 1\n",
      "tensor([[-0.8485, -1.3430, -4.1142, -0.0473,  7.2801, -4.4408]]) 1\n",
      "tensor([[-1.5206,  0.4524, -0.1572, -0.3940, -2.1006,  4.1470]]) 0\n",
      "tensor([[-1.0977,  0.4718, -1.8895, -0.8089,  4.0145, -1.9837]]) 1\n",
      "tensor([[ 0.1511,  1.9019,  5.2315, -0.3191, -2.7267, -1.9857]]) 1\n",
      "tensor([[ 0.4142,  6.9521, -2.0587, -0.6447, -0.9809, -2.9759]]) 1\n",
      "tensor([[-1.6167, -1.1923, -2.8694, -0.9624,  7.8222, -3.5144]]) 1\n",
      "tensor([[-3.7369, -1.6178,  0.4538,  0.1866, -0.3178,  5.8673]]) 1\n",
      "tensor([[-1.4254, -1.5844,  3.8756,  0.6961, -1.3185,  1.9619]]) 1\n",
      "tensor([[ 1.7468, -0.2578, -2.9647,  6.4713, -2.0541, -2.0274]]) 1\n",
      "tensor([[ 3.6047,  1.6702, -2.4458, -3.5062, -2.4015, -1.2769]]) 1\n",
      "tensor([[-0.0526,  1.4730,  6.0167, -1.2133, -1.7848, -1.4239]]) 0\n",
      "tensor([[-2.3872,  0.5704, -2.2085,  1.4078,  3.0603, -0.9628]]) 1\n",
      "tensor([[ 0.6538, -2.2268, -4.1804,  5.8434, -1.0325, -0.4535]]) 1\n",
      "tensor([[ 0.1723,  0.7021,  6.3494, -0.3547, -2.8050, -1.2562]]) 1\n",
      "tensor([[-0.9851, -0.7831, -0.6693, -0.2224, -0.7456,  3.0594]]) 1\n",
      "tensor([[-0.3333, -0.8457,  2.2661,  4.9581, -1.7599, -1.5070]]) 0\n",
      "tensor([[-0.9370,  6.7386, -0.6879, -0.7256,  0.1896, -2.3889]]) 1\n",
      "tensor([[ 0.3495,  1.1725, -2.5752, -0.4213, -1.1324,  0.8365]]) 0\n",
      "tensor([[-1.5477, -1.9479, -3.2329, -0.0338,  6.2961, -3.0804]]) 1\n",
      "tensor([[-3.5913, -0.5495, -0.0526, -0.3962, -0.6909,  6.0460]]) 0\n",
      "tensor([[-0.6851, -1.8226, -3.0132,  1.0633,  2.8937, -1.7256]]) 1\n",
      "tensor([[ 1.1352,  1.7884, -2.0973,  3.8330, -1.8288, -3.0959]]) 1\n",
      "tensor([[-1.3917,  0.8918, -1.6325, -0.8160,  1.4436, -1.1326]]) 0\n",
      "tensor([[-0.2333, -2.0666,  0.3805,  7.7326, -2.3516, -1.1840]]) 1\n",
      "tensor([[-1.2910,  4.8059,  4.5478,  2.4086, -2.6643, -3.0726]]) 0\n",
      "tensor([[-2.1758, -0.8020, -1.6296, -0.7468, -0.0915,  4.4282]]) 0\n",
      "tensor([[-0.6851,  1.4694,  5.4427,  0.1242, -2.1781, -0.4282]]) 0\n",
      "tensor([[-5.7563e-01, -3.5697e-01, -1.8168e+00,  2.3370e-03,  3.6156e+00,\n",
      "         -1.3875e+00]]) 1\n",
      "tensor([[-3.0500, -2.2300, -2.0832,  1.8165,  0.7354,  2.9567]]) 1\n",
      "tensor([[-0.3658, -1.3647, -0.7186,  4.0870, -0.9796, -0.3152]]) 1\n",
      "tensor([[-2.7415, -1.0663, -1.4678,  6.8877,  1.1226, -1.2646]]) 0\n",
      "tensor([[-1.3952, -0.9905,  0.1952,  3.8507, -0.3196, -1.2606]]) 1\n",
      "tensor([[-3.3045,  0.1705, -1.8730, -1.8004,  4.6973,  0.3112]]) 0\n",
      "tensor([[-4.8617, -4.1511, -2.5703,  1.1079, -1.0905, 10.4206]]) 0\n",
      "tensor([[-1.3705, -0.2450,  2.3000,  0.7066, -1.3304,  1.5256]]) 0\n",
      "tensor([[-0.8245,  2.0582,  6.2815, -2.6403,  0.9002, -2.5940]]) 1\n",
      "tensor([[-1.4140, -1.5262, -0.1172,  6.8494, -1.2913, -0.3244]]) 1\n",
      "tensor([[-1.3566, -2.9184, -0.8602,  7.5072, -0.6863, -0.2965]]) 0\n",
      "tensor([[ 0.9264,  6.2105, -2.5275, -0.9928, -1.0324, -3.6163]]) 1\n",
      "tensor([[ 2.3726, -0.6403,  0.9670,  0.9816, -1.1739, -3.1390]]) 0\n",
      "tensor([[-0.7468,  0.8392,  4.2017, -0.2883, -0.8710, -0.1837]]) 0\n",
      "tensor([[ 1.0146,  1.2910,  6.6179, -0.3323, -2.5541, -2.6736]]) 0\n",
      "tensor([[-3.5343, -0.3422, -5.1404, -0.3336,  8.1780, -3.5055]]) 1\n",
      "tensor([[ 0.4325,  2.6134,  9.1387, -1.3630, -4.1446, -1.9978]]) 0\n",
      "tensor([[-1.4849, -0.0904,  2.6610,  0.2983, -0.9721,  0.9147]]) 0\n",
      "tensor([[-1.0787, -0.6694, -2.4966, -0.1975,  4.7712, -1.9017]]) 1\n",
      "tensor([[-1.0485,  1.9627,  2.7817,  3.4997, -1.2126, -2.3351]]) 1\n",
      "tensor([[-3.4030, -1.9300, -2.4995, -1.7495,  1.4834,  5.9343]]) 0\n",
      "tensor([[ 2.7711,  0.9623, -0.9677, -0.1315, -1.8320, -1.8642]]) 1\n",
      "tensor([[ 0.2497,  6.7474,  0.5472, -2.3116, -1.9184, -3.9805]]) 0\n",
      "tensor([[-0.0874,  4.5839,  0.5619, -0.0305, -1.5974, -3.2595]]) 0\n",
      "tensor([[-1.5391,  0.1153,  4.2620, -1.4434, -1.0052,  1.1610]]) 1\n",
      "tensor([[-1.7011, -1.5436, -4.0853,  0.3137,  4.1216,  0.0711]]) 0\n",
      "tensor([[ 0.0217,  7.5610, -0.5514, -1.5127, -0.7201, -3.6107]]) 0\n",
      "tensor([[ 0.2276,  7.3289, -2.1652, -1.7403, -0.8711, -3.4759]]) 0\n",
      "tensor([[ 0.6191,  4.4846, -0.3568,  0.4315, -1.1017, -2.6375]]) 0\n",
      "tensor([[-2.0596,  1.4076,  7.7032,  0.6898, -2.1729, -0.3604]]) 1\n",
      "tensor([[-2.7408, -3.1283, -1.5156,  7.8511,  0.2259,  0.6451]]) 0\n",
      "tensor([[ 0.6953,  6.2201, -2.6890,  0.3103, -0.0372, -4.2232]]) 1\n",
      "tensor([[-2.8276,  0.5686, -2.9535, -0.2196,  7.3364, -2.6510]]) 0\n",
      "tensor([[ 0.6326,  8.7046, -1.7577, -3.3909, -0.1087, -4.3571]]) 0\n",
      "tensor([[-2.2231,  0.1679, -4.1178,  0.5767,  4.2972, -0.5587]]) 0\n",
      "tensor([[-1.2376, -1.3492, -1.4631, -0.0140,  4.0490, -1.2211]]) 1\n",
      "tensor([[-1.6621,  6.8694,  2.6003, -2.4491, -1.0102, -2.6880]]) 1\n",
      "tensor([[-1.5532,  4.3752, -2.6099, -0.5056,  0.5019, -2.1554]]) 1\n",
      "tensor([[-0.3790,  1.7432, -0.7372, -0.7103,  0.7978, -0.5565]]) 0\n",
      "tensor([[ 3.3095, -0.8644, -0.4402,  0.6806, -2.5982, -2.1066]]) 0\n",
      "tensor([[ 6.6965, -2.6767, -3.9654,  5.0582, -3.0097, -4.9358]]) 0\n",
      "tensor([[ 1.7321, -0.0760,  2.7140, -0.4310, -2.4001, -1.0979]]) 1\n",
      "tensor([[ 4.9674,  1.5892, -2.5569, -0.0543, -2.4185, -4.7707]]) 1\n",
      "tensor([[-0.9421, -2.2738, -0.6332,  4.9160, -0.6603, -0.1416]]) 1\n",
      "tensor([[ 0.1396,  7.7494, -1.3787, -0.2208, -1.7863, -4.6152]]) 1\n",
      "tensor([[-0.7600, -0.3272,  6.7404,  1.7224, -2.2028, -0.7507]]) 1\n",
      "tensor([[-2.2393, -1.4465,  1.7094, -1.1691,  2.0595,  1.0228]]) 0\n",
      "tensor([[-0.8556,  1.4620,  5.9149,  1.6883, -3.7718, -0.3074]]) 1\n",
      "tensor([[-2.7984, -2.3220, -0.0055,  1.5984,  0.1586,  3.8061]]) 1\n",
      "tensor([[ 0.0837,  4.0433,  0.7319, -0.4803, -0.5037, -2.2114]]) 1\n",
      "tensor([[ 0.3873,  2.5792,  5.6583, -2.1954, -1.4057, -2.1793]]) 0\n",
      "tensor([[ 0.3010,  6.2481, -1.1460, -2.3022, -0.5673, -3.2221]]) 1\n",
      "tensor([[-0.8340, -2.7382, -2.0893,  6.8183,  0.1867, -1.7906]]) 1\n",
      "tensor([[-2.2451, -1.7885,  0.8885,  7.1365, -1.5131, -0.3505]]) 1\n",
      "tensor([[-1.5978,  4.0013, -0.7933, -1.5991,  0.8767, -1.1069]]) 1\n",
      "tensor([[-2.3125, -0.7091,  1.7945,  0.6611, -0.5654,  2.0099]]) 0\n",
      "tensor([[-2.4921, -1.5556, -3.7960,  0.3071,  7.9223, -3.2306]]) 1\n",
      "tensor([[-1.8968, -1.8575,  2.0658,  6.9525, -1.6621, -0.3138]]) 1\n",
      "tensor([[-0.5290, -2.1471, -0.6490,  7.4546, -0.4559, -2.2179]]) 0\n",
      "tensor([[-1.7428,  1.2391,  8.2237,  0.2841, -3.2035, -0.5501]]) 0\n",
      "tensor([[ 0.1897,  5.5992, -1.4808, -2.9520,  1.3641, -4.2619]]) 1\n",
      "tensor([[ 4.2290,  4.4046, -1.3395, -1.9402, -3.2822, -3.9816]]) 1\n",
      "tensor([[ 1.9838,  3.6131, -1.2216, -2.1939, -0.3826, -3.1843]]) 0\n",
      "tensor([[-0.8694, -0.2342,  4.9052,  1.5941, -1.9071, -0.7943]]) 1\n",
      "tensor([[-1.7817, -0.9305,  5.4870,  1.3505, -1.7840,  1.0482]]) 0\n",
      "tensor([[-2.2874, -0.3583,  5.8073,  1.6768, -0.9306, -0.3590]]) 1\n",
      "tensor([[-1.5055, -0.5370, -0.2664,  0.2258,  1.5517,  0.0544]]) 0\n",
      "tensor([[-0.5136, -1.1077,  1.3121,  4.9920, -1.6883, -0.2885]]) 0\n",
      "tensor([[-3.0610, -1.3647, -2.2093, -1.2190, -1.2696,  7.3626]]) 0\n",
      "tensor([[-1.6991, -3.1538,  1.1984,  4.8769, -0.0432,  0.2106]]) 1\n",
      "tensor([[-3.1930, -1.1984, -3.1366, -0.7162, -0.5853,  6.6691]]) 1\n",
      "tensor([[-1.2433,  2.9072,  4.4470, -0.2026, -2.0387, -1.1539]]) 0\n",
      "tensor([[-0.6753, -1.1672,  0.7460,  3.1033, -1.2953,  0.1311]]) 1\n",
      "tensor([[-1.6081,  2.2594, -1.4150, -1.9059, -1.5804,  2.4853]]) 1\n",
      "tensor([[-1.5442,  1.3885, -2.1555,  1.2905, -0.3599,  0.5039]]) 0\n",
      "tensor([[-0.1530, -2.6420, -2.3259,  4.8285,  0.4800, -0.9889]]) 1\n",
      "tensor([[-2.4077, -2.0341, -2.2382,  0.1309,  3.0990,  1.7959]]) 1\n",
      "tensor([[ 0.7507, -0.7885,  2.3293,  0.6911, -0.7398, -0.8571]]) 0\n",
      "tensor([[ 4.1572,  0.1027, -1.3231,  0.3762, -1.3415, -4.4895]]) 0\n",
      "tensor([[ 0.0715,  3.3409, -0.9639, -0.3572, -1.8678, -0.5187]]) 1\n",
      "tensor([[-2.6572,  1.0389,  2.0260, -2.8506,  1.5631,  1.2841]]) 0\n",
      "tensor([[-0.1924, -0.2261,  2.7755,  2.6644, -1.5146, -1.7139]]) 0\n",
      "tensor([[ 0.4966,  6.8624, -1.9225, -1.9929,  0.9966, -5.9020]]) 0\n",
      "tensor([[-0.9511,  1.3053, -3.3427, -0.8182,  3.1909, -1.3284]]) 0\n",
      "tensor([[-1.3335, -1.0514,  2.8268,  4.8702, -0.2368, -2.2456]]) 0\n",
      "tensor([[-0.9103,  0.5680, -0.6579,  1.3096,  0.0047, -1.3179]]) 0\n",
      "tensor([[-0.0123, -0.5821,  8.2498,  2.7351, -3.1126, -1.6464]]) 1\n",
      "tensor([[-0.1218, -1.4442, -1.5919,  2.9639,  0.1142, -0.1119]]) 1\n",
      "tensor([[ 3.9377, -0.4082, -2.9217, -0.0296, -2.1794, -3.4944]]) 1\n",
      "tensor([[-0.3137,  4.0890, -1.7056, -1.8877,  3.7332, -5.9084]]) 0\n",
      "tensor([[-3.2003, -0.9703,  0.9186, -0.9366,  5.3005, -1.1753]]) 1\n",
      "tensor([[-0.4944,  0.1779, -1.1733,  1.2052, -2.4668,  2.0457]]) 0\n",
      "tensor([[-1.0276,  0.0137, -0.7285, -0.8296, -0.1146,  1.1908]]) 0\n",
      "tensor([[-2.6225,  1.6614, -2.3985,  1.5729, -0.1690,  1.7222]]) 0\n",
      "tensor([[ 4.1452, -1.8291, -4.7145, -0.5846, -2.0992, -0.6281]]) 0\n",
      "tensor([[-3.5342,  0.0901, -0.7251,  0.8830,  5.2584, -1.9184]]) 1\n",
      "tensor([[-2.1840,  0.0881, -0.0767, -0.0924,  3.6559, -1.8717]]) 0\n",
      "tensor([[-0.4007, -1.2811,  6.3422,  0.4628, -2.2377,  0.3101]]) 1\n",
      "tensor([[-1.8756, -3.4675, -0.1184,  9.8782, -1.2372, -0.6436]]) 0\n",
      "tensor([[-2.6667, -1.5619, -0.0981, -0.4285, -1.7714,  6.9370]]) 0\n",
      "tensor([[-0.9336, -3.6199, -1.3281,  7.7597, -0.6321, -0.3565]]) 1\n",
      "tensor([[-1.0347,  1.0471,  5.8820,  0.9040, -0.5677, -2.6725]]) 1\n",
      "tensor([[ 1.3561, -2.0234, -4.1368,  0.2286,  0.2441, -0.4828]]) 0\n",
      "tensor([[-1.8215, -0.7504,  6.4735,  0.8639, -0.7792, -0.7274]]) 0\n",
      "tensor([[-2.5121,  0.0193,  1.5649,  1.7412, -0.0872,  0.2271]]) 1\n",
      "tensor([[-0.0322,  0.1994, -0.7436,  3.9706, -2.2209, -0.1169]]) 1\n",
      "tensor([[-0.5646,  8.9090, -2.4803, -3.4368,  2.6677, -4.7785]]) 0\n",
      "tensor([[-0.4724,  3.8784, -0.1820, -2.1238, -0.0648, -1.4370]]) 0\n",
      "tensor([[-0.4054,  7.3544, -4.6900, -1.9836,  0.0677, -1.6915]]) 0\n",
      "tensor([[-0.2064,  3.4166,  2.0538,  0.7792, -3.8735, -0.5607]]) 0\n",
      "tensor([[-2.3303, -1.6299, -1.6833, -0.2566,  5.0230, -1.5546]]) 0\n",
      "tensor([[-0.4899, -0.3175, -1.6144,  1.4609,  1.3234, -1.1576]]) 0\n",
      "tensor([[-1.1190,  0.1079,  4.9940,  1.3328, -1.9827,  0.3597]]) 1\n",
      "tensor([[ 0.7495,  7.7684,  1.0452, -2.0709, -2.9837, -3.2544]]) 1\n",
      "tensor([[-3.7319, -0.5940,  2.4616, -0.3759, -2.3224,  6.5587]]) 1\n",
      "tensor([[ 0.3033, -0.5697,  7.7420, -0.0677, -2.9187, -1.0166]]) 1\n",
      "tensor([[-2.0172, -1.3805, -1.7295, -0.5444,  5.2991, -1.2611]]) 1\n",
      "tensor([[-0.9084, -4.4042, -2.8372, 10.3214,  0.1652, -1.0665]]) 1\n",
      "tensor([[-2.1917, -0.0129, -3.6070,  0.9870,  3.7840,  0.5680]]) 1\n",
      "tensor([[-1.1130,  1.1460, -3.4744, -1.7010,  6.3567, -4.2589]]) 0\n",
      "tensor([[-1.5605, -3.2974, -0.6628,  8.5980, -0.3348, -1.0225]]) 1\n",
      "tensor([[ 0.0461,  0.7151, -0.2554, -0.5765,  0.6552, -2.2709]]) 0\n",
      "tensor([[-2.7379, -0.5856, -0.6200,  2.0062,  1.7588, -0.3406]]) 0\n",
      "tensor([[-1.3655, -0.0309,  7.0485,  0.8674, -1.7741, -0.7771]]) 1\n",
      "tensor([[-1.6911, -0.5535,  5.8010, -0.2567, -1.2072,  0.6985]]) 1\n",
      "tensor([[ 0.7220,  6.4811, -2.7065, -2.5100, -0.8434, -2.3676]]) 1\n",
      "tensor([[ 3.1634,  0.7902, -1.5781, -1.9350, -0.4258, -2.2864]]) 0\n",
      "tensor([[-0.8610,  5.1351,  5.2446, -1.4726, -2.9373, -2.1153]]) 0\n",
      "tensor([[-0.3808, -0.7336,  2.2811,  5.9470, -1.5649, -2.2755]]) 0\n",
      "tensor([[-0.8789,  2.6128, -0.4336,  1.9881, -1.8937, -0.7774]]) 1\n",
      "tensor([[-2.4130,  4.3342, -1.5513,  0.8986,  0.7056, -0.3742]]) 0\n",
      "tensor([[-0.8907, -0.0221,  4.4832,  0.4288, -1.9786, -0.0835]]) 1\n",
      "tensor([[ 0.6065,  0.3559,  7.6828,  0.4687, -3.4631, -1.5901]]) 1\n",
      "tensor([[-1.2924, -1.3966,  2.4318,  5.4502, -0.7992, -1.0951]]) 1\n",
      "tensor([[-4.4168, -4.1497, -2.7491, -0.5404, -0.7356, 10.7510]]) 0\n",
      "tensor([[-0.4981, -1.5772,  1.9299,  4.1147, -2.1614, -0.1444]]) 0\n",
      "tensor([[ 1.0516, 10.4996, -2.3139, -1.2492, -1.4169, -6.5324]]) 1\n",
      "tensor([[-1.9832, -4.5066, -0.0783,  6.5106, -0.8629,  2.4580]]) 0\n",
      "tensor([[-3.1474,  0.4946, -3.1752, -0.1535,  4.5358, -0.9033]]) 1\n",
      "tensor([[-1.7194, -3.6375, -1.7635,  6.8991,  0.4040,  0.9154]]) 1\n",
      "tensor([[-1.7256, -1.6805,  0.4707,  4.3887, -1.1047,  1.2807]]) 1\n",
      "tensor([[-1.9218, -1.8251, -3.2544, -0.6153,  5.8873, -1.2893]]) 1\n",
      "tensor([[-2.6460, -2.0172, -3.1769, -1.2623,  5.7241,  0.7484]]) 1\n",
      "tensor([[-1.3689, -0.9577, -2.6991,  0.4367,  3.6847, -0.4288]]) 1\n",
      "tensor([[ 2.7853, -0.2014, -1.0915, -1.1706, -1.4990, -1.1644]]) 1\n",
      "tensor([[-4.3741, -1.2522, -1.1212, -0.8300,  0.7525,  6.4902]]) 1\n",
      "tensor([[ 0.9093,  4.5425, -3.8364, -0.0298,  1.5057, -5.3453]]) 0\n",
      "tensor([[-2.3052, -2.0416,  2.7972,  1.1536,  0.2489,  1.5449]]) 0\n",
      "tensor([[-1.5093,  7.3727,  0.7724, -0.9933, -1.2814, -2.7773]]) 1\n",
      "tensor([[-0.7284, -2.3640, -3.4805,  6.7409,  0.3766, -0.9478]]) 1\n",
      "tensor([[ 1.2193,  0.9543,  4.8049,  0.2378, -3.4570, -1.9525]]) 1\n",
      "tensor([[-3.6697, -0.3841, -1.1121, -0.3441, -0.1593,  3.9875]]) 1\n",
      "tensor([[-0.5873, -1.5241, -2.3764,  6.3343, -0.1298, -2.0965]]) 1\n",
      "tensor([[-1.0404, -2.3204, -1.2420,  5.8424, -0.0451, -0.1587]]) 1\n",
      "tensor([[-2.9417, -2.1226,  0.6678,  0.3684, -1.3143,  6.3639]]) 1\n",
      "tensor([[-1.3010,  1.6547, -0.1337,  2.4856, -2.2569,  0.3218]]) 0\n",
      "tensor([[-1.8176, -2.6074, -2.1183,  1.8005,  4.1602, -0.7279]]) 1\n",
      "tensor([[-0.4910,  1.9646, -2.7563, -0.2466,  4.1748, -3.8884]]) 0\n",
      "tensor([[-1.4298, -0.7647,  4.3289,  0.5365, -2.5372,  2.2672]]) 0\n",
      "tensor([[-1.4923, -3.1569, -2.3278,  3.4582,  4.4204, -1.5014]]) 0\n",
      "tensor([[-0.7104, -1.1390,  1.3989,  1.8720,  0.2110, -0.4475]]) 1\n",
      "tensor([[-3.3024,  2.1872, -4.9851, -0.9061,  6.0610, -1.8240]]) 1\n",
      "tensor([[-2.6719, -0.2790,  2.9955, -2.1334,  4.5853, -2.9635]]) 1\n",
      "tensor([[-1.1571,  0.7335,  3.0338,  0.6257,  0.1291, -2.1420]]) 0\n",
      "tensor([[-2.6365,  0.3006,  0.6701,  2.6115,  0.9241, -0.8687]]) 0\n",
      "tensor([[-3.4935, -1.8826,  1.6725, -1.2738, -0.7579,  5.7931]]) 1\n",
      "tensor([[-2.6676, -2.3965, -4.5279, -0.9520,  8.2371, -0.3989]]) 1\n",
      "tensor([[-1.3648, -2.3282,  1.7536,  6.5807, -1.2960, -0.4223]]) 1\n",
      "tensor([[-1.8488, -0.9521, -1.6560, -0.8648,  4.0961,  0.0044]]) 0\n",
      "tensor([[-0.3100, -2.8416, -1.0540,  7.2642, -0.8208, -1.1474]]) 1\n",
      "tensor([[-0.4359,  0.8082,  8.9805,  0.2295, -2.4414, -1.3224]]) 0\n",
      "tensor([[-2.3158,  0.1306,  7.7237,  2.1133, -2.4878, -0.1980]]) 1\n",
      "tensor([[-1.1950, -0.9382, -3.0174,  1.6422,  4.4870, -1.7442]]) 1\n",
      "tensor([[ 0.5362, -0.8457, 10.0485, -1.0349, -2.5040, -2.0793]]) 1\n",
      "tensor([[ 0.0521,  5.1576, -1.4161, -2.1142, -1.8556, -1.0435]]) 1\n",
      "tensor([[-2.1612,  2.3145, -2.4972, -2.1008,  4.6319, -1.4041]]) 1\n",
      "tensor([[-3.8812,  1.2406, -1.2692, -1.6552,  2.4652,  2.4038]]) 0\n",
      "tensor([[-1.3821, -1.2852, -2.0032,  3.3943,  0.3976,  0.5207]]) 0\n",
      "tensor([[-2.9979, -1.8028, -5.4492, -0.8982,  8.4279, -2.9382]]) 1\n",
      "tensor([[ 1.9963,  2.1263, -2.6281,  0.2031, -1.1282, -3.1237]]) 1\n",
      "tensor([[-2.0553, -2.6249,  0.8288,  2.3260, -1.3183,  2.5838]]) 1\n",
      "tensor([[ 3.4408,  4.0248,  0.7376, -3.0233, -1.0434, -4.8266]]) 0\n",
      "tensor([[ 2.9973,  1.4649, -2.4394, -2.8163, -0.5062, -2.8421]]) 0\n",
      "tensor([[-0.5305, -3.0343, -2.2676,  6.9269,  0.5243, -1.4774]]) 1\n",
      "tensor([[-1.9222,  1.6632,  2.4317,  2.2298, -0.5036, -0.9121]]) 0\n",
      "tensor([[ 0.3151,  5.5497, -3.8163, -3.6547,  2.1559, -4.7286]]) 1\n",
      "tensor([[-1.2822,  4.1973, -1.6185,  1.7726, -0.5174, -2.7027]]) 0\n",
      "tensor([[-2.7515, -0.9077,  0.7208, -0.4081,  1.1739,  2.4632]]) 0\n",
      "tensor([[ 0.0622,  0.3429, -1.3948, -1.5962, -1.2660,  0.8486]]) 0\n",
      "tensor([[ 0.6276, -1.2963,  0.7930,  0.4483, -0.6774, -0.7378]]) 0\n",
      "tensor([[ 0.2237,  0.2438,  0.5821,  1.7834,  0.2413, -3.1399]]) 0\n",
      "tensor([[-1.4400,  1.5075,  6.1197, -0.7705, -2.2205,  0.6245]]) 0\n",
      "tensor([[-0.4710,  0.0893,  1.1734,  4.4182, -0.8060, -2.3422]]) 1\n",
      "tensor([[ 0.3155,  2.0912,  1.4897,  4.4887, -3.0810, -3.5625]]) 0\n",
      "tensor([[-4.8508,  1.4045, -1.1838, -1.1650,  6.2550, -0.9626]]) 0\n",
      "tensor([[-4.2049, -2.3198, -2.4377,  2.0012,  1.9031,  4.4762]]) 0\n",
      "tensor([[ 0.2406, -0.1867,  2.8033, -0.8190, -0.1898, -1.2988]]) 1\n",
      "tensor([[ 0.0810, -3.0514, -3.2817, -0.5645,  4.6048, -1.6230]]) 0\n",
      "tensor([[-3.0084, -2.2559, -1.6729,  0.6099,  2.9971,  1.2218]]) 1\n",
      "tensor([[-0.8945, -0.4961, -0.9832,  4.1856, -0.6336, -0.8040]]) 1\n",
      "tensor([[ 1.2111,  5.2838, -0.5746, -0.3985, -0.4945, -4.5289]]) 1\n",
      "tensor([[-0.9441,  6.3644,  0.7821, -2.9970, -0.1663, -2.1625]]) 0\n",
      "tensor([[ 0.0140, -0.4162,  4.8109,  0.9767, -2.2610, -1.1135]]) 1\n",
      "tensor([[ 0.1859,  0.5156,  6.8083,  0.8965, -1.9766, -3.3759]]) 0\n",
      "tensor([[-0.8088, -0.5153, -0.8796, -1.0047,  4.8530, -2.3271]]) 0\n",
      "tensor([[-2.3233,  0.4204, -0.9882, -2.2708,  2.7632,  0.8960]]) 1\n",
      "tensor([[-1.3615,  0.6699,  6.7651,  0.3800, -2.2892, -0.3658]]) 0\n",
      "tensor([[-1.8133,  1.0058,  1.8193, -1.7945, -3.0833,  4.3950]]) 0\n",
      "tensor([[-0.4225,  2.5524,  4.5780,  0.2665, -3.0776, -0.4838]]) 1\n",
      "tensor([[ 0.0173,  8.2995,  0.0308, -1.0058, -1.4146, -4.3482]]) 1\n",
      "tensor([[-2.1954,  0.1795, -1.5406,  0.0685, -0.6277,  3.6762]]) 0\n",
      "tensor([[-0.7001,  1.8114,  1.9907,  0.0175, -2.4095,  1.0598]]) 0\n",
      "tensor([[ 1.5036, -0.4130,  2.6340,  0.1511, -2.0916, -1.5043]]) 0\n",
      "tensor([[-2.4577, -1.5718, -0.5048,  4.8901, -0.5303,  1.0880]]) 0\n",
      "tensor([[-0.4513, -0.2334,  0.5134, -0.4948, -1.0534,  1.7097]]) 0\n",
      "tensor([[ 0.2356, -1.3397, -0.8136,  2.7753, -0.1616, -0.7931]]) 0\n",
      "tensor([[ 4.2017,  0.7690, -1.4710,  0.6966, -2.8166, -2.5906]]) 1\n",
      "tensor([[-2.3310, -0.2822, -1.8149,  1.0717,  1.9139,  1.1907]]) 0\n",
      "tensor([[-1.4711, -1.8318,  1.5110,  8.5243, -0.7502, -2.4469]]) 1\n",
      "tensor([[-4.8469, -1.0220, -0.6089, -1.4973,  0.7461,  5.4387]]) 1\n",
      "tensor([[-1.8327, -0.5416, -4.2423,  1.4050,  3.2102, -0.5094]]) 1\n",
      "tensor([[-1.0818, -1.9019,  3.3456,  5.0560, -1.9853, -0.9876]]) 0\n",
      "tensor([[-1.8908, -2.2510, -1.9338,  4.9205, -0.3554,  1.5274]]) 1\n",
      "tensor([[-0.9718, -2.2726, -0.9520,  7.9171, -1.3147, -1.0408]]) 1\n",
      "tensor([[ 4.2611,  0.5092, -3.7852,  2.8815, -2.1438, -4.8151]]) 1\n",
      "tensor([[-0.2026,  3.2376,  3.3235,  1.0528, -3.3965, -2.5103]]) 1\n",
      "tensor([[ 0.5594, 11.3590, -0.5015, -3.0120, -0.8301, -5.2215]]) 0\n",
      "tensor([[ 0.7148,  0.0052,  2.1031,  0.6420, -1.0077, -1.3338]]) 0\n",
      "tensor([[-1.8125, -1.8043, -3.6633,  0.7391,  6.8674, -3.2586]]) 1\n",
      "tensor([[ 1.4844,  6.0967, -0.4025, -1.0553, -1.9923, -4.2080]]) 1\n",
      "tensor([[-1.2802, -0.3222,  3.3793, -0.7070,  0.7226, -0.1252]]) 1\n",
      "tensor([[ 0.9960,  4.4929,  0.7820, -0.2753, -1.1029, -4.1669]]) 0\n",
      "tensor([[-3.1123, -2.0885, -1.0222, -0.3428, -0.7313,  6.9173]]) 0\n",
      "tensor([[-0.0178, -1.1615,  2.2776,  7.1453, -2.8307, -1.7670]]) 0\n",
      "tensor([[-0.5473,  4.6675, -0.0938, -1.8765, -0.8106, -0.9675]]) 0\n",
      "tensor([[ 1.3356, -4.0853, -2.6787,  6.3285, -0.5880, -1.6865]]) 0\n",
      "tensor([[ 4.6307, -0.0616, -3.4971, -0.6440, -0.5139, -7.5065]]) 1\n",
      "tensor([[-2.8233, -0.0529, -3.5432,  0.6147, -0.4525,  4.9733]]) 0\n",
      "tensor([[-0.4899, -2.7366, -1.1893,  5.2907,  0.1164, -0.5623]]) 0\n",
      "tensor([[ 0.8486,  5.3945,  1.6029, -2.1224, -0.3080, -5.2995]]) 0\n",
      "tensor([[-0.1571,  8.1084,  0.2455, -1.1147, -0.4215, -3.8654]]) 1\n",
      "tensor([[ 0.5265, -1.4752,  1.9090,  4.5271, -1.6384, -1.9411]]) 1\n",
      "tensor([[-1.2188, -2.2126,  0.2353,  7.2710, -1.4419, -0.2512]]) 1\n",
      "tensor([[-2.1141, -2.3244, -0.3449, -1.2190,  5.4336, -0.3991]]) 0\n",
      "tensor([[-0.8624,  1.0334,  4.6938, -0.1118, -2.7146,  0.3388]]) 0\n",
      "tensor([[-1.9878, -0.7751, -0.7266,  0.0992,  1.2524,  1.8302]]) 0\n",
      "tensor([[-2.1163, -2.4393, -0.8997, -0.1105,  4.3882, -0.1050]]) 1\n",
      "tensor([[ 2.2992,  3.9344, -0.3423, -1.3296, -1.1808, -4.9279]]) 1\n",
      "tensor([[-1.0810, -1.9482, -2.1853, -0.6569,  5.3432, -2.8783]]) 1\n",
      "tensor([[-0.5363, -0.6741, -1.0093,  4.6742, -0.8617, -0.0685]]) 1\n",
      "tensor([[-0.8808, -1.5106,  0.3697,  3.7458, -0.0138, -1.1120]]) 0\n",
      "tensor([[ 0.9229,  6.2192, -0.8384, -2.7675, -0.2887, -3.9878]]) 0\n",
      "tensor([[-0.6552, -0.6829, -3.1325,  0.0972,  5.5069, -3.7467]]) 0\n",
      "tensor([[ 0.3569,  3.0629,  7.2627, -1.6663, -3.7558, -2.3655]]) 0\n",
      "tensor([[ 0.0923,  1.7377,  5.1639,  0.4494, -1.2167, -3.7849]]) 0\n",
      "tensor([[ 0.4987, -0.9371,  1.0342,  2.6515, -1.1427, -1.0131]]) 0\n",
      "tensor([[-2.2472,  1.1866, -1.4813, -2.3473,  5.4031, -1.0986]]) 1\n",
      "tensor([[ 3.4253, -0.5232, -1.9336, -0.1181, -1.6322, -1.3692]]) 0\n",
      "tensor([[ 0.4190,  0.4152,  0.6816,  2.4423, -1.2133, -1.5717]]) 0\n",
      "tensor([[-0.3056,  3.3574, -1.2430, -1.1218,  0.0222, -2.1423]]) 1\n",
      "tensor([[-1.3013, -4.4101, -2.4460, 12.3228, -1.3230, -0.9871]]) 1\n",
      "tensor([[-1.4946,  0.8367,  7.6791, -0.5701, -1.2768, -1.1497]]) 0\n",
      "tensor([[-4.0055,  0.0122, -2.7591, -0.5870,  0.2482,  5.5357]]) 1\n",
      "tensor([[-1.2606e+00, -2.2422e-01,  5.4286e+00,  2.1410e-03, -1.9009e+00,\n",
      "          1.3213e+00]]) 1\n",
      "tensor([[-5.0069, -1.8177, -0.9294, -1.1513,  0.1224,  7.8574]]) 1\n",
      "tensor([[-2.0837, -1.2063,  1.4448,  9.7343, -2.1947, -1.4705]]) 1\n",
      "tensor([[-2.0935, -1.5804, -2.8813,  1.2045,  4.5766, -0.9083]]) 1\n",
      "tensor([[-3.6193, -2.2535, -5.5449,  0.3686,  8.8446, -0.5453]]) 1\n",
      "tensor([[-0.5829, -2.5138,  1.8085,  6.5146, -1.3679, -1.1166]]) 1\n",
      "tensor([[-2.4927, -1.0195,  6.1586,  0.2932, -1.6221,  2.3719]]) 0\n",
      "tensor([[ 1.7070,  5.8590, -1.0500, -2.5488, -1.3986, -3.8422]]) 0\n",
      "tensor([[-1.0169, -3.0032, -3.3420,  6.5596, -0.3279, -0.1839]]) 0\n",
      "tensor([[ 1.1840,  3.0273,  8.5940, -0.8683, -4.7970, -3.3373]]) 0\n",
      "tensor([[-1.2974,  0.6026,  1.2151, -2.2579, -1.1383,  2.7889]]) 0\n",
      "tensor([[-2.7134,  0.2556,  0.7987, -2.7998,  3.0529,  1.4115]]) 0\n",
      "tensor([[-1.6619,  1.3327, 10.6568, -1.3563, -2.8371,  0.7506]]) 1\n",
      "tensor([[-1.0491, -1.7947, -1.6081,  1.7254,  4.8763, -2.3076]]) 0\n",
      "tensor([[-0.2245, -0.1871, -1.6767,  3.9911, -0.7655, -1.0492]]) 1\n",
      "tensor([[-0.3577,  2.2397,  6.7665, -0.5634, -2.4486, -2.1392]]) 0\n",
      "tensor([[-0.1543, -0.1486,  4.3721,  0.1737, -1.1634,  0.3577]]) 1\n",
      "tensor([[-1.3570,  5.3653,  2.0973, -1.3940, -1.5162, -2.0878]]) 1\n",
      "tensor([[-0.3511,  0.0440,  5.9791, -0.2512, -1.1566, -1.2977]]) 1\n",
      "tensor([[-0.2205, -1.4739, -0.5189,  4.7807, -1.4286, -0.4229]]) 0\n",
      "tensor([[ 2.1244,  1.3715, -0.5911, -1.0848, -2.3218, -1.1814]]) 1\n",
      "tensor([[-1.5935, -1.3421, -1.3296,  1.7369,  4.6198, -2.0907]]) 1\n",
      "tensor([[-1.1465,  0.1031,  6.3720, -0.0730, -1.9996, -0.6527]]) 0\n",
      "tensor([[-2.2132, -1.6978, -1.4464,  5.7075, -0.0444,  0.5975]]) 1\n",
      "tensor([[ 0.5235, -0.2595,  5.0369,  0.6610, -2.7077, -1.0898]]) 1\n",
      "tensor([[-2.0934,  6.5923,  1.1446, -2.3385, -0.7650, -1.0737]]) 1\n",
      "tensor([[ 1.1921,  4.3466,  3.5082, -0.7749, -2.8364, -3.5965]]) 0\n",
      "tensor([[-0.1893, -0.3012, -1.7558,  4.6460,  0.0964, -1.9431]]) 1\n",
      "tensor([[-2.0085, -3.7631, -0.7692,  7.6757,  0.1573,  0.5292]]) 1\n",
      "tensor([[-1.6596, -1.4882,  0.0550,  7.7959,  0.7559, -2.9046]]) 1\n",
      "tensor([[-2.7315e+00,  6.2477e-02, -1.3578e+00, -2.2055e-03,  4.2107e+00,\n",
      "         -8.0373e-01]]) 0\n",
      "tensor([[-0.0252, -0.8199, -0.3618,  2.7323, -0.7404, -1.0828]]) 0\n",
      "tensor([[-1.3989, -0.7004,  3.8721,  0.2062, -0.5713, -0.4817]]) 0\n",
      "tensor([[-4.0112, -1.9668, -2.9225,  0.6377,  6.2269,  0.7373]]) 0\n",
      "tensor([[-0.6667,  4.9037, -0.7602, -2.0379, -0.2147, -1.6382]]) 1\n",
      "tensor([[-0.9369,  1.1406, -3.1814, -0.6072,  4.2478, -2.6942]]) 1\n",
      "tensor([[-0.3446,  0.8160,  5.3979,  0.5998, -1.1133, -1.9794]]) 0\n",
      "tensor([[-0.0295,  0.4972,  3.2421,  2.1556, -2.1295, -0.6980]]) 0\n",
      "tensor([[-0.6876,  2.2705, -2.8462, -0.8136,  4.6611, -3.7677]]) 1\n",
      "tensor([[ 3.9328, -1.3830, -1.6204, -1.6263, -0.2891, -0.7915]]) 1\n",
      "tensor([[ 2.1226, -0.4504,  0.2423,  3.0421, -3.1526, -1.9127]]) 0\n",
      "tensor([[-1.2286, -1.0640,  7.2040,  2.8391, -3.0956, -0.7544]]) 0\n",
      "tensor([[ 2.4684, -1.7461, -1.5720,  2.2860,  0.1252, -3.4189]]) 1\n",
      "tensor([[ 0.1619, -0.2770, -2.0059, -0.4732,  0.7752, -0.1093]]) 0\n",
      "tensor([[ 4.8943, -1.7149, -1.7927, -0.9657, -1.3817, -3.1103]]) 0\n",
      "tensor([[-1.9315, -0.0805, -0.8875, -0.2100, -0.3629,  2.5300]]) 0\n",
      "tensor([[-1.1560,  1.2320,  5.8469,  1.4392, -2.1220, -1.1421]]) 0\n",
      "tensor([[ 1.8899,  8.9464, -0.5621, -1.6608, -2.8449, -5.3731]]) 0\n",
      "tensor([[-0.2290,  3.5909,  0.1403,  3.7312, -1.8281, -3.7901]]) 1\n",
      "tensor([[-4.1094, -0.1556,  0.7565, -1.2223,  1.6181,  4.0962]]) 0\n",
      "tensor([[ 0.4812,  6.4833, -1.3286, -2.6614, -0.0444, -3.0665]]) 0\n",
      "tensor([[-1.6147, -1.4459,  6.5990,  3.1348, -1.4167, -1.0204]]) 1\n",
      "tensor([[-3.8032e-03,  4.5041e-01,  9.1763e+00,  1.8389e-01, -2.2350e+00,\n",
      "         -2.3745e+00]]) 1\n",
      "tensor([[-3.3837, -2.1497, -1.8041, -0.5229, -1.1370,  8.5001]]) 1\n",
      "tensor([[-0.9725, -0.9884, -2.9642, -0.3781,  3.8939, -2.3647]]) 1\n",
      "tensor([[-1.3681, -1.7495, -1.9773, -0.5279, -0.4183,  4.5121]]) 1\n",
      "tensor([[-1.7723,  0.1484, -2.4100, -0.2988, -1.4055,  4.5748]]) 1\n",
      "tensor([[ 0.1657, -0.4389,  4.3054,  2.3461, -1.3833, -2.7143]]) 1\n",
      "tensor([[-2.9005, -1.2470, -0.2844, -0.4771,  0.0325,  5.1606]]) 0\n",
      "tensor([[ 0.0132,  8.2460,  0.2015, -1.0313, -1.5665, -4.5209]]) 1\n",
      "tensor([[-2.5397, -0.0657, -1.4115,  0.1072, -0.9594,  3.5448]]) 1\n",
      "tensor([[-0.6881, -0.8250,  3.4975, -0.6704, -0.4687,  0.5140]]) 0\n",
      "tensor([[-1.6063, -1.1064, -1.8254, -0.1781, -1.1044,  4.2803]]) 1\n",
      "tensor([[ 0.2365,  1.8225,  3.7636, -1.5380, -1.3680, -1.5116]]) 0\n",
      "tensor([[-2.4445, -0.9602,  8.1889, -0.6881, -2.3900,  2.2505]]) 1\n",
      "tensor([[ 3.5916,  0.0100, -1.2924, -0.2593, -2.6377, -2.0455]]) 1\n",
      "tensor([[-1.1730, -2.1837,  3.3064,  5.5590, -1.2718, -1.1804]]) 0\n",
      "tensor([[-0.1287, -0.5541, -1.7051, -0.6725,  3.2871, -2.7079]]) 0\n",
      "tensor([[-1.2492, -0.6220, -3.0162,  1.3385, -1.4652,  3.5776]]) 0\n",
      "tensor([[-1.5465,  1.7611,  1.2834, -0.5136,  0.8753, -0.6087]]) 0\n",
      "tensor([[-2.1285, -1.0279, -0.5149,  1.3503,  1.3172,  1.0086]]) 0\n",
      "tensor([[-2.6763,  0.7187,  5.5304,  0.0177, -3.1419,  2.3345]]) 1\n",
      "tensor([[ 3.5693, -0.9802, -3.9625, -0.9565,  0.0675, -3.1366]]) 0\n",
      "tensor([[-2.6701,  0.6885,  2.7799, -0.8228, -2.8564,  4.4799]]) 0\n",
      "tensor([[-2.1009, -2.4036,  0.1523,  8.0303, -0.8755, -0.7838]]) 1\n",
      "tensor([[-1.9511, -0.9652, -1.2270, -0.0749,  3.5398, -0.6275]]) 0\n",
      "tensor([[ 3.1996, -2.1638, -2.8472,  3.2805, -0.8096, -3.4023]]) 1\n",
      "tensor([[-0.6845,  5.2837, -3.0943, -1.7827, -0.3593, -2.0269]]) 1\n",
      "tensor([[-1.4692,  3.2032, -0.3512,  3.4727, -1.1045, -2.1085]]) 1\n",
      "tensor([[-2.6483,  0.4560, -3.9378,  0.1146,  6.0958, -2.8801]]) 0\n",
      "tensor([[-0.0242, -2.4098, -2.0774,  3.0785,  2.3062, -2.4271]]) 1\n",
      "tensor([[ 0.5717, -0.7486, -3.9469,  6.8289,  0.6328, -3.8908]]) 1\n",
      "tensor([[-3.2721, -0.5461, -0.7589, -0.7030, -1.3698,  6.2015]]) 1\n",
      "tensor([[-0.0985, -1.1742, -2.4113,  0.5202,  5.1983, -5.3095]]) 1\n",
      "tensor([[-0.8674,  1.3708,  5.7862, -1.0298, -1.6992, -0.1248]]) 1\n",
      "tensor([[-0.4940, -1.3977, -1.6258, -0.4755,  3.1039, -0.7773]]) 0\n",
      "tensor([[ 3.8227, -0.3536, -2.9104, -0.8749, -1.8905, -1.6116]]) 1\n",
      "tensor([[ 0.6937, -0.3492,  2.7410, -0.8858,  0.8144, -2.8056]]) 0\n",
      "tensor([[-2.8675,  6.6009,  2.6920, -2.9549,  0.7588, -2.4773]]) 0\n",
      "tensor([[ 0.3844, -1.5172,  0.2846,  4.4758, -1.9319, -1.8048]]) 1\n",
      "tensor([[ 0.7530,  4.3082,  0.6054, -2.4232, -1.3170, -2.1599]]) 0\n",
      "tensor([[-2.5856, -0.6452,  0.3825,  0.7945, -2.5220,  4.6055]]) 0\n",
      "tensor([[ 0.8197,  6.4831, -0.0981, -2.4750, -1.8675, -3.1539]]) 1\n",
      "tensor([[-0.8766, -0.6675, -0.3424,  5.4909, -1.5959, -1.1955]]) 1\n",
      "tensor([[-1.5059,  0.0424,  2.1219,  3.6976, -2.1254,  0.4762]]) 0\n",
      "tensor([[-1.8317, -0.8946, -2.1356, -0.0540, -3.0582,  4.9880]]) 0\n",
      "tensor([[-0.5686, -3.1405, -2.2749,  7.4654,  0.0377, -1.6822]]) 0\n",
      "tensor([[-0.9917,  5.1729,  0.6015, -1.7924, -0.3601, -0.8257]]) 1\n",
      "tensor([[-0.1435, -4.4613, -1.3647,  2.0869, -1.6511,  4.0643]]) 1\n",
      "tensor([[-0.6586,  3.4021,  7.5390, -2.2760, -1.7580, -2.3955]]) 1\n",
      "tensor([[-3.0398, -1.7529, -0.3632,  3.8215,  2.3667, -0.6217]]) 0\n",
      "tensor([[-0.0894, -1.7389, -0.9441,  4.7469, -0.4816, -1.4537]]) 1\n",
      "tensor([[-0.6093,  1.0028,  5.6296,  1.1690, -1.5644, -2.3403]]) 0\n",
      "tensor([[-4.5175, -0.0966, -2.0287, -1.2552, -2.4492,  9.0732]]) 0\n",
      "tensor([[-0.0946, -1.0978,  4.7522,  5.9183, -3.6759, -2.3181]]) 1\n",
      "tensor([[-1.7964, -0.4202, -2.6223,  0.9022,  4.0678, -1.5094]]) 1\n",
      "tensor([[-0.4724, -0.7469, -0.7892, -0.8080,  0.3783,  1.3367]]) 1\n",
      "tensor([[ 3.9687, -1.9725, -2.5711, -0.4426,  1.0643, -4.0313]]) 1\n",
      "tensor([[-1.4411,  8.4869,  0.4640, -2.5162,  0.5167, -3.8636]]) 1\n",
      "tensor([[ 4.2399,  0.5362, -3.2983, -0.1740, -2.0180, -3.5770]]) 1\n",
      "tensor([[-1.0137,  4.8810,  0.8877, -0.7357, -1.8031, -0.8917]]) 0\n",
      "tensor([[-1.4590,  0.4752, -1.5589, -0.9132,  1.2268,  1.1389]]) 0\n",
      "tensor([[-1.4330, -1.6993,  1.0744, -0.6061, -0.8223,  4.6935]]) 0\n",
      "tensor([[ 4.7051, -1.2531,  0.9868,  0.0432, -0.9097, -4.6458]]) 1\n",
      "tensor([[-1.3042, -1.8644,  1.1616,  6.3259, -1.8025, -0.3857]]) 0\n",
      "tensor([[-0.6553, -0.0472, -2.7918,  1.1286, -3.2398,  3.6150]]) 1\n",
      "tensor([[-2.6750, -1.8349, -3.8209,  0.6871,  4.8633,  0.9971]]) 1\n",
      "tensor([[ 2.0774,  1.4403,  0.1577, -1.8511, -1.7481, -2.2938]]) 0\n",
      "tensor([[ 0.3395,  2.5454,  7.9085, -1.1878, -3.5280, -2.0858]]) 1\n",
      "tensor([[-4.6625, -1.7908, -1.0210,  0.0195,  0.0800,  7.4327]]) 1\n",
      "tensor([[-2.8785, -2.2395, -1.9075, -0.8316,  1.3215,  3.7633]]) 0\n",
      "tensor([[-2.3964,  0.3155, -4.3784, -1.1967,  7.5413, -2.7368]]) 1\n",
      "tensor([[-0.1741,  2.6837,  4.0976, -0.5057, -0.7396, -2.4551]]) 0\n",
      "tensor([[-2.0711,  0.0618, -2.1591,  5.5197, -0.6280,  0.0909]]) 0\n",
      "tensor([[ 0.9594,  3.6192, -1.7696, -1.5930, -0.7943, -2.9059]]) 0\n",
      "tensor([[-0.6231,  1.4903, -0.9014,  2.7160, -1.5198, -0.7205]]) 0\n",
      "tensor([[-2.7085, -0.5158, -3.5829, -1.5224,  5.4417, -0.8955]]) 1\n",
      "tensor([[ 3.8279, -0.3037, -1.6443, -0.9711, -0.7705, -3.0950]]) 1\n",
      "tensor([[-2.6256, -1.5339, -0.3736,  5.9631,  1.3517, -0.3283]]) 0\n",
      "tensor([[-2.1789, -3.3060, -5.0399,  0.1620,  5.5196,  1.5313]]) 1\n",
      "tensor([[ 4.3459, -0.8604, -1.8331, -0.6900, -1.7301, -3.0192]]) 1\n",
      "tensor([[-2.5606,  0.1464, -1.7286, -1.3862,  4.0964,  0.5404]]) 1\n",
      "tensor([[-1.4267,  4.3945, -0.9302,  0.6950, -0.2538, -1.6119]]) 1\n",
      "tensor([[-1.0752,  6.5298,  0.4478, -0.1791, -1.4999, -1.9216]]) 1\n",
      "tensor([[-1.2086,  5.1341, -1.5613, -1.9920,  1.1911, -2.5100]]) 1\n",
      "tensor([[-1.1496, -2.7628,  0.0502,  7.8285, -0.0902, -1.2566]]) 1\n",
      "tensor([[-1.3357,  1.5259, -1.8961,  1.4051, -0.3587, -0.1191]]) 0\n",
      "tensor([[ 2.4128, -1.1485,  3.2030,  0.6686,  0.1114, -4.8433]]) 0\n",
      "tensor([[-1.8463, -0.7179,  4.2411,  4.5830, -1.8650, -0.6281]]) 1\n",
      "tensor([[ 0.7273,  0.0334,  5.2920, -0.4049, -1.9821, -1.8431]]) 1\n",
      "tensor([[-1.8145,  1.8322,  0.4530, -0.2690, -0.9796,  1.0220]]) 0\n",
      "tensor([[-1.5193, -2.5747,  0.8704,  6.8509, -0.8843, -0.0363]]) 1\n",
      "tensor([[ 0.6077,  2.6273,  3.1307,  0.7646, -1.8967, -4.2794]]) 1\n",
      "tensor([[-0.4310,  0.1478, -3.0158,  6.1246,  0.7164, -3.0199]]) 1\n",
      "tensor([[ 0.1731,  1.0257, -1.2186,  3.3860, -1.1527, -0.7929]]) 0\n",
      "tensor([[-0.7572, -1.9675,  1.3837,  7.7097, -1.7201, -1.1904]]) 1\n",
      "tensor([[ 3.9696,  0.0698, -0.4800, -0.0507, -1.6981, -2.2416]]) 1\n",
      "tensor([[-3.0937,  0.9690, -1.2235,  1.1036,  3.8914, -2.5128]]) 0\n",
      "tensor([[-0.7883,  1.3931,  5.4730, -0.7767, -2.0853, -0.1845]]) 1\n",
      "tensor([[ 0.5854,  2.0010,  1.2158,  0.2141, -1.9487, -2.5161]]) 0\n",
      "tensor([[-1.6669, -0.9044,  2.0203,  0.2839, -0.9722,  2.5145]]) 0\n",
      "tensor([[-0.4813, -0.7888, -3.2161,  1.0330,  2.2449, -1.1586]]) 0\n",
      "tensor([[-1.2051,  0.8583,  1.8526,  4.8688, -1.2935, -1.6680]]) 1\n",
      "tensor([[ 0.6870,  6.3318, -0.1773, -2.2241, -1.9366, -2.3478]]) 1\n",
      "tensor([[-0.2862, -3.4283, -1.5378,  1.6816, -0.8111,  3.6386]]) 1\n",
      "tensor([[-3.1687, -2.0577, -2.2584,  0.0315,  1.1932,  7.3989]]) 1\n",
      "tensor([[-3.7068, -1.4885, -3.3229,  1.1150,  0.2171,  5.5906]]) 1\n",
      "tensor([[ 0.2530, -3.5274, -2.0367,  6.7654, -1.4259, -0.6603]]) 0\n",
      "tensor([[-0.5872, -0.7068,  8.8060,  0.6520, -2.3810, -1.3846]]) 0\n",
      "tensor([[ 3.2945, -2.9779,  2.9798,  3.2382, -1.6632, -5.2580]]) 1\n",
      "tensor([[-1.4124, -2.4500, -1.1373,  6.7875,  0.6716, -1.4278]]) 0\n",
      "tensor([[-0.4063, -2.4766, -3.8339,  1.9700,  4.7489, -2.2533]]) 0\n",
      "tensor([[-0.0135,  1.8732,  4.9584,  1.3148, -3.6132, -1.5007]]) 0\n",
      "tensor([[-1.3650, -2.0324, -3.0918,  1.9968,  5.4017, -2.8826]]) 1\n",
      "tensor([[-1.1907,  0.0150, -2.9901, -1.1788,  4.3385, -1.8039]]) 1\n",
      "tensor([[-2.8707,  0.4634, -1.4657, -0.8683, -0.3873,  4.6625]]) 0\n",
      "tensor([[-1.0086, -3.3169, -1.9296,  6.5073, -0.0071,  0.5479]]) 1\n",
      "tensor([[-3.0058, -1.8326, -1.1114,  1.2410,  0.7364,  3.8469]]) 1\n",
      "tensor([[ 3.6635, -1.8119, -0.8183, -0.8444,  0.2136, -3.0033]]) 1\n",
      "tensor([[-2.3204,  0.0791,  4.0673, -0.6592, -0.0946,  0.8822]]) 1\n",
      "tensor([[ 1.2396,  2.6811, -1.8268,  1.5689, -1.5498, -2.4976]]) 0\n",
      "tensor([[-1.5948,  0.0721,  0.5797,  3.5793, -1.1470, -0.4719]]) 0\n",
      "tensor([[-1.3324,  3.5784,  4.7954, -1.4474, -0.5611, -1.8930]]) 0\n",
      "tensor([[-2.7140, -0.3825, -3.6861, -1.7074,  4.9929, -0.0631]]) 0\n",
      "tensor([[-1.3851, -2.5551,  0.5792,  6.7694, -0.7254, -1.2682]]) 1\n",
      "tensor([[-0.4872,  6.2965,  0.6650, -1.6170, -0.8788, -2.4868]]) 0\n",
      "tensor([[-2.5507, -1.7135, -1.4313,  0.4897, -0.0167,  4.4197]]) 1\n",
      "tensor([[ 1.8387e+00,  4.8427e+00,  1.3490e-03, -1.3169e+00, -2.4650e+00,\n",
      "         -2.5433e+00]]) 0\n",
      "tensor([[-1.4543, -2.1797, -0.2558,  1.9436, -1.9251,  3.1475]]) 0\n",
      "tensor([[-0.7979,  7.0804, -0.9819, -3.4379, -0.2681, -1.9108]]) 0\n",
      "tensor([[ 3.9832,  0.2033, -2.2500, -1.9146, -1.6555, -2.5493]]) 1\n",
      "tensor([[-0.1403,  0.6467,  0.4345,  3.3944, -3.0055, -0.6440]]) 0\n",
      "tensor([[-0.0469, -2.5166, -1.8773,  9.0458, -2.5929, -0.4396]]) 1\n",
      "tensor([[-0.7604,  5.4842, -1.9719, -2.5173,  0.4470, -2.4796]]) 0\n",
      "tensor([[-0.3328, -1.1106,  0.7618,  5.7560, -2.0160, -1.3945]]) 1\n",
      "tensor([[-1.1297, -0.5299,  0.2249,  3.7361, -2.4119,  1.3262]]) 0\n",
      "tensor([[-1.8722,  2.0028,  1.0705, -1.7374,  2.8528, -2.4686]]) 0\n",
      "tensor([[ 3.5249, -1.4512, -2.8708,  0.5476, -0.5481, -4.8479]]) 1\n",
      "tensor([[-0.6248, -0.3617, -1.8111, -0.8212,  3.3897, -1.6944]]) 1\n",
      "tensor([[-2.5809, -0.5641, -1.1988, -0.1710, -0.7054,  4.3026]]) 0\n",
      "tensor([[-3.2786,  0.2283,  5.3956,  0.0523, -0.5415,  1.9185]]) 1\n",
      "tensor([[-2.2000, -1.4268, -2.0622, -0.4957,  5.7446, -0.6462]]) 1\n",
      "tensor([[-2.4323, -2.1520, -1.3292,  1.0566, -0.9272,  4.4259]]) 1\n",
      "tensor([[-1.8212, -0.1574, -3.2545, -1.6337,  5.0329, -0.9382]]) 1\n",
      "tensor([[-0.8232, -0.2939, -2.5311, -1.2977,  1.0286,  0.7317]]) 0\n",
      "tensor([[-3.2541, -1.8764, -1.6226, -0.0402, -1.4319,  7.6710]]) 1\n",
      "tensor([[ 0.0779,  0.1901,  1.3683,  3.2983, -1.6930, -1.5023]]) 0\n",
      "tensor([[-2.8603, -2.0759, -0.4455, -0.2077, -0.6850,  7.1860]]) 1\n",
      "tensor([[-0.2718, -1.9946,  3.2122, -0.1272, -0.0218, -1.5347]]) 1\n",
      "tensor([[-0.1966,  4.2118, -1.5116, -1.4974,  0.5828, -2.5342]]) 0\n",
      "tensor([[ 4.9817, -0.0305,  0.0619,  1.3642, -2.4716, -4.8000]]) 1\n",
      "tensor([[-0.6029,  0.7851,  6.7076,  0.6137, -1.9473, -1.9228]]) 0\n",
      "tensor([[-3.9526, -4.5073, -3.3602,  1.7091,  8.2202, -0.5504]]) 0\n",
      "tensor([[-1.3747,  0.7585, -0.9317, -1.3992,  4.5968, -2.1209]]) 0\n",
      "tensor([[-1.8610, -1.8231, -0.8718,  1.0473, -0.2532,  2.9969]]) 0\n",
      "tensor([[-1.8293, -2.8396, -2.1941,  1.0038,  4.4973, -0.2994]]) 0\n",
      "tensor([[-2.9381, -0.0662, -1.2580, -0.8422, -2.5066,  7.5327]]) 1\n",
      "tensor([[-1.4344, -1.3517, -0.2940,  3.2746,  0.4920, -0.4696]]) 0\n",
      "tensor([[-1.5543,  1.4836,  8.3199, -0.7427, -1.9471, -1.1473]]) 1\n",
      "tensor([[ 2.8436,  1.7791, -3.1867, -2.1619,  2.5560, -6.5650]]) 0\n",
      "tensor([[-1.6003,  0.0818,  1.7810,  1.7353, -0.6278,  0.6878]]) 0\n",
      "tensor([[ 0.8560,  8.2601, -0.7833, -4.0267, -0.6724, -3.8657]]) 0\n",
      "tensor([[-0.4864, 10.9237, -0.2841, -1.7281, -1.5938, -4.7322]]) 0\n",
      "tensor([[ 1.9086,  1.4168, -2.3323,  0.0899, -1.0503, -2.1777]]) 0\n",
      "tensor([[-2.1970, -2.7833,  0.4705,  9.7036, -1.0288, -0.5391]]) 1\n",
      "tensor([[-3.3937, -2.2392,  1.0057,  0.5348,  0.2583,  4.6645]]) 1\n",
      "tensor([[-3.0144, -1.7959,  1.4349,  0.0727, -1.9523,  5.2501]]) 1\n",
      "tensor([[ 0.7545,  7.4984, -0.7684, -3.2856, -0.5142, -3.8671]]) 1\n",
      "tensor([[ 1.0589,  2.7658, -1.9732,  1.5239, -0.5108, -3.3754]]) 1\n",
      "tensor([[-2.4360, -0.6596,  1.5074,  2.6244, -2.0995,  2.3498]]) 0\n",
      "tensor([[-1.0790,  1.8568,  1.5438, -0.2045, -0.7725, -0.6617]]) 0\n",
      "tensor([[-2.4794, -2.5917, -4.8926,  1.7804,  8.1975, -3.6759]]) 0\n",
      "tensor([[-2.3755, -1.6656,  0.9540, -1.0402,  0.8521,  2.5078]]) 0\n",
      "tensor([[-0.7675,  3.4899, -2.4200, -1.5479,  0.2479, -0.2543]]) 1\n",
      "tensor([[-2.2449,  0.5560,  5.8838, -0.4096, -3.1474,  1.9897]]) 1\n",
      "tensor([[ 0.7765, -0.3236,  0.5017,  5.0597, -1.1735, -3.6453]]) 1\n",
      "tensor([[-0.5877, -0.0976, -1.1983, -1.2234,  3.3790, -1.8554]]) 1\n",
      "tensor([[-3.2600, -1.9069, -1.2908,  2.1468, -1.3862,  5.6912]]) 0\n",
      "tensor([[-4.0168, -2.8175, -1.8762,  1.0386, -1.1586,  7.7902]]) 1\n",
      "tensor([[-1.3585,  3.4711, -2.1669, -1.5691,  3.0359, -1.7563]]) 0\n",
      "tensor([[-0.3858, -2.4656, -0.6346,  8.2410, -1.0666, -1.9814]]) 0\n",
      "tensor([[-0.3688,  2.4244, -1.6217, -0.1233,  2.2596, -3.2890]]) 0\n",
      "tensor([[ 0.7763, -2.0455,  0.9354,  0.1662,  1.0023, -3.0442]]) 1\n",
      "tensor([[ 3.3525, -0.6964, -0.3772, -0.9166, -0.5433, -2.8698]]) 0\n",
      "tensor([[ 0.7469,  0.2763, -1.8087, -1.9186,  2.7537, -3.8060]]) 0\n",
      "tensor([[-2.0342, -1.0094, -0.4124, -0.6058,  0.0523,  4.1767]]) 1\n",
      "tensor([[-2.9740,  0.1405,  0.9539,  4.3112,  0.8620, -1.3946]]) 1\n",
      "tensor([[ 3.6243,  1.9846, -0.2376, -1.4591, -2.8480, -2.7462]]) 1\n",
      "tensor([[-0.2054,  7.4097, -0.1936, -1.8776, -1.1052, -2.6317]]) 1\n",
      "tensor([[-0.6923,  6.0505, -1.5961,  1.8837, -2.3108, -2.9420]]) 1\n",
      "tensor([[ 2.5456, -0.4480, -1.0774,  0.5958, -1.0295, -1.3496]]) 0\n",
      "tensor([[-1.9660, -0.0553, -1.0104,  0.0434, -1.3359,  4.7759]]) 1\n",
      "tensor([[ 4.1444,  3.2040, -4.4077, -0.7206, -2.4333, -4.0389]]) 1\n",
      "tensor([[-1.3079,  0.3609, -2.9400,  0.4159,  4.1756, -2.5488]]) 0\n",
      "tensor([[-3.7613, -1.8500, -1.8871, -0.7301,  0.5495,  6.6154]]) 1\n",
      "tensor([[-0.2594,  5.5759,  0.1979,  1.3258, -2.7190, -2.2196]]) 1\n",
      "tensor([[ 0.3173,  5.0204, -1.7419, -2.6864, -0.3589, -2.2903]]) 0\n",
      "tensor([[ 0.0811,  1.7477,  6.7239, -0.4887, -2.6030, -2.0323]]) 0\n",
      "tensor([[-2.8303, -1.6821, -4.8636, -0.0957,  7.3560, -1.1808]]) 1\n",
      "tensor([[-0.1230,  1.9410,  3.2043, -0.6430, -0.3769, -3.5169]]) 0\n",
      "tensor([[-0.9446,  0.2377,  4.9025,  0.3969, -1.2048, -0.0711]]) 0\n",
      "tensor([[-1.1378,  0.1171,  5.2002,  1.9761, -2.3069, -0.2007]]) 1\n",
      "tensor([[-0.4065,  1.3682,  0.5060, -0.7850, -0.1304, -0.9631]]) 0\n",
      "tensor([[-0.3677, -2.1146, -0.5144,  4.7817,  0.0684, -0.8749]]) 0\n",
      "tensor([[-1.5909, -0.8132,  2.2885,  1.7509, -1.0468,  1.1311]]) 0\n",
      "tensor([[ 4.5966,  0.3989, -0.8034, -0.0993, -3.0355, -3.1092]]) 1\n",
      "tensor([[-1.0136,  4.7065,  0.2367, -0.5068, -0.3125, -2.1837]]) 0\n",
      "tensor([[ 0.1638, -1.7959, -3.4077, -1.7479, -0.6957,  3.4744]]) 0\n",
      "tensor([[-0.8199,  0.9387, -2.9012, -1.1793,  2.9605, -1.5134]]) 0\n",
      "tensor([[ 0.4406,  1.1513,  0.3312, -1.1093,  0.2135, -0.9210]]) 0\n",
      "tensor([[ 0.1123,  0.2714,  3.5263, -1.3350, -0.9533, -0.9027]]) 1\n",
      "tensor([[-1.6453, -1.5352, -1.2330,  0.4528, -2.9417,  5.9582]]) 1\n",
      "tensor([[-0.9179, -0.0777,  6.0930,  2.9975, -2.2196, -1.8790]]) 1\n",
      "tensor([[-1.2128,  3.1366,  0.2812, -1.5948, -0.1490,  0.9070]]) 0\n",
      "tensor([[-0.7935, -3.6744, -0.2186,  7.5260, -0.4308, -0.3649]]) 1\n",
      "tensor([[ 0.0431, -0.5458,  4.7666, -0.0736, -1.8113, -0.4347]]) 1\n",
      "tensor([[-0.7157, -1.0600, -0.7776,  3.8061, -1.0700,  0.0760]]) 1\n",
      "tensor([[-1.6591, -2.8504, -0.8029,  2.0183,  4.4981, -1.6250]]) 0\n",
      "tensor([[ 0.1439,  4.5830, -1.6919, -1.0586, -0.6818, -2.2673]]) 1\n",
      "tensor([[-4.3318, -2.1676, -2.4780,  0.6813,  6.9101, -0.8311]]) 1\n",
      "tensor([[ 0.2007, -1.8292, -2.6638,  0.6677,  6.4377, -3.9070]]) 0\n",
      "tensor([[-1.8903,  0.3429, -3.0114, -2.6544,  6.4827, -2.7104]]) 1\n",
      "tensor([[-0.9530, -0.5441,  1.2731,  7.1867, -2.4391, -1.8428]]) 0\n",
      "tensor([[-1.7454,  6.2145, -1.2010, -2.8654,  1.3949, -2.5345]]) 1\n",
      "tensor([[-0.0611,  1.1170,  3.5190,  0.1644, -1.3479, -2.3552]]) 1\n",
      "tensor([[-2.0287, -2.1125, -0.9374,  1.3124,  0.5347,  2.9725]]) 0\n",
      "tensor([[-0.1364,  7.7342, -0.2302, -2.6919, -0.0484, -4.7888]]) 0\n",
      "tensor([[-1.1814,  1.4412,  4.3885, -1.3092, -2.3617,  0.9671]]) 1\n",
      "tensor([[-2.2317, -0.6765,  2.0071,  2.1341, -1.6529,  1.6268]]) 0\n",
      "tensor([[-2.9566,  0.1566, -2.6791, -0.0912,  5.6646, -2.1126]]) 0\n",
      "tensor([[-1.5507, -3.1039, -3.0446,  1.8006,  5.0082, -0.0675]]) 0\n",
      "tensor([[ 1.3953, -2.1468, -3.1538, -0.3158, -0.8945,  1.7030]]) 0\n",
      "tensor([[-2.3925, -0.6108,  5.3479, -0.4598, -0.9531,  2.3266]]) 1\n",
      "tensor([[-2.4613,  5.0824,  6.7132, -0.9957, -2.3923, -0.6212]]) 1\n",
      "tensor([[-2.9269, -1.8464, -1.4886,  2.5523,  5.4595, -2.5094]]) 0\n",
      "tensor([[ 4.2299, -0.2207, -2.4749, -0.8320, -1.0377, -2.9808]]) 1\n",
      "tensor([[-2.3956, -1.9821, -5.5477,  0.9633,  9.8607, -4.9121]]) 1\n",
      "tensor([[ 2.5069, -0.1751, -0.5417, -0.5987, -0.5108, -2.7995]]) 1\n",
      "tensor([[-2.9039, -2.2640, -1.0579,  1.3628,  1.8246,  3.0830]]) 0\n",
      "tensor([[-2.0434, -2.9461,  0.3096,  6.6308,  0.8470, -0.6280]]) 1\n",
      "tensor([[ 4.0347,  0.6853,  0.0546, -1.2397, -0.6171, -4.7984]]) 1\n",
      "tensor([[-2.0093, -0.3995, -1.3336, -0.4584, -2.3433,  5.1194]]) 1\n",
      "tensor([[-1.0487,  2.5126,  8.2263, -0.7834, -2.6574, -1.3661]]) 1\n",
      "tensor([[-0.6401,  5.1765,  6.5883, -0.4047, -2.7520, -3.4760]]) 1\n",
      "tensor([[-0.3555,  4.1855,  0.3254, -1.9580, -0.4948, -1.0813]]) 0\n",
      "tensor([[-1.8225, -1.2382, -1.2847, -0.4166,  0.3678,  3.7694]]) 0\n",
      "tensor([[-0.8927, -0.4164, -1.7913, -1.0519,  0.9906,  0.5912]]) 0\n",
      "tensor([[-1.1276,  0.1613,  3.9627,  3.1732, -1.6127, -1.5599]]) 0\n",
      "tensor([[-1.8717,  7.3090,  3.5692, -2.6367, -1.1302, -2.5600]]) 0\n",
      "tensor([[-1.2238, -0.4590,  5.8583, -0.8053, -0.7403,  0.6601]]) 1\n",
      "tensor([[ 1.9693, -3.1218, -1.8316,  4.9497, -0.6468, -1.3350]]) 1\n",
      "tensor([[-2.8667,  1.9233, -3.1246, -0.8289,  6.7599, -3.9557]]) 0\n",
      "tensor([[-1.3421, -2.7080, -0.5345,  7.3977,  0.1648, -1.0281]]) 1\n",
      "tensor([[-1.9217,  2.4995,  6.8764, -2.3314, -0.9062, -0.3673]]) 1\n",
      "tensor([[-2.7511, -1.8313, -3.6743,  1.9011,  2.2111,  2.1755]]) 0\n",
      "tensor([[-4.0404, -0.9124, -2.8336, -0.0166, -0.7082,  6.4033]]) 1\n",
      "tensor([[-2.0274, -1.9334, -0.9247,  0.6634, -0.3784,  4.3887]]) 1\n",
      "tensor([[ 1.8210,  2.8573,  2.2533, -2.1952, -0.8546, -4.4848]]) 0\n",
      "tensor([[-1.5904, -1.5185, -0.6638,  0.5212, -1.6859,  4.2667]]) 0\n",
      "tensor([[-2.5210,  9.1501,  0.2327, -1.7943, -1.7082, -1.0005]]) 1\n",
      "tensor([[ 0.8817,  3.7032, -0.3193, -1.9622, -1.1276, -2.3871]]) 0\n",
      "tensor([[-1.2067,  2.3370,  7.2541,  1.6086, -4.2499, -1.7211]]) 0\n",
      "tensor([[-2.1385, -2.1739, -0.5181,  5.6449, -0.7556,  1.0012]]) 0\n",
      "tensor([[-2.0596,  1.4076,  7.7032,  0.6898, -2.1729, -0.3604]]) 1\n",
      "tensor([[-2.1364,  4.4791, -1.3177,  1.5495, -1.6317, -0.3913]]) 0\n",
      "tensor([[ 0.4130, -1.8016, -3.5645,  0.6198,  2.0184, -2.5222]]) 0\n",
      "tensor([[-1.2883, -4.6334, -3.2386, 11.7632,  0.3047, -1.8207]]) 1\n",
      "tensor([[-3.3477, -1.2003, -1.7483,  0.7185,  2.7356,  1.0746]]) 1\n",
      "tensor([[ 2.6024,  0.3195, -0.3991, -0.0040, -2.3835, -0.5940]]) 1\n",
      "tensor([[-2.6490, -3.1117, -1.3828,  1.0380, -0.0281,  5.6380]]) 1\n",
      "tensor([[-3.4944, -1.7797, -2.7842,  1.5412,  5.0561, -1.1924]]) 1\n",
      "tensor([[-2.7697, -0.7317, -0.3105, -0.4357,  4.2603, -0.3847]]) 1\n",
      "tensor([[ 0.1051,  4.9986,  0.5976, -1.6610, -1.8680, -3.1117]]) 1\n",
      "tensor([[-1.7850, -1.9430, -4.4501,  1.3841,  7.6508, -3.4914]]) 0\n",
      "tensor([[-1.1555, -2.9305, -2.3436,  1.2540,  5.1202, -2.0008]]) 1\n",
      "tensor([[-2.0246, -2.0136,  0.4589,  4.4428, -1.6972,  2.8166]]) 0\n",
      "tensor([[-1.3965, -0.3352, -1.8253, -0.1987,  4.0306, -1.4368]]) 0\n",
      "tensor([[-1.8944, -1.6207, -3.3173,  0.2737,  2.7315,  0.2767]]) 0\n",
      "tensor([[-1.0894, -0.8240, -2.6663, -1.4784,  2.0803,  1.0794]]) 0\n",
      "tensor([[-1.1301, -1.3894, -1.1804,  0.7304,  1.9397, -0.0528]]) 0\n",
      "tensor([[-2.5520, -0.6435,  3.0421,  1.8970,  0.5664, -0.6469]]) 0\n",
      "tensor([[ 5.9282, -0.6252, -3.2938, -1.3379, -1.1079, -6.5030]]) 1\n",
      "tensor([[ 0.6414, -0.7227, -0.5833,  2.2280,  1.0365, -3.6924]]) 0\n",
      "tensor([[-0.7457,  8.2993,  1.9178, -2.3631, -1.2222, -4.3837]]) 1\n",
      "tensor([[-1.9348,  2.4949,  4.9570, -1.4031, -2.3077,  1.2717]]) 1\n",
      "tensor([[ 4.2388, -0.8690, -2.6777, -0.9219, -0.3461, -3.8947]]) 1\n",
      "tensor([[-0.0617,  1.8768,  7.3421, -2.0303, -0.6968, -2.5738]]) 1\n",
      "tensor([[-1.0677, -1.4184, -1.2492,  6.7055, -0.6897, -0.6640]]) 0\n",
      "tensor([[-1.7342, -0.0082,  3.2725, -0.4515,  0.7630, -1.1996]]) 1\n",
      "tensor([[ 1.1331,  3.2042, -0.5790, -1.9807, -0.7975, -3.1228]]) 0\n",
      "tensor([[-3.0327, -1.7219, -1.8776,  1.0473, -0.3875,  5.6500]]) 0\n",
      "tensor([[-2.8171, -4.6312, -0.6159,  1.7363,  3.9703,  2.5194]]) 0\n",
      "tensor([[ 0.7443,  2.5872,  3.8004, -0.1882, -2.6011, -2.4375]]) 0\n",
      "tensor([[ 1.4791,  5.3784,  0.5938, -0.0185, -1.9528, -4.5514]]) 0\n",
      "tensor([[-1.0931, -0.3198,  2.9065, -0.6920, -0.0381,  0.1297]]) 0\n",
      "tensor([[-3.0658, -0.6976,  0.8038, -0.1449, -1.3696,  5.9333]]) 0\n",
      "tensor([[ 0.9185, -0.4988,  0.2592,  2.1432, -0.7281, -2.9612]]) 1\n",
      "tensor([[-3.5524, -2.1255,  1.1843,  0.4957, -0.9561,  5.8523]]) 0\n",
      "tensor([[-0.3097,  0.0178,  6.7392,  1.1078, -3.0199, -0.2767]]) 1\n",
      "tensor([[-0.9269,  0.4418, -0.6003,  4.5590, -1.6819, -0.8355]]) 0\n",
      "tensor([[-1.1686e+00, -1.9268e+00,  4.4522e-03,  6.8136e+00,  4.6898e-03,\n",
      "         -1.7103e+00]]) 0\n",
      "tensor([[-2.2528, -2.0674, -1.3799,  1.0864, -1.8104,  4.2090]]) 1\n",
      "tensor([[-0.0540,  3.9113,  3.6500, -1.7300, -1.8613, -3.0369]]) 1\n",
      "tensor([[-2.8079, -1.0283,  9.4203,  0.5959, -3.0169,  1.9132]]) 1\n",
      "tensor([[ 0.0309, -1.2966, -0.5187,  2.8430,  0.5457, -2.2293]]) 1\n",
      "tensor([[-1.4664, -1.2196, -1.8270,  5.0270,  1.5930, -1.3818]]) 0\n",
      "tensor([[ 0.3097,  4.1520,  4.7539, -2.2723, -2.5487, -1.7727]]) 0\n",
      "tensor([[-2.5464, -2.5868, -0.5323,  5.7849, -0.6916,  1.2573]]) 1\n",
      "tensor([[ 1.4512,  0.7484,  5.1526, -1.0391, -0.5375, -3.0848]]) 1\n",
      "tensor([[ 3.6156,  0.0949,  2.3773, -2.1549, -0.9981, -3.9607]]) 1\n",
      "tensor([[-1.1101, -3.6722, -3.3340,  9.2241,  0.0653, -0.4866]]) 1\n",
      "tensor([[-1.8860, -1.9932, -3.6269,  1.0364,  6.0747, -3.3359]]) 0\n",
      "tensor([[ 2.0619, -2.8824, -0.4928,  5.2340, -1.6437, -1.9760]]) 0\n",
      "tensor([[-0.0668, -2.1054, -2.3835,  6.6823, -0.3001, -1.5286]]) 1\n",
      "tensor([[-1.0339, -0.5246,  3.6188,  3.9046, -1.4225, -0.8715]]) 0\n",
      "tensor([[-4.2794, -0.9962, -0.1750, -0.4413, -0.1644,  6.8597]]) 0\n",
      "tensor([[ 0.2116,  1.1261, -1.9342, -0.7699, -0.3744, -0.8991]]) 0\n",
      "tensor([[-0.8456, -1.3610, -0.9197, -0.4720,  1.8481,  0.1530]]) 0\n",
      "tensor([[-4.5290, -2.8955, -3.1956,  1.0605,  0.2431,  6.1810]]) 0\n",
      "tensor([[-0.4515,  9.9101, -1.5678, -3.9279, -0.2642, -4.0103]]) 1\n",
      "tensor([[-1.4971,  7.0799, -0.9259, -2.9066,  0.3112, -3.0026]]) 1\n",
      "tensor([[ 2.9409, -1.2544, -1.7254,  4.0685, -1.9493, -3.0805]]) 0\n",
      "tensor([[-0.1592, -1.3101, -1.7841,  6.2588, -1.1409, -0.7261]]) 0\n",
      "tensor([[-0.0216, -0.5600,  2.6706,  4.2697, -1.1902, -2.6384]]) 0\n",
      "tensor([[-3.0491, -4.2011, -2.3871, 10.4664,  0.3937,  1.0808]]) 0\n",
      "tensor([[-1.3831, -4.2827, -3.6619, 10.8970, -1.4617,  0.7513]]) 1\n",
      "tensor([[-2.5244, -2.4955, -1.2535,  5.7288,  0.8016, -0.0107]]) 0\n",
      "tensor([[-0.5056,  0.7274,  6.7522, -0.9966, -1.1593, -1.3218]]) 0\n",
      "tensor([[-1.7094,  1.2031,  8.2234, -1.6545, -1.5633, -0.5821]]) 1\n",
      "tensor([[-0.9081,  1.0938,  7.8874, -1.2544, -1.4757, -1.7071]]) 0\n",
      "tensor([[-2.7042, -0.8214, -1.0695,  2.9422, -0.3270,  1.5476]]) 1\n",
      "tensor([[-4.2393, -2.9252,  0.3159, -0.7936, -1.9747, 10.6427]]) 1\n",
      "tensor([[-3.6749, -1.5479, -0.6449, -1.3006, -1.2809,  8.3587]]) 0\n",
      "tensor([[-1.5353,  1.7757,  5.3096, -0.2027, -1.3428, -0.6168]]) 0\n",
      "tensor([[-1.4966,  2.4788, -1.3162,  1.3722, -0.1869, -0.7849]]) 0\n",
      "tensor([[-1.0836,  0.1347,  6.3296, -0.6409, -0.0492, -0.6884]]) 0\n",
      "tensor([[ 0.5848,  3.9578, -1.4536,  1.5954, -3.0086, -2.1768]]) 0\n",
      "tensor([[-1.0313,  7.3759,  0.0934, -1.1990, -0.4050, -3.4911]]) 0\n",
      "tensor([[ 0.4813, -1.9190,  1.0792,  5.3041, -1.3524, -1.5429]]) 1\n",
      "tensor([[-2.1111, -1.9287, -4.4042,  2.3323,  7.9351, -4.1778]]) 1\n",
      "tensor([[-0.1933, -1.1347,  3.1271,  5.3968, -1.3438, -2.9649]]) 1\n",
      "tensor([[-1.2169,  1.7509,  3.4485, -0.7104, -2.7777,  0.9777]]) 0\n",
      "tensor([[-1.3713, -1.0500,  0.3053,  1.7285, -0.1011,  0.7608]]) 0\n",
      "tensor([[ 0.6190, -0.0126,  0.7314,  0.4610,  0.7231, -2.1838]]) 0\n",
      "tensor([[-1.4215,  1.8842, 11.3837, -2.3430, -1.7881, -1.5776]]) 1\n",
      "tensor([[-0.2532,  2.0477,  9.7783, -2.4254, -2.2416, -2.0179]]) 0\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 1\n",
      "tensor([[-0.9748, -0.3207, -0.4798,  6.3847, -2.1769, -0.1936]]) 0\n",
      "tensor([[-1.3934, -2.0167,  1.0988,  5.9809, -1.5696,  0.2996]]) 1\n",
      "tensor([[-1.9599,  0.1171, -2.9235,  0.2147,  6.4929, -4.0552]]) 1\n",
      "tensor([[ 0.4407,  8.4211, -1.2190, -1.8488, -1.2270, -4.6715]]) 1\n",
      "tensor([[-2.0190,  1.4149,  8.0958,  2.1545, -3.2795, -1.4568]]) 1\n",
      "tensor([[-1.2056, -0.2571,  9.2996,  1.9043, -2.5399, -2.2165]]) 1\n",
      "tensor([[-0.8455, -0.9631,  5.2542, -0.3754, -2.2199,  1.9715]]) 1\n",
      "tensor([[-0.5957,  5.8393,  6.9273, -2.4037, -3.1715, -1.4698]]) 0\n",
      "tensor([[-1.9254,  2.1644,  1.2021,  1.8804, -0.7787, -1.2943]]) 0\n",
      "tensor([[-0.9619,  8.8613, -2.6247, -3.8071, -0.7774, -2.9011]]) 0\n",
      "tensor([[-1.4309,  1.2591, -2.4739,  0.2872,  0.2485,  1.5001]]) 0\n",
      "tensor([[-0.3041,  9.1441,  2.2304, -2.2486, -1.0380, -5.6181]]) 0\n",
      "tensor([[ 1.9688,  0.4272,  1.7365, -0.4241, -2.4002, -0.7695]]) 1\n",
      "tensor([[ 0.1224,  5.8744,  0.3885, -2.7381,  0.3951, -3.5044]]) 1\n",
      "tensor([[-0.9478, -1.2799,  3.5511,  6.4641, -1.6485, -2.5552]]) 1\n",
      "tensor([[-1.1076, -2.8174, -0.9310,  7.8478, -0.6751, -1.0061]]) 1\n",
      "tensor([[-1.8670, -3.6369,  3.6091,  8.7488, -1.6599, -1.4846]]) 0\n",
      "tensor([[-1.1108,  3.3873,  4.4336, -1.3743, -0.9919, -2.0118]]) 0\n",
      "tensor([[-0.4154,  0.4839,  5.9139, -1.3406, -1.4166, -0.5670]]) 1\n",
      "tensor([[-1.3175,  0.4280,  3.1857,  4.4123, -2.3177, -1.1643]]) 1\n",
      "tensor([[-2.4987,  0.5566, -1.8802, -0.7916,  4.3422, -0.6120]]) 1\n",
      "tensor([[ 5.7238, -1.4324, -2.2766,  0.0348, -0.9936, -6.9562]]) 1\n",
      "tensor([[-0.7355,  5.2959,  0.5798, -1.7333, -0.9193, -2.5274]]) 0\n",
      "tensor([[-1.3784,  4.2160, -1.3411, -1.8150, -0.3578, -0.6527]]) 1\n",
      "tensor([[-1.1597, -2.2873, -0.5560,  7.2086, -0.7447, -0.1108]]) 1\n",
      "tensor([[-2.7724, -1.0843, -4.9712,  0.3450,  6.3935, -1.7532]]) 0\n",
      "tensor([[-0.5604, -2.2222, -1.4943,  6.0298, -0.2595, -0.7717]]) 0\n",
      "tensor([[-0.2887,  0.0774,  0.4915,  4.5560, -1.9696, -1.6927]]) 1\n",
      "tensor([[-1.2927,  0.1417, -0.7211, -0.2798, -1.6345,  3.5614]]) 1\n",
      "tensor([[-4.1454, -2.0932,  1.6049, -0.6479,  3.0410,  4.0592]]) 1\n",
      "tensor([[-2.7512, -1.0862,  3.6244, -1.3240,  1.4180,  2.5994]]) 1\n",
      "tensor([[-0.7437,  0.9845,  5.7948,  0.2987, -1.6099, -0.9763]]) 1\n",
      "tensor([[ 0.2046, -2.2822, -1.7904,  7.9621, -1.1313, -3.2004]]) 0\n",
      "tensor([[ 0.1611, -1.9687, -2.7225,  6.8804, -1.0194, -1.3354]]) 0\n",
      "tensor([[-0.5393, -1.4344,  2.6140,  3.7492, -2.5663,  0.0474]]) 0\n",
      "tensor([[-0.3430,  4.1760,  6.1285, -2.0005, -1.7431, -3.4431]]) 1\n",
      "tensor([[-0.1169, -1.4948,  1.0093,  4.5647, -0.7111, -1.8676]]) 0\n",
      "tensor([[-4.2358, -0.6618,  0.6152, -0.7451, -1.0465,  5.7156]]) 0\n",
      "tensor([[-4.9948, -2.2598, -5.7922,  0.7084,  5.2337,  2.8962]]) 0\n",
      "tensor([[ 2.2707,  0.9662,  5.9868,  0.0657, -3.6482, -3.5172]]) 0\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 1\n",
      "tensor([[-0.0894,  3.3736, -1.0448, -2.1530, -1.6353,  1.9044]]) 0\n",
      "tensor([[-2.3975, -1.1267, -1.9538, -0.1474,  6.3848, -1.2955]]) 1\n",
      "tensor([[-2.8567, -3.1413, -1.2216,  0.7176,  4.9345,  0.7986]]) 0\n",
      "tensor([[-1.0371, -2.0321, -2.4422,  1.0734,  4.7474, -1.2582]]) 1\n",
      "tensor([[-0.0976,  2.8232, -0.4321, -1.3068, -1.3504, -1.8388]]) 0\n",
      "tensor([[-2.0378e+00,  1.2712e-03,  6.0831e+00, -3.3160e-01, -1.8528e+00,\n",
      "          1.3658e+00]]) 0\n",
      "tensor([[ 0.9340,  6.5431, -4.1918, -3.2777,  2.5395, -5.3795]]) 1\n",
      "tensor([[ 0.3964,  1.5356,  3.7435,  1.6423, -2.6423, -3.3516]]) 1\n",
      "tensor([[-0.5843, -1.7639,  0.4124,  6.4825, -0.5847, -2.3739]]) 0\n",
      "tensor([[-2.3807, -0.6529, -2.9436,  1.7535, -1.2286,  3.6726]]) 0\n",
      "tensor([[-2.7694, -1.0199, -4.1346,  1.1181,  6.6478, -2.3695]]) 1\n",
      "tensor([[-2.5703, -1.1548, -2.3327,  2.0202,  5.7018, -3.1603]]) 0\n",
      "tensor([[ 0.7928,  7.1036,  0.3679, -0.9658, -0.2319, -6.5638]]) 1\n",
      "tensor([[-0.1248,  5.6032,  5.5227, -0.7188, -2.3930, -5.2812]]) 0\n",
      "tensor([[-1.4866,  3.8468,  1.4917,  0.0267, -2.6643, -0.8581]]) 0\n",
      "tensor([[-1.6835, -1.2968, -1.9385, -0.4036, -1.0279,  5.0625]]) 0\n",
      "tensor([[-2.0584, -1.7959,  4.2495,  4.8243, -0.2229, -0.5845]]) 1\n",
      "tensor([[-3.1759,  0.5469, -2.6168, -1.3421,  7.0760, -3.1101]]) 0\n",
      "tensor([[ 3.7708,  0.3123, -3.8498,  0.5450,  1.1065, -6.4015]]) 0\n",
      "tensor([[-0.8147, -1.6591, -0.8030,  4.5792, -0.1474, -0.8692]]) 1\n",
      "tensor([[-3.1498,  0.6871,  7.0513, -0.4137, -1.1373,  1.0417]]) 0\n",
      "tensor([[-3.4381, -2.2574, -0.0072,  1.0038, -0.5273,  5.4586]]) 0\n",
      "tensor([[-1.6258,  0.6253,  1.0784,  4.2711, -1.9093, -1.0321]]) 1\n",
      "tensor([[-0.9753, -1.4691,  0.4154,  0.6382, -0.6333,  2.1352]]) 0\n",
      "tensor([[-2.0260, -0.1401, -0.7307, -1.3105, -2.1021,  5.7748]]) 1\n",
      "tensor([[-4.4923, -3.2657, -0.3399,  0.9667, -1.1827,  8.1633]]) 1\n",
      "tensor([[-3.0278, -2.8471, -1.7653,  0.8677, -1.4255,  6.3285]]) 0\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 1\n",
      "tensor([[ 0.9671, -1.2165, -1.3454,  4.5047, -0.5582, -1.3546]]) 1\n",
      "tensor([[-2.5558, -0.2294, -1.3050, -0.4805,  5.1247, -1.2476]]) 1\n",
      "tensor([[ 4.4569, -0.1844, -1.3268,  2.4528, -2.8661, -3.9186]]) 0\n",
      "tensor([[-3.0527, -0.6289, -0.7191,  0.7132,  7.0586, -3.6702]]) 0\n",
      "tensor([[ 1.9578,  6.6284,  4.1731, -2.1478, -2.5292, -6.0117]]) 1\n",
      "tensor([[ 3.6168, -1.3542,  2.4844,  1.5055, -0.8168, -6.2655]]) 1\n",
      "tensor([[-1.7631, -0.8598, -0.1478, -1.1522, -3.2618,  5.4186]]) 1\n",
      "tensor([[ 4.0426,  0.4457,  1.3768,  1.0106, -2.7370, -3.7388]]) 0\n",
      "tensor([[ 0.0405,  4.3568, -1.7918, -2.2925,  0.4273, -2.2561]]) 1\n",
      "tensor([[-1.1449, -1.6623,  2.2179,  4.8442, -0.3835, -0.3869]]) 0\n",
      "tensor([[-1.7143,  1.5386,  3.7912,  0.6141, -0.7579,  0.0101]]) 0\n",
      "tensor([[-2.5043, -1.3479,  0.0514,  0.5989, -2.4298,  5.7992]]) 0\n",
      "tensor([[ 4.2502, -3.0498, -0.6979,  7.3114, -3.2492, -2.9201]]) 0\n",
      "tensor([[ 0.7693,  3.6639, -2.7710,  1.5507,  0.2919, -4.2612]]) 0\n",
      "tensor([[-1.0031, -3.8791,  0.8756,  6.4910,  0.2739, -1.1822]]) 0\n",
      "tensor([[ 1.7280,  5.4651, -0.1350, -2.4354, -0.7400, -4.4879]]) 1\n",
      "tensor([[-1.6402,  1.5339,  7.5680,  0.1584, -2.5185, -0.7667]]) 0\n",
      "tensor([[-1.9044, -1.2040,  0.6279,  5.4018, -0.4088, -0.5807]]) 0\n",
      "tensor([[ 1.6898,  6.4197, -2.5136, -2.7077, -1.1692, -4.9288]]) 0\n",
      "tensor([[-0.5669,  6.3712, -0.8338, -2.0677,  0.1657, -3.1798]]) 1\n",
      "tensor([[-1.6190, -0.3310,  7.8005,  0.1931, -1.6184, -0.6724]]) 0\n",
      "tensor([[-0.0872,  2.6467, -1.4198, -1.0069, -0.5355, -0.0785]]) 0\n",
      "tensor([[-0.2134, -0.7831,  6.8842,  1.7352, -1.3324, -2.0527]]) 1\n",
      "tensor([[ 0.8679, -0.2899,  1.7868,  6.1178, -1.2939, -5.0018]]) 0\n",
      "tensor([[ 0.5788, -0.1756,  6.2584, -0.9673, -2.1382, -0.8253]]) 1\n",
      "tensor([[ 5.7665, -2.2838, -3.1814,  2.9060, -0.6225, -5.1590]]) 1\n",
      "tensor([[-1.1560, -0.7953,  2.4683,  6.9693, -1.3667, -3.1221]]) 0\n",
      "tensor([[-4.2137, -1.5683, -1.2522, -1.3215,  8.1208, -1.1899]]) 1\n",
      "tensor([[-1.4966, -1.2087,  0.2828,  0.1991,  2.8499, -1.4699]]) 0\n",
      "tensor([[ 6.1160,  0.1682, -1.5745,  1.0145, -1.5623, -6.6232]]) 1\n",
      "tensor([[ 3.2012, -0.1221, -0.3343,  0.9588, -1.8194, -2.7121]]) 0\n",
      "tensor([[ 3.6375, -0.5986, -2.0459, -0.1724,  0.5856, -3.3728]]) 0\n",
      "tensor([[-0.1076,  7.3447, -1.9765, -2.6798,  0.9895, -4.8182]]) 1\n",
      "tensor([[-1.1545,  0.6793, 10.6093, -0.1000, -3.0742, -0.4575]]) 1\n",
      "tensor([[-1.6864,  0.1701,  4.7009,  3.4588, -1.7018, -1.7133]]) 1\n",
      "tensor([[-0.8383,  2.4359,  3.8046,  1.4827, -2.7952, -0.7353]]) 1\n",
      "tensor([[ 2.4860, -0.7934, -2.9026, -0.0800, -1.1993, -0.5704]]) 0\n",
      "tensor([[ 3.8715, -0.8202, -3.6330, -0.9980, -1.5788, -1.1223]]) 1\n",
      "tensor([[-2.0670, -3.3604,  0.8901,  8.4873, -0.2427, -1.5426]]) 1\n",
      "tensor([[-1.0977, -1.1963,  4.5246,  3.1730, -1.8531, -0.6162]]) 1\n",
      "tensor([[-2.0677, -1.2216, -3.1430, -1.3377,  4.7895,  1.1801]]) 0\n",
      "tensor([[-0.7550,  4.2992,  2.4283, -0.0503, -1.1662, -2.6324]]) 0\n",
      "tensor([[ 0.7091,  3.9442,  0.9629, -2.5955, -1.8717, -1.3074]]) 0\n",
      "tensor([[-0.5909,  4.0668, -0.3998, -3.7106,  2.1954, -3.9378]]) 0\n",
      "tensor([[-0.7402,  2.7120, -4.0450, -0.5223,  3.7254, -4.6167]]) 0\n",
      "tensor([[-0.1608, -1.3634,  4.6782,  3.5799, -2.5834, -2.0383]]) 0\n",
      "tensor([[-0.1063,  2.7282,  5.7064, -0.8905, -2.5524, -1.5676]]) 1\n",
      "tensor([[ 1.7674, -3.1299, -0.2960, -0.8021, -2.6979,  2.6694]]) 1\n",
      "tensor([[-1.0964,  2.5455,  2.5607,  2.5141, -2.5678, -0.8001]]) 0\n",
      "tensor([[-0.2274,  1.5145,  3.6357,  1.5549, -1.4516, -3.5175]]) 1\n",
      "tensor([[-1.7396,  0.5462,  4.8684,  2.3623, -1.7076, -1.4714]]) 0\n",
      "tensor([[-0.1362, -0.3991, -0.0654,  4.9068, -1.8923, -1.7105]]) 1\n",
      "tensor([[ 0.6643,  8.1103, -0.8286, -4.3081, -1.4751, -2.8967]]) 1\n",
      "tensor([[-0.1835,  0.2792,  2.0506,  5.0375, -1.7232, -2.7944]]) 1\n",
      "tensor([[ 0.0854, -1.1601, -1.1435,  4.2467, -1.8676, -0.1801]]) 0\n",
      "tensor([[-2.7225, -0.1741,  5.7844,  1.4773, -1.5721,  0.3906]]) 0\n",
      "tensor([[ 0.2933, -2.1812, -0.0316,  0.7407,  1.4820, -1.0725]]) 1\n",
      "tensor([[-1.7725, -0.0169,  4.7089,  0.3343, -2.5385,  1.9880]]) 1\n",
      "tensor([[-0.9807,  0.3446,  7.8967, -1.1994, -0.8943, -0.6610]]) 1\n",
      "tensor([[-1.7642, -0.9361, -1.6172, -2.9510,  1.3493,  4.2709]]) 1\n",
      "tensor([[ 0.1232, -0.0441,  6.4365, -0.0218, -1.4009, -2.1776]]) 1\n",
      "tensor([[-2.7273, -2.2189, -1.5240,  1.0723, -1.1976,  5.8354]]) 0\n",
      "tensor([[-4.5529, -0.0771, -1.3429, -1.8795,  3.1359,  3.6683]]) 0\n",
      "tensor([[-1.3208, -1.3520, -0.1862,  0.1051, -1.4060,  3.6176]]) 1\n",
      "tensor([[-1.7840, -2.4658, -2.1320, -0.4741,  2.3361,  2.9792]]) 1\n",
      "tensor([[ 3.7611, -1.5470,  0.0567, -0.9630, -1.2496, -1.4607]]) 1\n",
      "tensor([[ 0.9126,  5.4060,  2.5431, -2.3138, -1.9377, -4.3775]]) 0\n",
      "tensor([[-3.3915,  0.2251,  2.8294,  1.9120, -0.7789,  1.7253]]) 0\n",
      "tensor([[ 3.7993,  0.0605, -1.8058,  0.9696, -1.1880, -4.5869]]) 1\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 0\n",
      "tensor([[-0.6299, -0.3252, -1.6945, -1.0934,  5.1106, -2.4780]]) 0\n",
      "tensor([[-1.2213, -1.0647,  1.8973,  4.5234, -2.3315, -0.3977]]) 1\n",
      "tensor([[-2.7471, -0.8576, -0.9721,  4.4876, -1.3442,  2.0073]]) 0\n",
      "tensor([[-0.7099, -4.2544, -2.2921,  9.5588, -0.9253,  0.1310]]) 0\n",
      "tensor([[-2.0006, -0.7311,  2.4014,  2.5486,  0.5647, -0.4410]]) 0\n",
      "tensor([[ 0.5979,  1.5937,  0.6994,  0.4528,  0.0371, -3.8139]]) 0\n",
      "tensor([[-4.6979, -2.4660,  0.0670,  1.8069, -1.0065,  5.3736]]) 1\n",
      "tensor([[ 1.0331, -0.9601, -2.2456,  0.6698, -0.8951,  0.6543]]) 0\n",
      "tensor([[-0.7176,  1.0040,  0.0724,  5.0516, -0.8408, -2.5873]]) 1\n",
      "tensor([[-0.7345, -1.8674, -3.2571, -0.3714, -1.4308,  4.6506]]) 0\n",
      "tensor([[-1.1346, -1.8879,  3.7198,  3.4127, -1.5041,  0.2326]]) 0\n",
      "tensor([[ 0.9351,  6.7511, -2.6705,  0.0576, -0.1416, -4.9606]]) 0\n",
      "tensor([[-1.2998,  9.3523, -1.8422, -2.8782,  2.3157, -5.2711]]) 1\n",
      "tensor([[-0.8627,  2.7733,  1.1718,  0.3589,  0.1387, -1.6092]]) 1\n",
      "tensor([[-0.5470, 11.5536, -0.9302, -4.7679, -1.8629, -3.1184]]) 1\n",
      "tensor([[-0.5286,  8.2465,  2.7670, -2.8262, -1.7426, -4.0630]]) 0\n",
      "tensor([[-3.3900, -3.8093, -3.6362,  2.3198,  5.6336,  0.5702]]) 1\n",
      "tensor([[-0.8387, -3.9377,  1.8226,  0.9314,  0.9287,  0.4789]]) 1\n",
      "tensor([[ 3.5717, -0.9691, -0.7961, -0.9202, -1.6839, -2.0869]]) 1\n",
      "tensor([[-2.2949, -0.1474, -0.8331, -0.8991,  4.4350, -2.2124]]) 0\n",
      "tensor([[-1.8108,  2.4468, -0.3187,  2.2821,  0.2864, -2.4958]]) 0\n",
      "tensor([[-1.6538, -2.6055,  0.5907,  1.6913, -1.5919,  4.0716]]) 1\n",
      "tensor([[-2.2075, -0.8934,  0.7689,  0.8549, -1.0099,  2.7571]]) 1\n",
      "tensor([[-4.2874, -1.7044, -4.2235,  0.2185,  4.1881,  2.4691]]) 1\n",
      "tensor([[-2.4083, -0.3876, -4.2082, -1.4895,  5.3264,  0.6718]]) 0\n",
      "tensor([[-2.5833, -1.6997, -4.6781, -0.6992,  6.9244, -0.6640]]) 0\n",
      "tensor([[-0.6641,  1.2673,  9.6801, -1.2818, -2.4626, -0.7859]]) 1\n",
      "tensor([[-2.4841, -0.6288,  0.7013,  7.1708, -0.8402, -1.6226]]) 0\n",
      "tensor([[ 0.6940,  3.7345, -2.2536, -1.8422, -1.5771, -0.5275]]) 1\n",
      "tensor([[ 3.9345, -0.5155, -2.1600, -0.8863,  0.8449, -5.6421]]) 1\n",
      "tensor([[ 2.5469,  5.8127, -2.6772, -2.4862, -0.1067, -5.3881]]) 0\n",
      "tensor([[ 3.7232, -0.0241, -2.2365, -1.5886, -1.1873, -1.6187]]) 1\n",
      "tensor([[-2.7776, -3.3652, -0.4939,  1.3250, -0.4102,  5.9561]]) 1\n",
      "tensor([[-0.6550,  3.5521,  3.5452,  0.8728, -3.3323, -1.3376]]) 0\n",
      "tensor([[-3.2345, -3.2536,  0.8783,  3.9608, -0.4250,  3.3737]]) 0\n",
      "tensor([[-0.9934, -4.2110,  0.4062,  5.0852, -0.8547,  1.1436]]) 1\n",
      "tensor([[-0.7192, -0.9962,  0.7576,  1.5245, -1.0138,  0.7072]]) 0\n",
      "tensor([[-1.6473, -1.4951, -0.6512,  1.9151,  0.0802,  1.9118]]) 0\n",
      "tensor([[-2.8236,  6.7800,  2.5854, -3.1219,  0.7476, -2.4950]]) 1\n",
      "tensor([[-2.3131, -3.1543, -3.1445,  7.2177,  1.6820, -0.5423]]) 1\n",
      "tensor([[-4.9913, -0.2967, -2.2684, -2.0753,  0.0140,  8.6708]]) 1\n",
      "tensor([[-3.8049, -1.2765, -0.5998, -0.4959, -2.0988,  7.6863]]) 0\n",
      "tensor([[-1.1793, -3.9615, -0.2527,  5.5482, -0.8359,  0.7586]]) 1\n",
      "tensor([[ 0.1351,  0.6333, 10.1186, -0.9152, -3.8427, -1.6142]]) 1\n",
      "tensor([[-0.6098,  0.0416,  9.3617,  1.7022, -2.9848, -2.4355]]) 1\n",
      "tensor([[-1.3991,  0.5208,  6.7502,  0.8597, -1.7091, -0.9143]]) 1\n",
      "tensor([[-0.7945,  1.0334, 10.5691, -1.5791, -1.7214, -1.0545]]) 0\n",
      "tensor([[ 0.5504, -4.7383, -4.8489,  9.8336, -0.8003, -0.8143]]) 0\n",
      "tensor([[-3.6401, -2.1684,  0.9182,  0.5871, -1.3669,  7.3560]]) 1\n",
      "tensor([[-0.3224,  1.4622,  0.3733, -1.3053,  0.3659, -0.6239]]) 0\n",
      "tensor([[ 0.4090,  4.5958, -0.2157, -1.8691, -1.1582, -2.6261]]) 0\n",
      "tensor([[-1.2602,  1.2950,  0.4460,  3.5053, -1.1116, -2.0274]]) 0\n",
      "tensor([[-0.4533,  2.8957, -0.4075,  5.3617, -1.9591, -4.3691]]) 1\n",
      "tensor([[-0.8976,  3.8174, -1.0534,  3.7135, -1.0588, -3.3609]]) 1\n",
      "tensor([[-0.5869,  0.4354,  7.7342, -2.3482, -1.7570, -0.7221]]) 1\n",
      "tensor([[-8.9213e-01, -3.0289e+00, -1.3670e+00,  7.4653e+00, -1.5359e+00,\n",
      "          7.0971e-03]]) 1\n",
      "tensor([[-0.6666, -2.3925, -1.9790,  7.6557, -0.5440, -1.9617]]) 1\n",
      "tensor([[-1.1250, -2.5784, -1.8480,  6.7321, -0.6956,  0.0484]]) 0\n",
      "tensor([[ 0.8792,  6.6901,  0.1973, -1.2889, -0.0797, -6.1640]]) 1\n",
      "tensor([[-0.1581,  2.9769, -4.3628, -2.7781,  4.1314, -5.2509]]) 1\n",
      "tensor([[-0.6309,  5.4673, -2.8825, -4.1253, -0.6876,  0.9721]]) 1\n",
      "tensor([[-1.8544, -2.3868, -4.4219,  1.0853,  6.6290, -1.9259]]) 0\n",
      "tensor([[-1.7542, -2.8161, -4.5327,  0.5534,  8.0568, -3.2577]]) 0\n",
      "tensor([[-1.4978, -4.4378, -6.0842,  4.0163,  7.6694, -3.6091]]) 1\n",
      "tensor([[-0.8599, -0.8951, -0.9586, -1.6179,  3.0571,  0.0108]]) 0\n",
      "tensor([[-1.7009,  2.1223,  4.1976, -0.7891, -1.1562, -0.3329]]) 1\n",
      "tensor([[-2.1845,  0.8990,  2.7142, -1.1646, -0.9587,  2.2961]]) 0\n",
      "tensor([[ 0.5107,  4.7711,  3.6169, -2.2373, -2.5037, -3.0206]]) 0\n",
      "tensor([[-1.7519, -0.9036,  7.3335,  1.4689, -1.9677,  0.5776]]) 0\n",
      "tensor([[-2.0125, -1.0627,  4.1874,  4.6470, -0.9383, -0.8388]]) 0\n",
      "tensor([[ 0.7152, -0.0524, -2.0446, -1.6955,  0.5715, -0.1635]]) 1\n",
      "tensor([[-2.2842, -0.3740, -4.0118,  1.3625,  4.6047, -1.0730]]) 1\n",
      "tensor([[-1.0387, -0.1455, -4.1630,  0.7894,  4.3078, -1.5795]]) 0\n",
      "tensor([[-2.6099, -0.1053,  8.0752,  0.1240, -1.3853,  0.9437]]) 1\n",
      "tensor([[-1.2436,  1.3937, -3.6533, -1.8988,  3.9784, -2.1242]]) 0\n",
      "tensor([[-0.3926, -0.5157,  4.6492,  0.7041, -0.6543, -1.4411]]) 0\n",
      "tensor([[-0.3357,  0.9052, -0.4156,  1.9976, -2.8881,  0.3060]]) 1\n",
      "tensor([[-1.5666,  5.2674, -1.1581, -0.5164,  0.3513, -1.4106]]) 1\n",
      "tensor([[ 0.4502,  6.5681, -2.2997, -0.5265, -0.6839, -3.9168]]) 1\n",
      "tensor([[ 0.2138,  0.1359,  7.9421, -0.2260, -2.6455, -0.8996]]) 1\n",
      "tensor([[ 0.1548,  9.3968, -2.7380, -2.6141, -0.0969, -3.9639]]) 0\n",
      "tensor([[-3.9484, -1.1315, -1.8388, -0.1339,  1.7557,  4.3919]]) 0\n",
      "tensor([[ 1.4272,  7.5793, -3.0787, -2.9173, -0.2446, -4.9997]]) 1\n",
      "tensor([[ 4.5115, -0.0115,  0.7507, -0.2401, -2.3269, -4.6068]]) 1\n",
      "tensor([[ 1.6389,  0.1176,  0.9612, -1.6879, -0.8591, -0.7961]]) 1\n",
      "tensor([[-0.9849, -1.4731,  1.0958,  6.9980, -1.3890, -2.2572]]) 0\n",
      "tensor([[-1.3889, -1.8688,  5.0470,  2.8144, -1.1612, -0.7164]]) 0\n",
      "tensor([[-0.5036, -0.9424,  6.1050, -1.5323, -0.9088,  0.7538]]) 1\n",
      "tensor([[-0.8595,  5.2659, -1.3870, -0.4835, -0.3624, -1.9815]]) 1\n",
      "tensor([[ 1.7158,  7.9137, -1.6425, -1.9902, -1.9917, -4.5385]]) 1\n",
      "tensor([[ 0.3024, -0.2988,  0.5369,  2.3968,  1.0963, -3.0219]]) 0\n",
      "tensor([[-1.4114, -1.6934,  0.8958,  0.2521, -1.0607,  3.9103]]) 1\n",
      "tensor([[-2.8891, -1.8518, -1.5778, -0.4167,  0.3977,  5.3597]]) 1\n",
      "tensor([[-0.8873, -1.3832,  5.1698,  0.4710, -1.5715,  0.2164]]) 0\n",
      "tensor([[-2.0063, -3.3348, -0.7273,  7.9822,  0.2897, -0.9864]]) 1\n",
      "tensor([[ 0.6079,  1.4952,  1.2311, -2.5116, -0.4601, -2.0772]]) 0\n",
      "tensor([[-1.3130, -2.3029, -0.2041,  6.0793, -0.1496, -0.2949]]) 0\n",
      "tensor([[-0.7699,  7.8415, -0.9907, -1.6847, -1.5538, -1.4478]]) 1\n",
      "tensor([[ 0.2235,  0.6219,  3.3100, -1.2595, -0.8835, -1.0666]]) 0\n",
      "tensor([[ 0.0820,  6.4353, -0.4875, -0.6369, -3.3793, -2.1919]]) 1\n",
      "tensor([[-1.0152, -2.8663,  0.9069,  6.8839, -0.7167, -1.2715]]) 1\n",
      "tensor([[-0.7068, -1.6663,  1.3166,  2.2798,  0.0065, -0.0030]]) 0\n",
      "tensor([[-2.6029,  1.2960, -3.6400, -2.5648,  6.0352, -1.1083]]) 0\n",
      "tensor([[-1.7557,  3.0882, -2.1577, -2.2787,  3.1112, -1.0011]]) 0\n",
      "tensor([[-1.7361, -0.7840,  0.9693,  7.1152, -2.5351, -0.2261]]) 1\n",
      "tensor([[-0.7838, -2.2807, -1.1739,  7.0597, -0.1425, -1.4627]]) 1\n",
      "tensor([[ 0.3874,  7.4346,  0.3573, -3.2006, -0.3542, -4.0358]]) 1\n",
      "tensor([[-2.2782, -1.0459, -0.2625,  6.0708, -2.2078,  1.0665]]) 1\n",
      "tensor([[-3.0432, -1.6623, -4.5607,  1.0184,  0.5059,  5.3134]]) 1\n",
      "tensor([[-2.9824, -2.1715, -0.7616,  0.0787,  4.3287,  1.8032]]) 0\n",
      "tensor([[ 4.3194,  0.1581, -1.6766, -0.8511, -4.4694, -0.6616]]) 1\n",
      "tensor([[-1.5897, -1.5438,  1.6777,  2.1271,  0.3469,  0.0841]]) 0\n",
      "tensor([[ 1.0664,  9.2059, -1.2504, -0.9184, -1.1194, -5.8272]]) 0\n",
      "tensor([[-0.7836,  1.5577, -2.8349, -2.4098,  4.8855, -3.4778]]) 1\n",
      "tensor([[ 4.8983e-03,  7.9140e+00, -1.3235e+00, -3.7780e+00, -1.2090e-01,\n",
      "         -3.3773e+00]]) 1\n",
      "tensor([[-2.9681, -1.0038, -5.1302, -1.0620,  9.5309, -3.2194]]) 1\n",
      "tensor([[-0.8800,  3.1902,  0.8520, -0.2202, -0.9462, -0.4591]]) 0\n",
      "tensor([[-1.6632, -1.8052, -0.7496,  0.1466, -0.8746,  4.3357]]) 1\n",
      "tensor([[ 1.1242, -0.9770, -0.0684,  5.1694, -2.0610, -1.5480]]) 0\n",
      "tensor([[ 0.0264,  1.5544,  4.2567,  0.5260, -3.6876, -0.0460]]) 1\n",
      "tensor([[-3.6103, -2.2729, -0.7807, -0.8617, -0.3927,  7.1985]]) 1\n",
      "tensor([[-2.6563, -1.2112, -0.2602, -0.3572,  3.0405,  0.9682]]) 1\n",
      "tensor([[ 0.9585,  0.8155,  4.6668,  0.0506, -2.2952, -1.2533]]) 1\n",
      "tensor([[-1.2802, -1.0674,  4.2523,  0.2877, -0.2757,  0.0661]]) 0\n",
      "tensor([[ 0.1811, -0.6065,  4.3157,  4.5933, -3.5099, -2.0221]]) 0\n",
      "tensor([[-2.1422, -0.8386,  3.3154,  4.9157, -1.7176, -0.2101]]) 1\n",
      "tensor([[-1.2865, -3.5853, -4.3978,  8.3892,  1.1361, -0.3651]]) 1\n",
      "tensor([[-4.6134, -1.2913, -0.8195, -0.4645, -1.9778,  8.7279]]) 0\n",
      "tensor([[-3.5598, -2.1620, -0.9516,  0.5181, -1.3285,  7.1489]]) 1\n",
      "tensor([[-2.1110, -0.0800,  1.8386,  0.9321, -3.4557,  4.0323]]) 0\n",
      "tensor([[-2.3591, -1.4627, -0.5973, -1.3136,  1.0638,  3.5830]]) 0\n",
      "tensor([[-0.2462,  3.0873, -3.5135, -1.7348, -1.6911,  1.4760]]) 1\n",
      "tensor([[-1.7395,  0.9976, -4.2153, -2.4538,  7.2170, -4.5510]]) 1\n",
      "tensor([[ 0.6652, -2.5784, -3.5285,  6.4659,  0.3503, -2.8793]]) 0\n",
      "tensor([[-1.7627, -1.1516, -3.5254, -0.6052,  4.7054, -1.0426]]) 0\n",
      "tensor([[-2.5982, -1.9990, -2.2578,  0.1149,  4.5378,  1.1724]]) 1\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 0\n",
      "tensor([[ 3.8920,  3.0865, -2.1258, -2.7601, -1.4849, -4.7666]]) 0\n",
      "tensor([[ 4.7587,  0.9864, -1.9799, -2.5454, -3.6009, -3.2008]]) 1\n",
      "tensor([[ 2.5257, -0.9151, -2.0893,  3.4722, -2.0782, -2.3810]]) 0\n",
      "tensor([[-1.2101, -2.4128,  4.3647,  0.5414, -2.0691,  3.3114]]) 1\n",
      "tensor([[-0.4940, -1.3977, -1.6258, -0.4755,  3.1039, -0.7773]]) 0\n",
      "tensor([[-1.7018,  3.6217,  7.0634, -1.0980, -2.0390, -1.6453]]) 0\n",
      "tensor([[-0.5888,  7.4903,  1.5827, -1.6247, -0.1858, -4.9334]]) 0\n",
      "tensor([[ 0.9702,  7.9761, -2.6561, -1.5034, -1.6124, -3.4592]]) 0\n",
      "tensor([[-2.4496, -0.8519, -0.3322, -3.6212,  6.2262, -1.1164]]) 0\n",
      "tensor([[-2.3919, -1.2509, -0.7239,  0.1121,  0.4217,  3.1441]]) 1\n",
      "tensor([[-1.8012,  3.3563, -2.1401, -0.2705,  1.9457, -1.9920]]) 0\n",
      "tensor([[-3.7433,  0.3231, -1.7182, -0.4970,  1.3590,  3.8464]]) 0\n",
      "tensor([[-2.2965, -1.6113, -1.1321, -0.9817,  1.1333,  3.7726]]) 1\n",
      "tensor([[-1.6912,  1.3343,  1.7095,  1.7851, -1.3806,  0.0906]]) 0\n",
      "tensor([[-0.3175,  3.6883,  0.0899, -0.9759, -0.3778, -1.5711]]) 0\n",
      "tensor([[ 3.8938, -0.9887, -1.8348,  0.5168, -2.2104, -2.8308]]) 0\n",
      "tensor([[-3.0253, -0.8014, -2.7846,  1.9401,  2.6358,  1.3114]]) 0\n",
      "tensor([[ 0.3403,  2.6185,  0.9494, -1.6674, -1.2883, -0.7880]]) 0\n",
      "tensor([[ 0.0195,  2.1361, -0.2935,  3.0130, -0.7536, -2.7981]]) 0\n",
      "tensor([[-2.8472,  0.8957, -0.2555, -1.0598, -0.8032,  4.0381]]) 1\n",
      "tensor([[-1.9287, -1.5176,  2.1883,  0.4025,  0.8236,  1.3443]]) 0\n",
      "tensor([[-0.6656,  0.6181, -4.8215, -0.5075,  4.6955, -2.4997]]) 0\n",
      "tensor([[ 0.3846,  0.9584, -2.3811, -0.9394,  2.8822, -2.7497]]) 0\n",
      "tensor([[-2.5212,  1.1180, -2.9207, -0.9989,  3.8053, -0.2398]]) 1\n",
      "tensor([[-1.5316,  1.3065,  3.7503,  0.6006, -1.2313, -0.8366]]) 0\n",
      "tensor([[-3.1836, -1.4607, -2.2018,  1.0630, -0.3924,  4.7080]]) 0\n",
      "tensor([[-0.2456,  6.7302, -1.3289, -2.1834, -1.5073, -1.6504]]) 1\n",
      "tensor([[-2.8446, -2.9629, -3.8761,  2.8384,  0.3055,  4.3264]]) 1\n",
      "tensor([[-0.0660, -2.2079, -0.8460,  6.5175,  0.2716, -2.6072]]) 0\n",
      "tensor([[-2.0934, -3.0856, -4.8548,  2.4309,  7.5120, -2.5679]]) 0\n",
      "tensor([[-1.4978,  0.2329,  5.4034, -0.4265, -2.8418,  2.0898]]) 1\n",
      "tensor([[-1.6278,  2.5072, -1.8316, -1.4648,  2.7460, -1.1874]]) 0\n",
      "tensor([[-1.4958, -0.1832, 12.2397, -1.6911, -2.9095, -0.3985]]) 1\n",
      "tensor([[-3.7116, -2.9517, -1.8902,  1.5349, -0.2392,  6.5925]]) 1\n",
      "tensor([[-2.3372, -2.4091, -1.0411,  3.3139, -0.3098,  3.5863]]) 1\n",
      "tensor([[-1.7348, -2.3367, -1.5716,  5.2566,  0.3296,  0.2899]]) 0\n",
      "tensor([[-0.4813, -2.1097, -2.1010,  6.1781, -1.2167,  0.3441]]) 1\n",
      "tensor([[-2.4559, -2.2653, -1.6029, -0.2962, -1.1349,  6.9391]]) 1\n",
      "tensor([[-2.2050, -3.4637,  0.8577,  9.1906, -0.6689,  0.1430]]) 0\n",
      "tensor([[-2.8582, -1.1578, -6.6824,  0.4682, 10.5397, -6.8723]]) 0\n",
      "tensor([[-1.5348,  0.0621, -2.7647, -1.3630,  6.2281, -3.0288]]) 0\n",
      "tensor([[-4.2243, -0.4303, -1.3196, -0.3618,  3.2700,  2.8216]]) 0\n",
      "tensor([[-0.8016, -0.2885, -1.8702, -0.0218,  2.8981, -1.1413]]) 0\n",
      "tensor([[ 0.4044,  0.2803, -1.2530, -1.0379, -0.3999,  0.7846]]) 1\n",
      "tensor([[-1.7770, -1.8433,  0.0736, -0.4449, -0.6577,  4.5594]]) 1\n",
      "tensor([[-1.0746,  2.7699, -1.4681, -0.4327,  0.2637, -1.4598]]) 0\n",
      "tensor([[ 0.5318,  7.1971, -0.6231, -1.4108, -0.3888, -4.8715]]) 0\n",
      "tensor([[-1.3858, -0.3925, -0.1638,  1.9723, -0.5287,  0.1342]]) 0\n",
      "tensor([[-0.8936, -3.1295,  0.9304,  7.1422, -0.6763, -0.5096]]) 1\n",
      "tensor([[-4.0921, -2.8115, -2.7808,  1.2514,  2.0632,  4.1703]]) 0\n",
      "tensor([[-0.9719,  3.2800, -0.2979,  1.1731, -1.3301, -0.6518]]) 0\n",
      "tensor([[-1.4433, -2.9223, -2.3202,  1.8285,  0.1911,  2.9755]]) 0\n",
      "tensor([[-0.5580, -0.1560,  4.5707,  1.0613, -1.1636, -0.9471]]) 1\n",
      "tensor([[-0.8639,  5.3205, -0.7394, -1.9004, -0.9385, -1.3642]]) 1\n",
      "tensor([[-1.4272e+00, -1.1974e+00, -3.7486e+00, -3.6862e-03,  5.6046e+00,\n",
      "         -2.0452e+00]]) 1\n",
      "tensor([[ 0.9320,  4.5529,  0.3472,  0.0716, -0.8012, -4.2335]]) 0\n",
      "tensor([[ 0.9284,  5.6715, -0.7043, -3.6940, -0.7658, -2.5935]]) 1\n",
      "tensor([[ 1.1084,  6.9137, -1.5009, -2.2801, -0.9284, -3.3619]]) 1\n",
      "tensor([[-1.4631, -0.2158, -0.9108,  1.7260, -1.1218,  1.4939]]) 0\n",
      "tensor([[ 3.6522,  0.2685, -1.7075, -0.1797, -1.7165, -2.6320]]) 1\n",
      "tensor([[-0.8811,  6.0222, -2.8097, -1.2228, -0.2834, -1.9483]]) 0\n",
      "tensor([[-3.7387, -1.1564, -0.3242,  0.5280, -1.4530,  6.1538]]) 1\n",
      "tensor([[-1.5975,  0.5711,  2.0755,  2.1055, -1.3618,  0.2473]]) 0\n",
      "tensor([[ 1.3860,  2.1812, -0.7855, -2.8087,  0.8950, -2.6847]]) 0\n",
      "tensor([[-0.1558, -0.6167, -2.4139,  1.7540, -1.8461,  1.7140]]) 0\n",
      "tensor([[-1.1349,  1.0079,  0.3630,  0.7453, -0.2887,  0.4276]]) 0\n",
      "tensor([[-0.8584,  3.9852, -1.3401, -1.3120, -2.2706,  0.8622]]) 0\n",
      "tensor([[-0.9590, -2.7824, -0.2628,  5.3145, -0.2930, -0.2223]]) 0\n",
      "tensor([[-2.2751, -0.7555, -0.8204,  5.1146, -0.1102,  0.2098]]) 0\n",
      "tensor([[-1.1863,  3.3880,  3.7782,  0.6799, -1.3580, -2.1981]]) 0\n",
      "tensor([[-1.0028, -3.5503, -1.8645,  6.9470, -0.6642,  0.5789]]) 1\n",
      "tensor([[-2.6821, -1.5361,  3.2229, -0.1196, -0.5380,  4.0822]]) 1\n",
      "tensor([[-3.9998, -2.1820, -0.4089, -0.6765, -1.1206,  7.8217]]) 1\n",
      "tensor([[-3.8167,  0.1017, -2.9377, -0.4572, -1.8854,  7.5548]]) 0\n",
      "tensor([[-1.4772, -2.0592, -2.0734,  0.7649, -0.7048,  4.2334]]) 0\n",
      "tensor([[-2.4002, -1.5650,  0.0079,  6.5709, -1.5167,  0.3853]]) 0\n",
      "tensor([[ 0.1089, -1.8823,  0.5966,  6.4297, -1.4406, -1.6287]]) 1\n",
      "tensor([[-1.8378, -1.1320,  0.6878, -0.8009, -1.6359,  5.6506]]) 0\n",
      "tensor([[ 0.4060, -0.3625,  5.3993,  1.2931, -1.5805, -2.2764]]) 0\n",
      "tensor([[-1.9096, -0.1052,  3.3635,  2.6269, -0.4420, -0.4904]]) 0\n",
      "tensor([[-1.3507, -1.1345, -0.0532,  6.2289, -1.3585, -0.9968]]) 1\n",
      "tensor([[-2.3273, -0.6596, -4.2030, -1.1527,  6.9328, -3.2551]]) 1\n",
      "tensor([[-2.8019, -2.3337, -2.0562,  5.4793,  0.9160,  0.8365]]) 1\n",
      "tensor([[-3.1385, -1.6008, -2.6615, -2.1305,  6.7312,  0.0949]]) 0\n",
      "tensor([[-0.9869,  0.3123, -2.3855,  0.1840, -0.9521,  0.9809]]) 0\n",
      "tensor([[ 4.9556, -0.2539, -3.4350, -0.5233,  1.0528, -5.6660]]) 0\n",
      "tensor([[ 3.8951, -1.6360, -0.9192,  0.3103, -1.1093, -3.6187]]) 1\n",
      "tensor([[-0.3802,  7.1305, -2.6743, -2.5593,  0.1683, -3.1725]]) 1\n",
      "tensor([[-0.0962, -2.9509, -0.7632,  7.8589, -1.4396, -1.2025]]) 1\n",
      "tensor([[-2.7490, -0.3805, -3.0068, -1.3652,  6.0017, -0.7835]]) 1\n",
      "tensor([[-2.3010,  0.3212,  0.5115, -1.7591, -3.0768,  7.0984]]) 1\n",
      "tensor([[ 0.8651,  0.0374,  2.3765, -1.1338,  0.7630, -3.0168]]) 1\n",
      "tensor([[ 1.7418,  5.1643, -3.0371, -0.5591, -3.3587, -1.7913]]) 1\n",
      "tensor([[-2.3208,  7.3608, -1.1839, -1.4775,  0.3744, -2.4482]]) 1\n",
      "tensor([[-1.5376,  1.0358, -2.8255, -1.7985,  6.0570, -3.0218]]) 1\n",
      "tensor([[-1.3266, -0.8123,  6.2230,  1.2623, -2.3167,  0.6522]]) 0\n",
      "tensor([[-0.3895,  1.9668,  0.6123, -0.4194, -1.4235, -0.8334]]) 0\n",
      "tensor([[-3.9982e+00, -3.5250e+00, -2.4864e+00,  2.8540e+00, -1.0543e-03,\n",
      "          6.0274e+00]]) 1\n",
      "tensor([[-3.6438, -2.7698, -2.1450,  0.5896, -0.0135,  6.8671]]) 0\n",
      "tensor([[-2.1545, -1.3688, -3.8478,  0.6197,  5.2999, -1.9360]]) 1\n",
      "tensor([[ 3.0441, -1.5026, -3.1853,  0.9240,  0.5437, -3.2479]]) 1\n",
      "tensor([[-0.9690, -2.0331, -1.9604,  6.7555, -1.2734, -0.4525]]) 0\n",
      "tensor([[ 4.4649, -0.2049, -1.7826, -0.1905, -2.1304, -3.4659]]) 1\n",
      "tensor([[ 0.4948,  0.5548,  1.6309,  2.7962, -2.0126, -2.1800]]) 1\n",
      "tensor([[-1.3441, -1.7076, -3.8849,  0.1332,  3.5985,  0.0935]]) 0\n",
      "tensor([[ 0.6187, -2.0908, -1.3380,  5.9507, -0.8835, -1.7577]]) 1\n",
      "tensor([[-2.1667, -3.0650,  3.6556,  6.0314, -0.7729, -0.5936]]) 0\n",
      "tensor([[-1.0894, -1.3911,  1.1994,  7.5191, -1.3861, -2.2877]]) 1\n",
      "tensor([[-1.7614, -2.3300, -0.2103,  9.0982, -0.5099, -1.3235]]) 1\n",
      "tensor([[-1.6121, -2.4944,  2.0601,  7.6120, -1.5855, -0.6656]]) 0\n",
      "tensor([[-3.5040,  0.6532, -0.5010, -0.5256, -0.8182,  4.3636]]) 0\n",
      "tensor([[-0.9305, -1.3077,  0.4077,  6.4115, -1.8142, -0.1892]]) 0\n",
      "tensor([[ 2.9007,  1.3847, -1.5538, -0.9839, -2.4078, -2.9738]]) 0\n",
      "tensor([[ 3.1533,  0.4936, -1.1615, -1.3080, -0.4096, -3.2546]]) 1\n",
      "tensor([[-2.1288, -2.8275, -2.2902,  6.1780,  0.4092,  0.2249]]) 0\n",
      "tensor([[ 1.6991,  2.0905,  0.6200, -1.4095, -0.8637, -2.2888]]) 0\n",
      "tensor([[-2.9294, -2.2325, -1.4839,  1.4332, -0.9372,  5.8956]]) 1\n",
      "tensor([[-0.8343, -3.1394, -1.9323,  9.7495, -1.1903, -0.4984]]) 1\n",
      "tensor([[-0.4583, -2.1994, -0.9115,  6.3637, -0.5606, -1.3541]]) 0\n",
      "tensor([[-0.6909,  0.1162, -1.5454,  2.6613,  1.7524, -2.5203]]) 0\n",
      "tensor([[-3.7297,  3.2901, -2.1498, -3.4803, -1.1159,  5.9514]]) 1\n",
      "tensor([[-1.3869, -0.7335, -0.9201,  1.0786, -1.4904,  2.7734]]) 1\n",
      "tensor([[-0.4789,  0.2404,  0.0613, -0.2792, -1.8074,  2.1263]]) 0\n",
      "tensor([[-0.7014,  6.3306, -0.1175, -2.7726, -2.6754,  0.6697]]) 1\n",
      "tensor([[-1.4145,  3.1722,  0.2545,  0.0204,  0.1507, -1.8050]]) 0\n",
      "tensor([[-2.9587, -3.2763,  1.9136,  1.0322,  1.8711,  0.6419]]) 0\n",
      "tensor([[-1.9367, -0.5690,  0.7659, -0.6787,  1.0977,  1.6063]]) 0\n",
      "tensor([[-1.3904, -1.0701,  0.7976,  3.0376, -1.0073,  0.7729]]) 0\n",
      "tensor([[-1.2694,  0.6002,  0.8349, -0.3027, -0.4133,  1.1049]]) 0\n",
      "tensor([[-3.1176,  0.7238, -3.6581, -1.6758,  7.2481, -2.5683]]) 1\n",
      "tensor([[ 0.3128,  6.1359,  0.5887, -1.5952, -1.4059, -3.4981]]) 0\n",
      "tensor([[-1.3985, -0.0156, -1.9711, -1.3340, -0.7763,  4.0674]]) 0\n",
      "tensor([[-0.9199, -1.5382, -1.9883,  1.4579,  2.0357, -0.3060]]) 0\n",
      "tensor([[-0.2678,  5.6550, -1.2121,  0.2990, -1.2218, -3.3323]]) 0\n",
      "tensor([[-1.0964,  2.7815,  0.6299, -0.4806,  2.8113, -4.2094]]) 1\n",
      "tensor([[-4.4424, -1.4526, -2.4309,  0.3899, -1.9920,  8.9152]]) 0\n",
      "tensor([[-4.1599, -1.1320, -6.1621,  0.4056,  5.9905,  0.5965]]) 1\n",
      "tensor([[-2.5613,  0.3190, -2.9293, -1.0326,  4.0655, -0.9924]]) 0\n",
      "tensor([[-4.0808, -0.8627, -1.8553,  1.1960, -1.5848,  6.4721]]) 0\n",
      "tensor([[-1.9195, -2.5701,  0.0928,  5.8296,  0.3170, -0.3225]]) 0\n",
      "tensor([[ 5.1443,  2.4810,  0.0938, -1.3791, -2.3681, -5.3053]]) 1\n",
      "tensor([[-2.0218,  0.5534, -1.5468, -2.3172,  4.0008, -0.7559]]) 1\n",
      "tensor([[-1.4108, -1.6425, -0.3219,  5.8512, -1.7030,  0.3985]]) 0\n",
      "tensor([[-1.4713, -1.3047,  0.6450,  6.6663, -1.5098, -0.6284]]) 0\n",
      "tensor([[-0.8269,  0.9937, -3.1503, -0.6525,  6.2448, -4.8528]]) 0\n",
      "tensor([[ 1.3280,  3.2722, -1.3981,  0.6705, -1.5078, -3.0649]]) 0\n",
      "tensor([[-1.1040, -0.1128, -0.6106, -1.2503,  3.8070, -0.9429]]) 1\n",
      "tensor([[-0.5480,  3.9816, -1.9011, -0.8744, -0.7393, -1.0337]]) 1\n",
      "tensor([[-4.2464, -1.7030, -1.1015,  1.1153, -1.5420,  7.2389]]) 0\n",
      "tensor([[-3.2053, -1.9747, -7.3165,  1.7408, 10.5343, -4.1675]]) 1\n",
      "tensor([[ 0.2977,  3.1990, -0.6185,  1.2118, -1.1250, -2.7614]]) 1\n",
      "tensor([[-0.4343, -0.1537, -2.3774, -2.4457,  4.1225, -2.3921]]) 0\n",
      "tensor([[-2.0133, -2.2305,  0.7052,  5.0266, -1.1328,  1.2115]]) 1\n",
      "tensor([[-0.8743,  2.7998,  2.5990,  1.9740, -1.1088, -2.3998]]) 0\n",
      "tensor([[-1.2187,  2.5756,  2.6231,  0.9405, -0.9049, -2.2258]]) 1\n",
      "tensor([[-2.1356, -0.4090,  4.5453, -0.9831, -1.0766,  2.6446]]) 1\n",
      "tensor([[-1.8370, -1.3099, -2.9502,  1.4359, -1.8454,  5.5165]]) 1\n",
      "tensor([[-1.7187, -3.7818, -4.3135,  6.1189,  0.7538,  2.6067]]) 1\n",
      "tensor([[-3.1181, -1.6101,  0.1884,  2.9103, -1.2216,  3.5656]]) 0\n",
      "tensor([[ 0.4548,  7.1520, -1.0302, -1.7970, -1.5716, -3.3950]]) 1\n",
      "tensor([[-1.9412, -2.9229,  0.2288,  8.8620, -1.2252, -0.3133]]) 0\n",
      "tensor([[-3.8515, -1.3438, -0.7702,  0.4295, -2.7290,  7.6375]]) 0\n",
      "tensor([[ 2.7753,  2.3633, -4.9217, -2.5787,  2.0851, -6.9964]]) 0\n",
      "tensor([[-0.3277, -0.7992,  1.0629,  2.6721, -1.4344, -0.2987]]) 0\n",
      "tensor([[ 0.1673, -2.6455, -2.0620,  0.7050,  3.6059, -2.4534]]) 0\n",
      "tensor([[-1.6231,  3.8748, -1.4937, -1.2815,  1.6384, -1.5079]]) 0\n",
      "tensor([[-4.1443, -0.5622, -1.2057, -0.4464,  5.1295,  1.0759]]) 1\n",
      "tensor([[-2.7907, -4.0257, -1.5137,  1.9020, -1.8365,  7.6219]]) 0\n",
      "tensor([[-3.1511, -0.2004, -0.7830, -0.4405, -1.0217,  5.5746]]) 1\n",
      "tensor([[-4.3851, -0.6373, -1.2615, -0.2938,  6.8926, -1.3352]]) 0\n",
      "tensor([[ 0.1891, -0.0236, -4.1946,  0.8398,  5.1569, -4.1061]]) 0\n",
      "tensor([[-1.8203, -1.3326, -0.6326,  5.8936, -2.1303,  0.3918]]) 1\n",
      "tensor([[-1.8470,  0.5550, -1.3147, -1.0268, -2.2972,  3.8623]]) 1\n",
      "tensor([[-0.8830, -2.9602, -0.5230,  5.0459,  0.1041,  0.1820]]) 1\n",
      "tensor([[-2.7945, -0.8833, -2.2050,  0.8514,  0.0327,  3.8380]]) 1\n",
      "tensor([[-1.1898,  0.9952, -2.4540,  1.1006,  4.2284, -2.6412]]) 1\n",
      "tensor([[ 3.1716,  6.6374, -1.6371, -1.7851, -3.5004, -5.3994]]) 0\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 1\n",
      "tensor([[ 3.0611, -0.5660, -2.3026, -0.2224, -0.4651, -3.5559]]) 1\n",
      "tensor([[-0.3901,  8.7073, -1.4132, -0.9574, -0.3814, -5.1740]]) 0\n",
      "tensor([[-2.7785,  0.3998, -2.9619, -1.4657,  4.6133, -1.6156]]) 1\n",
      "tensor([[-2.3620, -0.2164,  0.2341,  5.1058, -2.6165,  1.0284]]) 1\n",
      "tensor([[-3.2173,  0.0810, -1.7758, -1.4019,  6.9216, -1.8260]]) 0\n",
      "tensor([[-3.2366, -2.4511, -2.9732,  1.9990,  4.6278, -0.0340]]) 0\n",
      "tensor([[-0.0844,  6.7220, -0.8004, -1.9489, -0.6472, -3.5266]]) 1\n",
      "tensor([[-0.0089,  7.1527, -1.8197, -2.7800,  1.0487, -5.4204]]) 1\n",
      "tensor([[-0.9670, -0.5885,  1.0500,  5.8289, -2.4299, -0.8828]]) 1\n",
      "tensor([[-2.2293,  0.8075, -2.2556, -0.4587,  0.3006,  2.6070]]) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(t_data.num_nodes):\n",
    "    \n",
    "    if t_data.train_mask[i]:\n",
    "        pred = query_zero_hop(t_model, t_data.x[i]).detach().cpu()\n",
    "        print(pred, 1)\n",
    "    elif t_data.test_mask[i]:\n",
    "        pred = query_zero_hop(t_model, t_data.x[i]).detach().cpu()\n",
    "        print(pred, 0)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b3e1cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327, 6], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef2794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
