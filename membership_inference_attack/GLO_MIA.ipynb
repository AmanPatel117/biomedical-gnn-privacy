{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370682f1",
   "metadata": {},
   "source": [
    "# Graph-level label only membership inference attack (GLO-MIA).\n",
    "As described here: https://arxiv.org/pdf/2503.19070. This attack method is suitable for multi-graph datasets, and assumes the strictest black box scenario in which the attacker has no access to model architecture, and queries only return labels insteasd of logits/probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17ef12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71a2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.graphproppred import PygGraphPropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2bb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from torch import nn, optim\n",
    "from torch_geometric import nn as gnn, transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader as GDataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from ml_util import GATProteinsModel, GATMolhivModel, train_model_multi_graph, load_model\n",
    "from util import onehot_transform, graph_train_test_split, calculate_robustness_scores, _get_onehot_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d6a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = ('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f413967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de36c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories = 6\n",
    "num_feat = 3\n",
    "\n",
    "# dataset = TUDataset(root='/home/hice1/khom9/scratch/CSE-8803-MLG-Data/PROTEINS', name='PROTEINS', transform=_get_onehot_transform(n_categories))\n",
    "dataset = TUDataset(root='/home/hice1/khom9/scratch/CSE-8803-MLG-Data/ENZYMES', name='ENZYMES', transform=_get_onehot_transform(n_categories))\n",
    "# dataset = TUDataset(root='/home/hice1/khom9/scratch/CSE-8803-MLG-Data/DD', name='DD', transform=_get_onehot_transform(n_categories))\n",
    "# dataset = TUDataset(root='/home/hice1/khom9/scratch/CSE-8803-MLG-Data/MUTAG', name='MUTAG', transform=_get_onehot_transform(n_categories))\n",
    "# dataset = PygGraphPropPredDataset(root='/home/hice1/khom9/scratch/CSE-8803-MLG-Data/ogb-molhiv', name='ogbg-molhiv', transform=_ogb_molhiv_transform) \n",
    "dataset = onehot_transform(dataset, categories=list(range(n_categories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e182b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset inhalf for the target model dataset and shadow model dataset\n",
    "t_dataset, s_dataset = graph_train_test_split(dataset, test_size=0.5)\n",
    "\n",
    "# Split each dataset into train/test splits\n",
    "t_dataset_train, t_dataset_test = graph_train_test_split(t_dataset, test_size=0.5)\n",
    "s_dataset_train, s_dataset_test = graph_train_test_split(s_dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5019352c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "Scheduler: <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x155202159150>\n",
      "Training for 100 epochs, with batch size=8\n",
      "Using validation data (150 samples)\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/100-----\n",
      "Batch 10/19 | loss: 1.91568 (0.126s) | train acc: 0.125, train AUC: 0.438\n",
      "Batch 19/19 | loss: 1.83647 (0.111s) | train acc: 0.153, train AUC: 0.483\n",
      "Validation: val loss: 1.823 | val acc: 0.187 | val F1: 0.125 | val AUC: 0.556\n",
      "\n",
      "-----Epoch 2/100-----\n",
      "Batch 10/19 | loss: 1.78539 (0.118s) | train acc: 0.263, train AUC: 0.555\n",
      "Batch 19/19 | loss: 1.81746 (0.105s) | train acc: 0.240, train AUC: 0.551\n",
      "Validation: val loss: 1.826 | val acc: 0.193 | val F1: 0.182 | val AUC: 0.517\n",
      "\n",
      "-----Epoch 3/100-----\n",
      "Batch 10/19 | loss: 1.62947 (0.118s) | train acc: 0.338, train AUC: 0.691\n",
      "Batch 19/19 | loss: 1.77865 (0.105s) | train acc: 0.300, train AUC: 0.655\n",
      "Validation: val loss: 1.818 | val acc: 0.200 | val F1: 0.158 | val AUC: 0.556\n",
      "\n",
      "-----Epoch 4/100-----\n",
      "Batch 10/19 | loss: 1.68153 (0.118s) | train acc: 0.338, train AUC: 0.681\n",
      "Batch 19/19 | loss: 1.66816 (0.105s) | train acc: 0.327, train AUC: 0.653\n",
      "Validation: val loss: 1.778 | val acc: 0.280 | val F1: 0.229 | val AUC: 0.597\n",
      "\n",
      "-----Epoch 5/100-----\n",
      "Batch 10/19 | loss: 1.63745 (0.118s) | train acc: 0.350, train AUC: 0.703\n",
      "Batch 19/19 | loss: 1.64322 (0.105s) | train acc: 0.347, train AUC: 0.693\n",
      "Validation: val loss: 1.754 | val acc: 0.320 | val F1: 0.287 | val AUC: 0.620\n",
      "\n",
      "-----Epoch 6/100-----\n",
      "Batch 10/19 | loss: 1.56748 (0.118s) | train acc: 0.450, train AUC: 0.732\n",
      "Batch 19/19 | loss: 1.67080 (0.105s) | train acc: 0.380, train AUC: 0.683\n",
      "Validation: val loss: 1.752 | val acc: 0.273 | val F1: 0.260 | val AUC: 0.626\n",
      "\n",
      "-----Epoch 7/100-----\n",
      "Batch 10/19 | loss: 1.55134 (0.118s) | train acc: 0.362, train AUC: 0.744\n",
      "Batch 19/19 | loss: 1.63563 (0.105s) | train acc: 0.320, train AUC: 0.714\n",
      "Validation: val loss: 1.779 | val acc: 0.320 | val F1: 0.304 | val AUC: 0.628\n",
      "\n",
      "-----Epoch 8/100-----\n",
      "Batch 10/19 | loss: 1.59080 (0.119s) | train acc: 0.338, train AUC: 0.693\n",
      "Batch 19/19 | loss: 1.59479 (0.105s) | train acc: 0.360, train AUC: 0.705\n",
      "Validation: val loss: 1.756 | val acc: 0.300 | val F1: 0.270 | val AUC: 0.636\n",
      "\n",
      "-----Epoch 9/100-----\n",
      "Batch 10/19 | loss: 1.59476 (0.118s) | train acc: 0.362, train AUC: 0.714\n",
      "Batch 19/19 | loss: 1.62295 (0.105s) | train acc: 0.373, train AUC: 0.704\n",
      "Validation: val loss: 1.803 | val acc: 0.287 | val F1: 0.277 | val AUC: 0.616\n",
      "\n",
      "-----Epoch 10/100-----\n",
      "Batch 10/19 | loss: 1.50298 (0.119s) | train acc: 0.388, train AUC: 0.767\n",
      "Batch 19/19 | loss: 1.68651 (0.105s) | train acc: 0.360, train AUC: 0.724\n",
      "Validation: val loss: 1.797 | val acc: 0.293 | val F1: 0.272 | val AUC: 0.631\n",
      "\n",
      "-----Epoch 11/100-----\n",
      "Batch 10/19 | loss: 1.45111 (0.118s) | train acc: 0.487, train AUC: 0.775\n",
      "Batch 19/19 | loss: 1.59812 (0.105s) | train acc: 0.427, train AUC: 0.750\n",
      "Validation: val loss: 1.750 | val acc: 0.293 | val F1: 0.287 | val AUC: 0.628\n",
      "\n",
      "-----Epoch 12/100-----\n",
      "Batch 10/19 | loss: 1.52909 (0.118s) | train acc: 0.388, train AUC: 0.750\n",
      "Batch 19/19 | loss: 1.51134 (0.105s) | train acc: 0.367, train AUC: 0.749\n",
      "Validation: val loss: 1.767 | val acc: 0.267 | val F1: 0.246 | val AUC: 0.639\n",
      "\n",
      "-----Epoch 13/100-----\n",
      "Batch 10/19 | loss: 1.53851 (0.118s) | train acc: 0.400, train AUC: 0.737\n",
      "Batch 19/19 | loss: 1.44082 (0.105s) | train acc: 0.440, train AUC: 0.761\n",
      "Validation: val loss: 1.762 | val acc: 0.273 | val F1: 0.275 | val AUC: 0.638\n",
      "\n",
      "-----Epoch 14/100-----\n",
      "Batch 10/19 | loss: 1.48787 (0.118s) | train acc: 0.425, train AUC: 0.779\n",
      "Batch 19/19 | loss: 1.52669 (0.105s) | train acc: 0.400, train AUC: 0.755\n",
      "Validation: val loss: 1.774 | val acc: 0.267 | val F1: 0.254 | val AUC: 0.633\n",
      "\n",
      "-----Epoch 15/100-----\n",
      "Batch 10/19 | loss: 1.47996 (0.118s) | train acc: 0.400, train AUC: 0.789\n",
      "Batch 19/19 | loss: 1.56887 (0.105s) | train acc: 0.393, train AUC: 0.747\n",
      "Validation: val loss: 1.695 | val acc: 0.340 | val F1: 0.341 | val AUC: 0.680\n",
      "\n",
      "-----Epoch 16/100-----\n",
      "Batch 10/19 | loss: 1.46211 (0.118s) | train acc: 0.350, train AUC: 0.767\n",
      "Batch 19/19 | loss: 1.58312 (0.105s) | train acc: 0.367, train AUC: 0.742\n",
      "Validation: val loss: 1.726 | val acc: 0.333 | val F1: 0.314 | val AUC: 0.684\n",
      "\n",
      "-----Epoch 17/100-----\n",
      "Batch 10/19 | loss: 1.50202 (0.118s) | train acc: 0.375, train AUC: 0.751\n",
      "Batch 19/19 | loss: 1.48476 (0.105s) | train acc: 0.400, train AUC: 0.758\n",
      "Validation: val loss: 1.709 | val acc: 0.313 | val F1: 0.306 | val AUC: 0.662\n",
      "\n",
      "-----Epoch 18/100-----\n",
      "Batch 10/19 | loss: 1.56907 (0.118s) | train acc: 0.375, train AUC: 0.723\n",
      "Batch 19/19 | loss: 1.43645 (0.106s) | train acc: 0.427, train AUC: 0.749\n",
      "Validation: val loss: 1.791 | val acc: 0.267 | val F1: 0.252 | val AUC: 0.639\n",
      "\n",
      "-----Epoch 19/100-----\n",
      "Batch 10/19 | loss: 1.45674 (0.119s) | train acc: 0.450, train AUC: 0.784\n",
      "Batch 19/19 | loss: 1.51332 (0.105s) | train acc: 0.407, train AUC: 0.770\n",
      "Validation: val loss: 1.753 | val acc: 0.300 | val F1: 0.283 | val AUC: 0.652\n",
      "\n",
      "-----Epoch 20/100-----\n",
      "Batch 10/19 | loss: 1.46554 (0.118s) | train acc: 0.362, train AUC: 0.764\n",
      "Batch 19/19 | loss: 1.46729 (0.105s) | train acc: 0.387, train AUC: 0.773\n",
      "Validation: val loss: 1.775 | val acc: 0.307 | val F1: 0.303 | val AUC: 0.652\n",
      "\n",
      "-----Epoch 21/100-----\n",
      "Batch 10/19 | loss: 1.46240 (0.118s) | train acc: 0.400, train AUC: 0.770\n",
      "Batch 19/19 | loss: 1.53326 (0.105s) | train acc: 0.400, train AUC: 0.745\n",
      "Validation: val loss: 1.918 | val acc: 0.260 | val F1: 0.227 | val AUC: 0.631\n",
      "\n",
      "-----Epoch 22/100-----\n",
      "Batch 10/19 | loss: 1.44311 (0.118s) | train acc: 0.487, train AUC: 0.761\n",
      "Batch 19/19 | loss: 1.43485 (0.105s) | train acc: 0.480, train AUC: 0.772\n",
      "Validation: val loss: 1.786 | val acc: 0.280 | val F1: 0.262 | val AUC: 0.633\n",
      "\n",
      "-----Epoch 23/100-----\n",
      "Batch 10/19 | loss: 1.54674 (0.118s) | train acc: 0.400, train AUC: 0.736\n",
      "Batch 19/19 | loss: 1.38967 (0.105s) | train acc: 0.433, train AUC: 0.773\n",
      "Validation: val loss: 1.777 | val acc: 0.320 | val F1: 0.306 | val AUC: 0.654\n",
      "\n",
      "-----Epoch 24/100-----\n",
      "Batch 10/19 | loss: 1.51175 (0.118s) | train acc: 0.325, train AUC: 0.749\n",
      "Batch 19/19 | loss: 1.38996 (0.105s) | train acc: 0.407, train AUC: 0.776\n",
      "Validation: val loss: 1.842 | val acc: 0.320 | val F1: 0.304 | val AUC: 0.650\n",
      "\n",
      "-----Epoch 25/100-----\n",
      "Batch 10/19 | loss: 1.45793 (0.118s) | train acc: 0.438, train AUC: 0.768\n",
      "Batch 19/19 | loss: 1.33850 (0.105s) | train acc: 0.473, train AUC: 0.793\n",
      "Validation: val loss: 1.714 | val acc: 0.273 | val F1: 0.268 | val AUC: 0.672\n",
      "\n",
      "-----Epoch 26/100-----\n",
      "Batch 10/19 | loss: 1.34316 (0.118s) | train acc: 0.512, train AUC: 0.818\n",
      "Batch 19/19 | loss: 1.38159 (0.105s) | train acc: 0.500, train AUC: 0.809\n",
      "Validation: val loss: 1.744 | val acc: 0.300 | val F1: 0.284 | val AUC: 0.647\n",
      "\n",
      "-----Epoch 27/100-----\n",
      "Batch 10/19 | loss: 1.35497 (0.118s) | train acc: 0.450, train AUC: 0.816\n",
      "Batch 19/19 | loss: 1.40561 (0.105s) | train acc: 0.447, train AUC: 0.805\n",
      "Validation: val loss: 1.761 | val acc: 0.333 | val F1: 0.318 | val AUC: 0.648\n",
      "\n",
      "-----Epoch 28/100-----\n",
      "Batch 10/19 | loss: 1.46212 (0.118s) | train acc: 0.438, train AUC: 0.770\n",
      "Batch 19/19 | loss: 1.38498 (0.105s) | train acc: 0.467, train AUC: 0.781\n",
      "Validation: val loss: 1.797 | val acc: 0.320 | val F1: 0.308 | val AUC: 0.653\n",
      "\n",
      "-----Epoch 29/100-----\n",
      "Batch 10/19 | loss: 1.42473 (0.118s) | train acc: 0.450, train AUC: 0.769\n",
      "Batch 19/19 | loss: 1.34423 (0.105s) | train acc: 0.473, train AUC: 0.796\n",
      "Validation: val loss: 1.714 | val acc: 0.320 | val F1: 0.306 | val AUC: 0.671\n",
      "\n",
      "-----Epoch 30/100-----\n",
      "Batch 10/19 | loss: 1.40257 (0.119s) | train acc: 0.425, train AUC: 0.789\n",
      "Batch 19/19 | loss: 1.41887 (0.105s) | train acc: 0.447, train AUC: 0.788\n",
      "Validation: val loss: 1.737 | val acc: 0.380 | val F1: 0.367 | val AUC: 0.658\n",
      "\n",
      "-----Epoch 31/100-----\n",
      "Batch 10/19 | loss: 1.39622 (0.119s) | train acc: 0.425, train AUC: 0.790\n",
      "Batch 19/19 | loss: 1.46715 (0.105s) | train acc: 0.453, train AUC: 0.779\n",
      "Validation: val loss: 1.844 | val acc: 0.313 | val F1: 0.310 | val AUC: 0.630\n",
      "\n",
      "-----Epoch 32/100-----\n",
      "Batch 10/19 | loss: 1.29010 (0.118s) | train acc: 0.562, train AUC: 0.848\n",
      "Batch 19/19 | loss: 1.45612 (0.106s) | train acc: 0.507, train AUC: 0.815\n",
      "Validation: val loss: 1.794 | val acc: 0.320 | val F1: 0.310 | val AUC: 0.663\n",
      "\n",
      "-----Epoch 33/100-----\n",
      "Batch 10/19 | loss: 1.20439 (0.118s) | train acc: 0.550, train AUC: 0.874\n",
      "Batch 19/19 | loss: 1.35317 (0.105s) | train acc: 0.500, train AUC: 0.847\n",
      "Validation: val loss: 1.850 | val acc: 0.320 | val F1: 0.317 | val AUC: 0.653\n",
      "\n",
      "-----Epoch 34/100-----\n",
      "Batch 10/19 | loss: 1.18990 (0.120s) | train acc: 0.575, train AUC: 0.864\n",
      "Batch 19/19 | loss: 1.46971 (0.105s) | train acc: 0.513, train AUC: 0.824\n",
      "Validation: val loss: 1.909 | val acc: 0.293 | val F1: 0.289 | val AUC: 0.637\n",
      "\n",
      "-----Epoch 35/100-----\n",
      "Batch 10/19 | loss: 1.21614 (0.119s) | train acc: 0.537, train AUC: 0.859\n",
      "Batch 19/19 | loss: 1.33883 (0.105s) | train acc: 0.480, train AUC: 0.833\n",
      "Validation: val loss: 1.892 | val acc: 0.300 | val F1: 0.296 | val AUC: 0.651\n",
      "\n",
      "-----Epoch 36/100-----\n",
      "Batch 10/19 | loss: 1.18862 (0.118s) | train acc: 0.588, train AUC: 0.875\n",
      "Batch 19/19 | loss: 1.34811 (0.106s) | train acc: 0.527, train AUC: 0.845\n",
      "Validation: val loss: 1.901 | val acc: 0.300 | val F1: 0.296 | val AUC: 0.639\n",
      "\n",
      "-----Epoch 37/100-----\n",
      "Batch 10/19 | loss: 1.29767 (0.118s) | train acc: 0.525, train AUC: 0.829\n",
      "Batch 19/19 | loss: 1.28748 (0.105s) | train acc: 0.507, train AUC: 0.826\n",
      "Validation: val loss: 1.947 | val acc: 0.273 | val F1: 0.260 | val AUC: 0.638\n",
      "\n",
      "-----Epoch 38/100-----\n",
      "Batch 10/19 | loss: 1.40557 (0.118s) | train acc: 0.487, train AUC: 0.787\n",
      "Batch 19/19 | loss: 1.29246 (0.105s) | train acc: 0.500, train AUC: 0.802\n",
      "Validation: val loss: 1.821 | val acc: 0.307 | val F1: 0.302 | val AUC: 0.653\n",
      "\n",
      "-----Epoch 39/100-----\n",
      "Batch 10/19 | loss: 1.30925 (0.119s) | train acc: 0.475, train AUC: 0.823\n",
      "Batch 19/19 | loss: 1.21485 (0.106s) | train acc: 0.520, train AUC: 0.845\n",
      "Validation: val loss: 1.816 | val acc: 0.340 | val F1: 0.318 | val AUC: 0.655\n",
      "\n",
      "-----Epoch 40/100-----\n",
      "Batch 10/19 | loss: 1.18824 (0.118s) | train acc: 0.525, train AUC: 0.863\n",
      "Batch 19/19 | loss: 1.25802 (0.105s) | train acc: 0.520, train AUC: 0.852\n",
      "Validation: val loss: 1.901 | val acc: 0.353 | val F1: 0.341 | val AUC: 0.637\n",
      "\n",
      "-----Epoch 41/100-----\n",
      "Batch 10/19 | loss: 1.21883 (0.118s) | train acc: 0.550, train AUC: 0.854\n",
      "Batch 19/19 | loss: 1.25545 (0.105s) | train acc: 0.547, train AUC: 0.846\n",
      "Validation: val loss: 1.878 | val acc: 0.313 | val F1: 0.308 | val AUC: 0.636\n",
      "\n",
      "-----Epoch 42/100-----\n",
      "Batch 10/19 | loss: 1.31112 (0.118s) | train acc: 0.500, train AUC: 0.836\n",
      "Batch 19/19 | loss: 1.23820 (0.105s) | train acc: 0.520, train AUC: 0.836\n",
      "Validation: val loss: 1.833 | val acc: 0.313 | val F1: 0.291 | val AUC: 0.651\n",
      "\n",
      "-----Epoch 43/100-----\n",
      "Batch 10/19 | loss: 1.11072 (0.118s) | train acc: 0.637, train AUC: 0.892\n",
      "Batch 19/19 | loss: 1.22393 (0.105s) | train acc: 0.567, train AUC: 0.866\n",
      "Validation: val loss: 1.855 | val acc: 0.307 | val F1: 0.306 | val AUC: 0.659\n",
      "\n",
      "-----Epoch 44/100-----\n",
      "Batch 10/19 | loss: 1.17073 (0.119s) | train acc: 0.575, train AUC: 0.866\n",
      "Batch 19/19 | loss: 1.28973 (0.105s) | train acc: 0.560, train AUC: 0.845\n",
      "Validation: val loss: 1.908 | val acc: 0.293 | val F1: 0.286 | val AUC: 0.640\n",
      "\n",
      "-----Epoch 45/100-----\n",
      "Batch 10/19 | loss: 1.23686 (0.118s) | train acc: 0.500, train AUC: 0.839\n",
      "Batch 19/19 | loss: 1.22047 (0.105s) | train acc: 0.547, train AUC: 0.845\n",
      "Validation: val loss: 1.810 | val acc: 0.347 | val F1: 0.349 | val AUC: 0.665\n",
      "\n",
      "-----Epoch 46/100-----\n",
      "Batch 10/19 | loss: 1.14694 (0.119s) | train acc: 0.637, train AUC: 0.856\n",
      "Batch 19/19 | loss: 1.34401 (0.106s) | train acc: 0.560, train AUC: 0.840\n",
      "Validation: val loss: 1.872 | val acc: 0.313 | val F1: 0.299 | val AUC: 0.642\n",
      "\n",
      "-----Epoch 47/100-----\n",
      "Batch 10/19 | loss: 1.29125 (0.119s) | train acc: 0.463, train AUC: 0.821\n",
      "Batch 19/19 | loss: 1.22282 (0.105s) | train acc: 0.493, train AUC: 0.835\n",
      "Validation: val loss: 1.830 | val acc: 0.353 | val F1: 0.331 | val AUC: 0.644\n",
      "\n",
      "-----Epoch 48/100-----\n",
      "Batch 10/19 | loss: 1.22067 (0.118s) | train acc: 0.550, train AUC: 0.853\n",
      "Batch 19/19 | loss: 1.30417 (0.106s) | train acc: 0.533, train AUC: 0.834\n",
      "Validation: val loss: 1.830 | val acc: 0.300 | val F1: 0.301 | val AUC: 0.659\n",
      "\n",
      "-----Epoch 49/100-----\n",
      "Batch 10/19 | loss: 1.09772 (0.118s) | train acc: 0.562, train AUC: 0.896\n",
      "Batch 19/19 | loss: 1.22952 (0.105s) | train acc: 0.527, train AUC: 0.872\n",
      "Validation: val loss: 1.906 | val acc: 0.300 | val F1: 0.290 | val AUC: 0.634\n",
      "\n",
      "-----Epoch 50/100-----\n",
      "Batch 10/19 | loss: 1.10747 (0.118s) | train acc: 0.575, train AUC: 0.891\n",
      "Batch 19/19 | loss: 1.26017 (0.105s) | train acc: 0.520, train AUC: 0.859\n",
      "Validation: val loss: 1.845 | val acc: 0.307 | val F1: 0.303 | val AUC: 0.659\n",
      "\n",
      "-----Epoch 51/100-----\n",
      "Batch 10/19 | loss: 1.26405 (0.118s) | train acc: 0.550, train AUC: 0.839\n",
      "Batch 19/19 | loss: 1.26812 (0.105s) | train acc: 0.520, train AUC: 0.835\n",
      "Validation: val loss: 1.893 | val acc: 0.280 | val F1: 0.287 | val AUC: 0.647\n",
      "\n",
      "-----Epoch 52/100-----\n",
      "Batch 10/19 | loss: 1.27990 (0.119s) | train acc: 0.525, train AUC: 0.826\n",
      "Batch 19/19 | loss: 1.12650 (0.106s) | train acc: 0.560, train AUC: 0.850\n",
      "Validation: val loss: 2.026 | val acc: 0.260 | val F1: 0.229 | val AUC: 0.642\n",
      "\n",
      "-----Epoch 53/100-----\n",
      "Batch 10/19 | loss: 1.18692 (0.118s) | train acc: 0.562, train AUC: 0.856\n",
      "Batch 19/19 | loss: 1.24753 (0.106s) | train acc: 0.573, train AUC: 0.851\n",
      "Validation: val loss: 1.978 | val acc: 0.273 | val F1: 0.262 | val AUC: 0.624\n",
      "\n",
      "-----Epoch 54/100-----\n",
      "Batch 10/19 | loss: 1.13307 (0.119s) | train acc: 0.588, train AUC: 0.868\n",
      "Batch 19/19 | loss: 1.12747 (0.105s) | train acc: 0.587, train AUC: 0.874\n",
      "Validation: val loss: 1.874 | val acc: 0.320 | val F1: 0.308 | val AUC: 0.663\n",
      "\n",
      "-----Epoch 55/100-----\n",
      "Batch 10/19 | loss: 1.11857 (0.119s) | train acc: 0.550, train AUC: 0.883\n",
      "Batch 19/19 | loss: 1.20298 (0.105s) | train acc: 0.553, train AUC: 0.869\n",
      "Validation: val loss: 1.834 | val acc: 0.353 | val F1: 0.335 | val AUC: 0.650\n",
      "\n",
      "-----Epoch 56/100-----\n",
      "Batch 10/19 | loss: 1.15177 (0.118s) | train acc: 0.588, train AUC: 0.868\n",
      "Batch 19/19 | loss: 0.98963 (0.105s) | train acc: 0.587, train AUC: 0.888\n",
      "Validation: val loss: 1.844 | val acc: 0.320 | val F1: 0.314 | val AUC: 0.670\n",
      "\n",
      "-----Epoch 57/100-----\n",
      "Batch 10/19 | loss: 1.11420 (0.119s) | train acc: 0.525, train AUC: 0.878\n",
      "Batch 19/19 | loss: 1.19426 (0.105s) | train acc: 0.540, train AUC: 0.868\n",
      "Validation: val loss: 1.870 | val acc: 0.293 | val F1: 0.287 | val AUC: 0.685\n",
      "\n",
      "-----Epoch 58/100-----\n",
      "Batch 10/19 | loss: 1.16236 (0.119s) | train acc: 0.550, train AUC: 0.855\n",
      "Batch 19/19 | loss: 1.15739 (0.106s) | train acc: 0.560, train AUC: 0.865\n",
      "Validation: val loss: 2.120 | val acc: 0.260 | val F1: 0.272 | val AUC: 0.632\n",
      "\n",
      "-----Epoch 59/100-----\n",
      "Batch 10/19 | loss: 1.08979 (0.118s) | train acc: 0.575, train AUC: 0.881\n",
      "Batch 19/19 | loss: 1.06134 (0.105s) | train acc: 0.600, train AUC: 0.891\n",
      "Validation: val loss: 1.990 | val acc: 0.320 | val F1: 0.324 | val AUC: 0.635\n",
      "\n",
      "-----Epoch 60/100-----\n",
      "Batch 10/19 | loss: 1.19214 (0.119s) | train acc: 0.613, train AUC: 0.862\n",
      "Batch 19/19 | loss: 1.25714 (0.106s) | train acc: 0.533, train AUC: 0.842\n",
      "Validation: val loss: 1.888 | val acc: 0.333 | val F1: 0.323 | val AUC: 0.667\n",
      "\n",
      "-----Epoch 61/100-----\n",
      "Batch 10/19 | loss: 1.09184 (0.119s) | train acc: 0.613, train AUC: 0.884\n",
      "Batch 19/19 | loss: 1.10211 (0.106s) | train acc: 0.627, train AUC: 0.882\n",
      "Validation: val loss: 1.990 | val acc: 0.353 | val F1: 0.349 | val AUC: 0.640\n",
      "\n",
      "-----Epoch 62/100-----\n",
      "Batch 10/19 | loss: 0.95322 (0.119s) | train acc: 0.600, train AUC: 0.913\n",
      "Batch 19/19 | loss: 1.02831 (0.105s) | train acc: 0.627, train AUC: 0.907\n",
      "Validation: val loss: 1.882 | val acc: 0.353 | val F1: 0.352 | val AUC: 0.676\n",
      "\n",
      "-----Epoch 63/100-----\n",
      "Batch 10/19 | loss: 1.04791 (0.119s) | train acc: 0.613, train AUC: 0.891\n",
      "Batch 19/19 | loss: 1.15938 (0.105s) | train acc: 0.607, train AUC: 0.878\n",
      "Validation: val loss: 1.992 | val acc: 0.320 | val F1: 0.319 | val AUC: 0.661\n",
      "\n",
      "-----Epoch 64/100-----\n",
      "Batch 10/19 | loss: 1.03941 (0.119s) | train acc: 0.625, train AUC: 0.906\n",
      "Batch 19/19 | loss: 1.16469 (0.106s) | train acc: 0.540, train AUC: 0.880\n",
      "Validation: val loss: 2.009 | val acc: 0.273 | val F1: 0.271 | val AUC: 0.637\n",
      "\n",
      "-----Epoch 65/100-----\n",
      "Batch 10/19 | loss: 1.07707 (0.119s) | train acc: 0.600, train AUC: 0.890\n",
      "Batch 19/19 | loss: 1.04179 (0.106s) | train acc: 0.600, train AUC: 0.891\n",
      "Validation: val loss: 1.863 | val acc: 0.293 | val F1: 0.270 | val AUC: 0.671\n",
      "\n",
      "-----Epoch 66/100-----\n",
      "Batch 10/19 | loss: 1.09623 (0.118s) | train acc: 0.588, train AUC: 0.874\n",
      "Batch 19/19 | loss: 1.01345 (0.105s) | train acc: 0.613, train AUC: 0.892\n",
      "Validation: val loss: 1.992 | val acc: 0.353 | val F1: 0.352 | val AUC: 0.643\n",
      "Epoch 00066: reducing learning rate of group 0 to 5.0000e-04.\n",
      "\n",
      "-----Epoch 67/100-----\n",
      "Batch 10/19 | loss: 1.03518 (0.119s) | train acc: 0.588, train AUC: 0.895\n",
      "Batch 19/19 | loss: 0.90503 (0.105s) | train acc: 0.627, train AUC: 0.910\n",
      "Validation: val loss: 1.845 | val acc: 0.313 | val F1: 0.310 | val AUC: 0.669\n",
      "\n",
      "-----Epoch 68/100-----\n",
      "Batch 10/19 | loss: 1.02870 (0.118s) | train acc: 0.637, train AUC: 0.912\n",
      "Batch 19/19 | loss: 0.92997 (0.105s) | train acc: 0.640, train AUC: 0.914\n",
      "Validation: val loss: 1.921 | val acc: 0.340 | val F1: 0.340 | val AUC: 0.653\n",
      "\n",
      "-----Epoch 69/100-----\n",
      "Batch 10/19 | loss: 0.87579 (0.119s) | train acc: 0.713, train AUC: 0.938\n",
      "Batch 19/19 | loss: 0.84797 (0.105s) | train acc: 0.713, train AUC: 0.936\n",
      "Validation: val loss: 1.907 | val acc: 0.380 | val F1: 0.374 | val AUC: 0.668\n",
      "\n",
      "-----Epoch 70/100-----\n",
      "Batch 10/19 | loss: 0.89954 (0.119s) | train acc: 0.688, train AUC: 0.940\n",
      "Batch 19/19 | loss: 0.91907 (0.106s) | train acc: 0.700, train AUC: 0.927\n",
      "Validation: val loss: 1.976 | val acc: 0.353 | val F1: 0.352 | val AUC: 0.660\n",
      "\n",
      "-----Epoch 71/100-----\n",
      "Batch 10/19 | loss: 0.86291 (0.119s) | train acc: 0.688, train AUC: 0.934\n",
      "Batch 19/19 | loss: 0.95508 (0.105s) | train acc: 0.667, train AUC: 0.925\n",
      "Validation: val loss: 2.023 | val acc: 0.340 | val F1: 0.338 | val AUC: 0.655\n",
      "\n",
      "-----Epoch 72/100-----\n",
      "Batch 10/19 | loss: 0.79807 (0.118s) | train acc: 0.787, train AUC: 0.946\n",
      "Batch 19/19 | loss: 0.99564 (0.105s) | train acc: 0.713, train AUC: 0.928\n",
      "Validation: val loss: 2.035 | val acc: 0.360 | val F1: 0.347 | val AUC: 0.653\n",
      "\n",
      "-----Epoch 73/100-----\n",
      "Batch 10/19 | loss: 0.84204 (0.119s) | train acc: 0.713, train AUC: 0.937\n",
      "Batch 19/19 | loss: 0.98095 (0.105s) | train acc: 0.680, train AUC: 0.917\n",
      "Validation: val loss: 1.995 | val acc: 0.380 | val F1: 0.384 | val AUC: 0.658\n",
      "\n",
      "-----Epoch 74/100-----\n",
      "Batch 10/19 | loss: 0.82360 (0.119s) | train acc: 0.713, train AUC: 0.928\n",
      "Batch 19/19 | loss: 0.88440 (0.105s) | train acc: 0.713, train AUC: 0.929\n",
      "Validation: val loss: 1.928 | val acc: 0.360 | val F1: 0.361 | val AUC: 0.670\n",
      "\n",
      "-----Epoch 75/100-----\n",
      "Batch 10/19 | loss: 0.87822 (0.118s) | train acc: 0.662, train AUC: 0.925\n",
      "Batch 19/19 | loss: 0.89759 (0.105s) | train acc: 0.687, train AUC: 0.925\n",
      "Validation: val loss: 1.953 | val acc: 0.353 | val F1: 0.358 | val AUC: 0.675\n",
      "\n",
      "-----Epoch 76/100-----\n",
      "Batch 10/19 | loss: 0.70996 (0.118s) | train acc: 0.787, train AUC: 0.957\n",
      "Batch 19/19 | loss: 0.89818 (0.105s) | train acc: 0.747, train AUC: 0.938\n",
      "Validation: val loss: 1.990 | val acc: 0.373 | val F1: 0.380 | val AUC: 0.663\n",
      "\n",
      "-----Epoch 77/100-----\n",
      "Batch 10/19 | loss: 0.74841 (0.117s) | train acc: 0.725, train AUC: 0.956\n",
      "Batch 19/19 | loss: 0.79214 (0.105s) | train acc: 0.720, train AUC: 0.949\n",
      "Validation: val loss: 1.913 | val acc: 0.333 | val F1: 0.332 | val AUC: 0.669\n",
      "\n",
      "-----Epoch 78/100-----\n",
      "Batch 10/19 | loss: 0.77198 (0.118s) | train acc: 0.762, train AUC: 0.957\n",
      "Batch 19/19 | loss: 0.80483 (0.105s) | train acc: 0.740, train AUC: 0.950\n",
      "Validation: val loss: 1.994 | val acc: 0.380 | val F1: 0.383 | val AUC: 0.665\n",
      "\n",
      "-----Epoch 79/100-----\n",
      "Batch 10/19 | loss: 0.70297 (0.117s) | train acc: 0.787, train AUC: 0.959\n",
      "Batch 19/19 | loss: 0.69339 (0.105s) | train acc: 0.773, train AUC: 0.961\n",
      "Validation: val loss: 2.009 | val acc: 0.353 | val F1: 0.357 | val AUC: 0.656\n",
      "\n",
      "-----Epoch 80/100-----\n",
      "Batch 10/19 | loss: 0.69972 (0.118s) | train acc: 0.762, train AUC: 0.964\n",
      "Batch 19/19 | loss: 0.88288 (0.105s) | train acc: 0.747, train AUC: 0.945\n",
      "Validation: val loss: 2.046 | val acc: 0.333 | val F1: 0.337 | val AUC: 0.653\n",
      "\n",
      "-----Epoch 81/100-----\n",
      "Batch 10/19 | loss: 0.79767 (0.117s) | train acc: 0.762, train AUC: 0.946\n",
      "Batch 19/19 | loss: 0.66398 (0.105s) | train acc: 0.800, train AUC: 0.954\n",
      "Validation: val loss: 2.085 | val acc: 0.373 | val F1: 0.374 | val AUC: 0.666\n",
      "\n",
      "-----Epoch 82/100-----\n",
      "Batch 10/19 | loss: 0.75072 (0.118s) | train acc: 0.713, train AUC: 0.953\n",
      "Batch 19/19 | loss: 0.85098 (0.105s) | train acc: 0.700, train AUC: 0.941\n",
      "Validation: val loss: 2.042 | val acc: 0.367 | val F1: 0.373 | val AUC: 0.675\n",
      "\n",
      "-----Epoch 83/100-----\n",
      "Batch 10/19 | loss: 0.58939 (0.118s) | train acc: 0.838, train AUC: 0.973\n",
      "Batch 19/19 | loss: 0.79783 (0.105s) | train acc: 0.787, train AUC: 0.962\n",
      "Validation: val loss: 2.084 | val acc: 0.340 | val F1: 0.346 | val AUC: 0.665\n",
      "\n",
      "-----Epoch 84/100-----\n",
      "Batch 10/19 | loss: 0.79759 (0.118s) | train acc: 0.750, train AUC: 0.939\n",
      "Batch 19/19 | loss: 0.83388 (0.105s) | train acc: 0.747, train AUC: 0.934\n",
      "Validation: val loss: 2.104 | val acc: 0.367 | val F1: 0.366 | val AUC: 0.669\n",
      "\n",
      "-----Epoch 85/100-----\n",
      "Batch 10/19 | loss: 0.69900 (0.117s) | train acc: 0.787, train AUC: 0.957\n",
      "Batch 19/19 | loss: 0.62596 (0.104s) | train acc: 0.793, train AUC: 0.965\n",
      "Validation: val loss: 2.143 | val acc: 0.307 | val F1: 0.308 | val AUC: 0.647\n",
      "\n",
      "-----Epoch 86/100-----\n",
      "Batch 10/19 | loss: 0.80699 (0.118s) | train acc: 0.688, train AUC: 0.932\n",
      "Batch 19/19 | loss: 0.53358 (0.105s) | train acc: 0.773, train AUC: 0.958\n",
      "Validation: val loss: 2.132 | val acc: 0.373 | val F1: 0.376 | val AUC: 0.653\n",
      "\n",
      "-----Epoch 87/100-----\n",
      "Batch 10/19 | loss: 0.73291 (0.118s) | train acc: 0.762, train AUC: 0.951\n",
      "Batch 19/19 | loss: 0.67011 (0.105s) | train acc: 0.787, train AUC: 0.955\n",
      "Validation: val loss: 2.049 | val acc: 0.380 | val F1: 0.385 | val AUC: 0.673\n",
      "\n",
      "-----Epoch 88/100-----\n",
      "Batch 10/19 | loss: 0.83261 (0.118s) | train acc: 0.637, train AUC: 0.933\n",
      "Batch 19/19 | loss: 0.66815 (0.105s) | train acc: 0.707, train AUC: 0.947\n",
      "Validation: val loss: 2.151 | val acc: 0.380 | val F1: 0.380 | val AUC: 0.660\n",
      "\n",
      "-----Epoch 89/100-----\n",
      "Batch 10/19 | loss: 0.53219 (0.118s) | train acc: 0.875, train AUC: 0.980\n",
      "Batch 19/19 | loss: 0.71083 (0.105s) | train acc: 0.827, train AUC: 0.970\n",
      "Validation: val loss: 1.978 | val acc: 0.373 | val F1: 0.374 | val AUC: 0.681\n",
      "\n",
      "-----Epoch 90/100-----\n",
      "Batch 10/19 | loss: 0.82126 (0.118s) | train acc: 0.650, train AUC: 0.935\n",
      "Batch 19/19 | loss: 0.74139 (0.105s) | train acc: 0.687, train AUC: 0.942\n",
      "Validation: val loss: 2.177 | val acc: 0.380 | val F1: 0.378 | val AUC: 0.643\n",
      "\n",
      "-----Epoch 91/100-----\n",
      "Batch 10/19 | loss: 0.60619 (0.118s) | train acc: 0.838, train AUC: 0.974\n",
      "Batch 19/19 | loss: 0.83108 (0.105s) | train acc: 0.793, train AUC: 0.957\n",
      "Validation: val loss: 2.060 | val acc: 0.367 | val F1: 0.359 | val AUC: 0.664\n",
      "\n",
      "-----Epoch 92/100-----\n",
      "Batch 10/19 | loss: 0.74458 (0.118s) | train acc: 0.762, train AUC: 0.954\n",
      "Batch 19/19 | loss: 0.67276 (0.105s) | train acc: 0.747, train AUC: 0.959\n",
      "Validation: val loss: 2.131 | val acc: 0.347 | val F1: 0.349 | val AUC: 0.647\n",
      "\n",
      "-----Epoch 93/100-----\n",
      "Batch 10/19 | loss: 0.64837 (0.118s) | train acc: 0.812, train AUC: 0.962\n",
      "Batch 19/19 | loss: 0.75416 (0.105s) | train acc: 0.760, train AUC: 0.955\n",
      "Validation: val loss: 2.060 | val acc: 0.393 | val F1: 0.394 | val AUC: 0.663\n",
      "\n",
      "-----Epoch 94/100-----\n",
      "Batch 10/19 | loss: 0.58112 (0.118s) | train acc: 0.838, train AUC: 0.974\n",
      "Batch 19/19 | loss: 0.70743 (0.105s) | train acc: 0.793, train AUC: 0.967\n",
      "Validation: val loss: 2.014 | val acc: 0.387 | val F1: 0.383 | val AUC: 0.669\n",
      "\n",
      "-----Epoch 95/100-----\n",
      "Batch 10/19 | loss: 0.75751 (0.118s) | train acc: 0.713, train AUC: 0.940\n",
      "Batch 19/19 | loss: 0.72850 (0.105s) | train acc: 0.733, train AUC: 0.946\n",
      "Validation: val loss: 2.111 | val acc: 0.333 | val F1: 0.334 | val AUC: 0.662\n",
      "\n",
      "-----Epoch 96/100-----\n",
      "Batch 10/19 | loss: 0.55109 (0.118s) | train acc: 0.887, train AUC: 0.977\n",
      "Batch 19/19 | loss: 0.70097 (0.105s) | train acc: 0.820, train AUC: 0.969\n",
      "Validation: val loss: 2.124 | val acc: 0.387 | val F1: 0.384 | val AUC: 0.664\n",
      "\n",
      "-----Epoch 97/100-----\n",
      "Batch 10/19 | loss: 0.72972 (0.118s) | train acc: 0.775, train AUC: 0.945\n",
      "Batch 19/19 | loss: 0.59835 (0.105s) | train acc: 0.780, train AUC: 0.958\n",
      "Validation: val loss: 2.073 | val acc: 0.327 | val F1: 0.321 | val AUC: 0.674\n",
      "\n",
      "-----Epoch 98/100-----\n",
      "Batch 10/19 | loss: 0.64447 (0.118s) | train acc: 0.762, train AUC: 0.967\n",
      "Batch 19/19 | loss: 0.75133 (0.105s) | train acc: 0.733, train AUC: 0.955\n",
      "Validation: val loss: 2.202 | val acc: 0.307 | val F1: 0.308 | val AUC: 0.667\n",
      "\n",
      "-----Epoch 99/100-----\n",
      "Batch 10/19 | loss: 0.71627 (0.118s) | train acc: 0.787, train AUC: 0.956\n",
      "Batch 19/19 | loss: 0.73296 (0.105s) | train acc: 0.767, train AUC: 0.950\n",
      "Validation: val loss: 1.972 | val acc: 0.373 | val F1: 0.374 | val AUC: 0.692\n",
      "\n",
      "-----Epoch 100/100-----\n",
      "Batch 10/19 | loss: 0.55256 (0.118s) | train acc: 0.825, train AUC: 0.980\n",
      "Batch 19/19 | loss: 0.73673 (0.105s) | train acc: 0.773, train AUC: 0.965\n",
      "Validation: val loss: 2.180 | val acc: 0.380 | val F1: 0.380 | val AUC: 0.669\n"
     ]
    }
   ],
   "source": [
    "# Create and train target model\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "weight_decay = 1e-4\n",
    "t_model = GATProteinsModel(num_feat=num_feat, num_classes=n_categories).to(DEVICE)\n",
    "optimizer = optim.Adam(t_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "t_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                       factor=0.5,\n",
    "                                                       patience=50,\n",
    "                                                       min_lr=1e-6,\n",
    "                                                       verbose=True)\n",
    "weight = compute_class_weight('balanced', classes=np.unique(t_dataset_train.y.argmax(dim=1)), y=t_dataset_train.y.argmax(dim=1).numpy())\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weight).to(DEVICE))\n",
    "# t_save_path = 'mia-models/t_model_proteins.pth'\n",
    "t_save_path = None\n",
    "\n",
    "train_model_multi_graph(t_model, optimizer, t_dataset_train, loss_fn, epochs, batch_size, val_dataset=t_dataset_test, save_path=t_save_path, save_freq=10, scheduler=t_scheduler, device=DEVICE)\n",
    "# t_model = load_model(t_model, t_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d9de28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "No learning rate scheduling!\n",
      "Training for 100 epochs, with batch size=8\n",
      "Using validation data (60 samples)\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/100-----\n",
      "Batch 10/30 | loss: 1.86216 (0.127s) | train acc: 0.188, train AUC: 0.459\n",
      "Batch 20/30 | loss: 1.79627 (0.126s) | train acc: 0.206, train AUC: 0.512\n",
      "Batch 30/30 | loss: 1.83973 (0.126s) | train acc: 0.204, train AUC: 0.529\n",
      "Validation: val loss: 1.834 | val acc: 0.217 | val F1: 0.181 | val AUC: 0.601\n",
      "\n",
      "-----Epoch 2/100-----\n",
      "Batch 10/30 | loss: 1.74822 (0.119s) | train acc: 0.250, train AUC: 0.625\n",
      "Batch 20/30 | loss: 1.69988 (0.119s) | train acc: 0.244, train AUC: 0.646\n",
      "Batch 30/30 | loss: 1.76220 (0.119s) | train acc: 0.271, train AUC: 0.640\n",
      "Validation: val loss: 1.857 | val acc: 0.217 | val F1: 0.170 | val AUC: 0.582\n",
      "\n",
      "-----Epoch 3/100-----\n",
      "Batch 10/30 | loss: 1.76648 (0.119s) | train acc: 0.287, train AUC: 0.632\n",
      "Batch 20/30 | loss: 1.64367 (0.119s) | train acc: 0.325, train AUC: 0.666\n",
      "Batch 30/30 | loss: 1.71468 (0.119s) | train acc: 0.300, train AUC: 0.648\n",
      "Validation: val loss: 1.857 | val acc: 0.250 | val F1: 0.211 | val AUC: 0.602\n",
      "\n",
      "-----Epoch 4/100-----\n",
      "Batch 10/30 | loss: 1.61358 (0.119s) | train acc: 0.375, train AUC: 0.749\n",
      "Batch 20/30 | loss: 1.68390 (0.119s) | train acc: 0.325, train AUC: 0.701\n",
      "Batch 30/30 | loss: 1.73558 (0.119s) | train acc: 0.333, train AUC: 0.670\n",
      "Validation: val loss: 1.842 | val acc: 0.283 | val F1: 0.264 | val AUC: 0.530\n",
      "\n",
      "-----Epoch 5/100-----\n",
      "Batch 10/30 | loss: 1.68896 (0.119s) | train acc: 0.237, train AUC: 0.659\n",
      "Batch 20/30 | loss: 1.60486 (0.119s) | train acc: 0.312, train AUC: 0.684\n",
      "Batch 30/30 | loss: 1.73857 (0.119s) | train acc: 0.300, train AUC: 0.662\n",
      "Validation: val loss: 1.864 | val acc: 0.233 | val F1: 0.224 | val AUC: 0.534\n",
      "\n",
      "-----Epoch 6/100-----\n",
      "Batch 10/30 | loss: 1.63159 (0.119s) | train acc: 0.362, train AUC: 0.669\n",
      "Batch 20/30 | loss: 1.64687 (0.119s) | train acc: 0.350, train AUC: 0.686\n",
      "Batch 30/30 | loss: 1.61594 (0.119s) | train acc: 0.375, train AUC: 0.699\n",
      "Validation: val loss: 1.788 | val acc: 0.333 | val F1: 0.308 | val AUC: 0.605\n",
      "\n",
      "-----Epoch 7/100-----\n",
      "Batch 10/30 | loss: 1.54822 (0.119s) | train acc: 0.375, train AUC: 0.763\n",
      "Batch 20/30 | loss: 1.67493 (0.119s) | train acc: 0.331, train AUC: 0.714\n",
      "Batch 30/30 | loss: 1.55783 (0.119s) | train acc: 0.354, train AUC: 0.718\n",
      "Validation: val loss: 1.722 | val acc: 0.333 | val F1: 0.307 | val AUC: 0.648\n",
      "\n",
      "-----Epoch 8/100-----\n",
      "Batch 10/30 | loss: 1.53610 (0.119s) | train acc: 0.388, train AUC: 0.746\n",
      "Batch 20/30 | loss: 1.61250 (0.118s) | train acc: 0.362, train AUC: 0.720\n",
      "Batch 30/30 | loss: 1.65047 (0.119s) | train acc: 0.350, train AUC: 0.708\n",
      "Validation: val loss: 1.784 | val acc: 0.233 | val F1: 0.250 | val AUC: 0.620\n",
      "\n",
      "-----Epoch 9/100-----\n",
      "Batch 10/30 | loss: 1.61764 (0.119s) | train acc: 0.362, train AUC: 0.686\n",
      "Batch 20/30 | loss: 1.51726 (0.119s) | train acc: 0.388, train AUC: 0.723\n",
      "Batch 30/30 | loss: 1.62222 (0.119s) | train acc: 0.358, train AUC: 0.713\n",
      "Validation: val loss: 1.873 | val acc: 0.317 | val F1: 0.296 | val AUC: 0.602\n",
      "\n",
      "-----Epoch 10/100-----\n",
      "Batch 10/30 | loss: 1.60721 (0.118s) | train acc: 0.338, train AUC: 0.696\n",
      "Batch 20/30 | loss: 1.63358 (0.119s) | train acc: 0.331, train AUC: 0.683\n",
      "Batch 30/30 | loss: 1.57447 (0.121s) | train acc: 0.350, train AUC: 0.698\n",
      "Validation: val loss: 1.716 | val acc: 0.333 | val F1: 0.303 | val AUC: 0.647\n",
      "\n",
      "-----Epoch 11/100-----\n",
      "Batch 10/30 | loss: 1.62539 (0.119s) | train acc: 0.325, train AUC: 0.677\n",
      "Batch 20/30 | loss: 1.60740 (0.119s) | train acc: 0.344, train AUC: 0.685\n",
      "Batch 30/30 | loss: 1.55097 (0.118s) | train acc: 0.358, train AUC: 0.702\n",
      "Validation: val loss: 1.878 | val acc: 0.317 | val F1: 0.299 | val AUC: 0.603\n",
      "\n",
      "-----Epoch 12/100-----\n",
      "Batch 10/30 | loss: 1.51601 (0.118s) | train acc: 0.388, train AUC: 0.778\n",
      "Batch 20/30 | loss: 1.63149 (0.118s) | train acc: 0.362, train AUC: 0.722\n",
      "Batch 30/30 | loss: 1.56045 (0.118s) | train acc: 0.375, train AUC: 0.717\n",
      "Validation: val loss: 1.755 | val acc: 0.333 | val F1: 0.314 | val AUC: 0.636\n",
      "\n",
      "-----Epoch 13/100-----\n",
      "Batch 10/30 | loss: 1.57234 (0.118s) | train acc: 0.362, train AUC: 0.725\n",
      "Batch 20/30 | loss: 1.59421 (0.118s) | train acc: 0.325, train AUC: 0.717\n",
      "Batch 30/30 | loss: 1.47305 (0.118s) | train acc: 0.362, train AUC: 0.731\n",
      "Validation: val loss: 1.756 | val acc: 0.317 | val F1: 0.311 | val AUC: 0.638\n",
      "\n",
      "-----Epoch 14/100-----\n",
      "Batch 10/30 | loss: 1.49061 (0.118s) | train acc: 0.325, train AUC: 0.746\n",
      "Batch 20/30 | loss: 1.50403 (0.118s) | train acc: 0.356, train AUC: 0.747\n",
      "Batch 30/30 | loss: 1.52537 (0.118s) | train acc: 0.358, train AUC: 0.747\n",
      "Validation: val loss: 1.805 | val acc: 0.283 | val F1: 0.261 | val AUC: 0.639\n",
      "\n",
      "-----Epoch 15/100-----\n",
      "Batch 10/30 | loss: 1.51565 (0.118s) | train acc: 0.388, train AUC: 0.750\n",
      "Batch 20/30 | loss: 1.49670 (0.118s) | train acc: 0.412, train AUC: 0.756\n",
      "Batch 30/30 | loss: 1.50672 (0.118s) | train acc: 0.404, train AUC: 0.755\n",
      "Validation: val loss: 1.783 | val acc: 0.250 | val F1: 0.224 | val AUC: 0.633\n",
      "\n",
      "-----Epoch 16/100-----\n",
      "Batch 10/30 | loss: 1.48787 (0.118s) | train acc: 0.338, train AUC: 0.759\n",
      "Batch 20/30 | loss: 1.37422 (0.118s) | train acc: 0.438, train AUC: 0.784\n",
      "Batch 30/30 | loss: 1.61440 (0.119s) | train acc: 0.417, train AUC: 0.752\n",
      "Validation: val loss: 1.752 | val acc: 0.300 | val F1: 0.295 | val AUC: 0.649\n",
      "\n",
      "-----Epoch 17/100-----\n",
      "Batch 10/30 | loss: 1.55079 (0.118s) | train acc: 0.388, train AUC: 0.719\n",
      "Batch 20/30 | loss: 1.50525 (0.118s) | train acc: 0.394, train AUC: 0.734\n",
      "Batch 30/30 | loss: 1.43595 (0.118s) | train acc: 0.433, train AUC: 0.749\n",
      "Validation: val loss: 1.798 | val acc: 0.317 | val F1: 0.326 | val AUC: 0.631\n",
      "\n",
      "-----Epoch 18/100-----\n",
      "Batch 10/30 | loss: 1.45863 (0.118s) | train acc: 0.475, train AUC: 0.761\n",
      "Batch 20/30 | loss: 1.38983 (0.118s) | train acc: 0.475, train AUC: 0.780\n",
      "Batch 30/30 | loss: 1.52959 (0.118s) | train acc: 0.429, train AUC: 0.768\n",
      "Validation: val loss: 1.724 | val acc: 0.267 | val F1: 0.248 | val AUC: 0.659\n",
      "\n",
      "-----Epoch 19/100-----\n",
      "Batch 10/30 | loss: 1.47322 (0.118s) | train acc: 0.450, train AUC: 0.797\n",
      "Batch 20/30 | loss: 1.54557 (0.118s) | train acc: 0.362, train AUC: 0.753\n",
      "Batch 30/30 | loss: 1.50687 (0.118s) | train acc: 0.392, train AUC: 0.750\n",
      "Validation: val loss: 1.772 | val acc: 0.333 | val F1: 0.319 | val AUC: 0.651\n",
      "\n",
      "-----Epoch 20/100-----\n",
      "Batch 10/30 | loss: 1.45142 (0.118s) | train acc: 0.375, train AUC: 0.774\n",
      "Batch 20/30 | loss: 1.44628 (0.118s) | train acc: 0.400, train AUC: 0.773\n",
      "Batch 30/30 | loss: 1.57364 (0.118s) | train acc: 0.408, train AUC: 0.753\n",
      "Validation: val loss: 1.741 | val acc: 0.267 | val F1: 0.247 | val AUC: 0.674\n",
      "\n",
      "-----Epoch 21/100-----\n",
      "Batch 10/30 | loss: 1.33629 (0.118s) | train acc: 0.512, train AUC: 0.835\n",
      "Batch 20/30 | loss: 1.41046 (0.118s) | train acc: 0.500, train AUC: 0.806\n",
      "Batch 30/30 | loss: 1.64333 (0.118s) | train acc: 0.450, train AUC: 0.767\n",
      "Validation: val loss: 1.815 | val acc: 0.317 | val F1: 0.293 | val AUC: 0.630\n",
      "\n",
      "-----Epoch 22/100-----\n",
      "Batch 10/30 | loss: 1.39688 (0.118s) | train acc: 0.500, train AUC: 0.790\n",
      "Batch 20/30 | loss: 1.44841 (0.118s) | train acc: 0.456, train AUC: 0.785\n",
      "Batch 30/30 | loss: 1.39697 (0.118s) | train acc: 0.442, train AUC: 0.785\n",
      "Validation: val loss: 1.826 | val acc: 0.300 | val F1: 0.293 | val AUC: 0.644\n",
      "\n",
      "-----Epoch 23/100-----\n",
      "Batch 10/30 | loss: 1.40849 (0.118s) | train acc: 0.475, train AUC: 0.796\n",
      "Batch 20/30 | loss: 1.54361 (0.118s) | train acc: 0.412, train AUC: 0.773\n",
      "Batch 30/30 | loss: 1.42795 (0.118s) | train acc: 0.429, train AUC: 0.770\n",
      "Validation: val loss: 1.767 | val acc: 0.283 | val F1: 0.272 | val AUC: 0.648\n",
      "\n",
      "-----Epoch 24/100-----\n",
      "Batch 10/30 | loss: 1.44852 (0.118s) | train acc: 0.412, train AUC: 0.769\n",
      "Batch 20/30 | loss: 1.39062 (0.118s) | train acc: 0.450, train AUC: 0.781\n",
      "Batch 30/30 | loss: 1.44902 (0.118s) | train acc: 0.433, train AUC: 0.775\n",
      "Validation: val loss: 1.836 | val acc: 0.217 | val F1: 0.176 | val AUC: 0.644\n",
      "\n",
      "-----Epoch 25/100-----\n",
      "Batch 10/30 | loss: 1.32462 (0.118s) | train acc: 0.500, train AUC: 0.824\n",
      "Batch 20/30 | loss: 1.49340 (0.118s) | train acc: 0.463, train AUC: 0.786\n",
      "Batch 30/30 | loss: 1.55534 (0.118s) | train acc: 0.433, train AUC: 0.762\n",
      "Validation: val loss: 1.833 | val acc: 0.267 | val F1: 0.259 | val AUC: 0.655\n",
      "\n",
      "-----Epoch 26/100-----\n",
      "Batch 10/30 | loss: 1.39346 (0.118s) | train acc: 0.425, train AUC: 0.783\n",
      "Batch 20/30 | loss: 1.40119 (0.118s) | train acc: 0.419, train AUC: 0.781\n",
      "Batch 30/30 | loss: 1.30862 (0.118s) | train acc: 0.425, train AUC: 0.796\n",
      "Validation: val loss: 1.745 | val acc: 0.300 | val F1: 0.302 | val AUC: 0.658\n",
      "\n",
      "-----Epoch 27/100-----\n",
      "Batch 10/30 | loss: 1.32856 (0.118s) | train acc: 0.487, train AUC: 0.809\n",
      "Batch 20/30 | loss: 1.38767 (0.118s) | train acc: 0.487, train AUC: 0.810\n",
      "Batch 30/30 | loss: 1.37325 (0.118s) | train acc: 0.467, train AUC: 0.799\n",
      "Validation: val loss: 1.738 | val acc: 0.317 | val F1: 0.313 | val AUC: 0.661\n",
      "\n",
      "-----Epoch 28/100-----\n",
      "Batch 10/30 | loss: 1.18113 (0.118s) | train acc: 0.600, train AUC: 0.881\n",
      "Batch 20/30 | loss: 1.32942 (0.118s) | train acc: 0.550, train AUC: 0.853\n",
      "Batch 30/30 | loss: 1.36184 (0.118s) | train acc: 0.504, train AUC: 0.833\n",
      "Validation: val loss: 1.759 | val acc: 0.300 | val F1: 0.293 | val AUC: 0.661\n",
      "\n",
      "-----Epoch 29/100-----\n",
      "Batch 10/30 | loss: 1.32373 (0.118s) | train acc: 0.500, train AUC: 0.817\n",
      "Batch 20/30 | loss: 1.38706 (0.118s) | train acc: 0.475, train AUC: 0.808\n",
      "Batch 30/30 | loss: 1.43808 (0.118s) | train acc: 0.475, train AUC: 0.797\n",
      "Validation: val loss: 1.847 | val acc: 0.283 | val F1: 0.271 | val AUC: 0.661\n",
      "\n",
      "-----Epoch 30/100-----\n",
      "Batch 10/30 | loss: 1.42506 (0.118s) | train acc: 0.438, train AUC: 0.783\n",
      "Batch 20/30 | loss: 1.36463 (0.118s) | train acc: 0.463, train AUC: 0.793\n",
      "Batch 30/30 | loss: 1.45064 (0.118s) | train acc: 0.458, train AUC: 0.788\n",
      "Validation: val loss: 1.803 | val acc: 0.300 | val F1: 0.278 | val AUC: 0.657\n",
      "\n",
      "-----Epoch 31/100-----\n",
      "Batch 10/30 | loss: 1.40006 (0.118s) | train acc: 0.438, train AUC: 0.790\n",
      "Batch 20/30 | loss: 1.45933 (0.118s) | train acc: 0.425, train AUC: 0.770\n",
      "Batch 30/30 | loss: 1.33609 (0.118s) | train acc: 0.450, train AUC: 0.784\n",
      "Validation: val loss: 1.873 | val acc: 0.283 | val F1: 0.277 | val AUC: 0.641\n",
      "\n",
      "-----Epoch 32/100-----\n",
      "Batch 10/30 | loss: 1.31935 (0.118s) | train acc: 0.525, train AUC: 0.819\n",
      "Batch 20/30 | loss: 1.39043 (0.118s) | train acc: 0.512, train AUC: 0.806\n",
      "Batch 30/30 | loss: 1.54479 (0.118s) | train acc: 0.454, train AUC: 0.780\n",
      "Validation: val loss: 1.767 | val acc: 0.267 | val F1: 0.263 | val AUC: 0.645\n",
      "\n",
      "-----Epoch 33/100-----\n",
      "Batch 10/30 | loss: 1.29535 (0.118s) | train acc: 0.475, train AUC: 0.817\n",
      "Batch 20/30 | loss: 1.37712 (0.118s) | train acc: 0.456, train AUC: 0.807\n",
      "Batch 30/30 | loss: 1.35153 (0.118s) | train acc: 0.463, train AUC: 0.807\n",
      "Validation: val loss: 1.761 | val acc: 0.317 | val F1: 0.314 | val AUC: 0.685\n",
      "\n",
      "-----Epoch 34/100-----\n",
      "Batch 10/30 | loss: 1.27442 (0.118s) | train acc: 0.475, train AUC: 0.834\n",
      "Batch 20/30 | loss: 1.32032 (0.118s) | train acc: 0.469, train AUC: 0.828\n",
      "Batch 30/30 | loss: 1.51200 (0.118s) | train acc: 0.458, train AUC: 0.796\n",
      "Validation: val loss: 1.707 | val acc: 0.333 | val F1: 0.296 | val AUC: 0.692\n",
      "\n",
      "-----Epoch 35/100-----\n",
      "Batch 10/30 | loss: 1.33754 (0.118s) | train acc: 0.487, train AUC: 0.821\n",
      "Batch 20/30 | loss: 1.31713 (0.118s) | train acc: 0.506, train AUC: 0.818\n",
      "Batch 30/30 | loss: 1.35097 (0.118s) | train acc: 0.492, train AUC: 0.816\n",
      "Validation: val loss: 1.637 | val acc: 0.367 | val F1: 0.351 | val AUC: 0.700\n",
      "\n",
      "-----Epoch 36/100-----\n",
      "Batch 10/30 | loss: 1.27891 (0.118s) | train acc: 0.500, train AUC: 0.823\n",
      "Batch 20/30 | loss: 1.39153 (0.118s) | train acc: 0.500, train AUC: 0.811\n",
      "Batch 30/30 | loss: 1.35847 (0.118s) | train acc: 0.483, train AUC: 0.805\n",
      "Validation: val loss: 1.693 | val acc: 0.300 | val F1: 0.285 | val AUC: 0.675\n",
      "\n",
      "-----Epoch 37/100-----\n",
      "Batch 10/30 | loss: 1.30053 (0.118s) | train acc: 0.500, train AUC: 0.844\n",
      "Batch 20/30 | loss: 1.26206 (0.118s) | train acc: 0.519, train AUC: 0.843\n",
      "Batch 30/30 | loss: 1.43582 (0.118s) | train acc: 0.479, train AUC: 0.817\n",
      "Validation: val loss: 1.906 | val acc: 0.283 | val F1: 0.279 | val AUC: 0.660\n",
      "\n",
      "-----Epoch 38/100-----\n",
      "Batch 10/30 | loss: 1.27048 (0.118s) | train acc: 0.562, train AUC: 0.835\n",
      "Batch 20/30 | loss: 1.21914 (0.118s) | train acc: 0.525, train AUC: 0.848\n",
      "Batch 30/30 | loss: 1.28429 (0.118s) | train acc: 0.521, train AUC: 0.842\n",
      "Validation: val loss: 1.806 | val acc: 0.333 | val F1: 0.331 | val AUC: 0.658\n",
      "\n",
      "-----Epoch 39/100-----\n",
      "Batch 10/30 | loss: 1.14534 (0.118s) | train acc: 0.550, train AUC: 0.874\n",
      "Batch 20/30 | loss: 1.28722 (0.118s) | train acc: 0.525, train AUC: 0.843\n",
      "Batch 30/30 | loss: 1.42983 (0.118s) | train acc: 0.487, train AUC: 0.821\n",
      "Validation: val loss: 1.759 | val acc: 0.317 | val F1: 0.293 | val AUC: 0.692\n",
      "\n",
      "-----Epoch 40/100-----\n",
      "Batch 10/30 | loss: 1.21861 (0.118s) | train acc: 0.487, train AUC: 0.839\n",
      "Batch 20/30 | loss: 1.32597 (0.118s) | train acc: 0.494, train AUC: 0.823\n",
      "Batch 30/30 | loss: 1.39323 (0.119s) | train acc: 0.487, train AUC: 0.813\n",
      "Validation: val loss: 1.829 | val acc: 0.350 | val F1: 0.314 | val AUC: 0.673\n",
      "\n",
      "-----Epoch 41/100-----\n",
      "Batch 10/30 | loss: 1.37068 (0.118s) | train acc: 0.487, train AUC: 0.788\n",
      "Batch 20/30 | loss: 1.21016 (0.118s) | train acc: 0.519, train AUC: 0.816\n",
      "Batch 30/30 | loss: 1.21445 (0.118s) | train acc: 0.537, train AUC: 0.829\n",
      "Validation: val loss: 1.695 | val acc: 0.400 | val F1: 0.389 | val AUC: 0.702\n",
      "\n",
      "-----Epoch 42/100-----\n",
      "Batch 10/30 | loss: 1.24719 (0.118s) | train acc: 0.500, train AUC: 0.840\n",
      "Batch 20/30 | loss: 1.18992 (0.118s) | train acc: 0.550, train AUC: 0.851\n",
      "Batch 30/30 | loss: 1.31362 (0.118s) | train acc: 0.525, train AUC: 0.839\n",
      "Validation: val loss: 1.846 | val acc: 0.333 | val F1: 0.330 | val AUC: 0.663\n",
      "\n",
      "-----Epoch 43/100-----\n",
      "Batch 10/30 | loss: 1.13553 (0.118s) | train acc: 0.588, train AUC: 0.869\n",
      "Batch 20/30 | loss: 1.19117 (0.118s) | train acc: 0.594, train AUC: 0.866\n",
      "Batch 30/30 | loss: 1.36995 (0.118s) | train acc: 0.533, train AUC: 0.844\n",
      "Validation: val loss: 1.769 | val acc: 0.350 | val F1: 0.338 | val AUC: 0.682\n",
      "\n",
      "-----Epoch 44/100-----\n",
      "Batch 10/30 | loss: 1.22435 (0.118s) | train acc: 0.537, train AUC: 0.831\n",
      "Batch 20/30 | loss: 1.25146 (0.118s) | train acc: 0.537, train AUC: 0.835\n",
      "Batch 30/30 | loss: 1.41690 (0.118s) | train acc: 0.508, train AUC: 0.816\n",
      "Validation: val loss: 1.825 | val acc: 0.300 | val F1: 0.253 | val AUC: 0.698\n",
      "\n",
      "-----Epoch 45/100-----\n",
      "Batch 10/30 | loss: 1.09234 (0.118s) | train acc: 0.500, train AUC: 0.888\n",
      "Batch 20/30 | loss: 1.16984 (0.118s) | train acc: 0.544, train AUC: 0.877\n",
      "Batch 30/30 | loss: 1.24299 (0.118s) | train acc: 0.521, train AUC: 0.862\n",
      "Validation: val loss: 1.830 | val acc: 0.417 | val F1: 0.407 | val AUC: 0.684\n",
      "\n",
      "-----Epoch 46/100-----\n",
      "Batch 10/30 | loss: 1.23231 (0.118s) | train acc: 0.512, train AUC: 0.839\n",
      "Batch 20/30 | loss: 1.30109 (0.118s) | train acc: 0.494, train AUC: 0.825\n",
      "Batch 30/30 | loss: 1.27390 (0.118s) | train acc: 0.483, train AUC: 0.829\n",
      "Validation: val loss: 1.714 | val acc: 0.383 | val F1: 0.376 | val AUC: 0.712\n",
      "\n",
      "-----Epoch 47/100-----\n",
      "Batch 10/30 | loss: 1.12511 (0.118s) | train acc: 0.550, train AUC: 0.874\n",
      "Batch 20/30 | loss: 1.13179 (0.118s) | train acc: 0.544, train AUC: 0.876\n",
      "Batch 30/30 | loss: 1.18826 (0.118s) | train acc: 0.554, train AUC: 0.868\n",
      "Validation: val loss: 1.863 | val acc: 0.333 | val F1: 0.327 | val AUC: 0.660\n",
      "\n",
      "-----Epoch 48/100-----\n",
      "Batch 10/30 | loss: 0.99456 (0.118s) | train acc: 0.662, train AUC: 0.915\n",
      "Batch 20/30 | loss: 1.13997 (0.118s) | train acc: 0.600, train AUC: 0.890\n",
      "Batch 30/30 | loss: 1.27232 (0.118s) | train acc: 0.571, train AUC: 0.868\n",
      "Validation: val loss: 2.007 | val acc: 0.367 | val F1: 0.357 | val AUC: 0.660\n",
      "\n",
      "-----Epoch 49/100-----\n",
      "Batch 10/30 | loss: 1.13101 (0.118s) | train acc: 0.600, train AUC: 0.862\n",
      "Batch 20/30 | loss: 1.14088 (0.118s) | train acc: 0.575, train AUC: 0.861\n",
      "Batch 30/30 | loss: 1.18308 (0.118s) | train acc: 0.567, train AUC: 0.859\n",
      "Validation: val loss: 1.695 | val acc: 0.367 | val F1: 0.364 | val AUC: 0.692\n",
      "\n",
      "-----Epoch 50/100-----\n",
      "Batch 10/30 | loss: 1.17115 (0.118s) | train acc: 0.550, train AUC: 0.857\n",
      "Batch 20/30 | loss: 1.07306 (0.119s) | train acc: 0.556, train AUC: 0.865\n",
      "Batch 30/30 | loss: 1.09616 (0.118s) | train acc: 0.579, train AUC: 0.865\n",
      "Validation: val loss: 1.817 | val acc: 0.350 | val F1: 0.344 | val AUC: 0.693\n",
      "\n",
      "-----Epoch 51/100-----\n",
      "Batch 10/30 | loss: 1.22037 (0.118s) | train acc: 0.512, train AUC: 0.849\n",
      "Batch 20/30 | loss: 1.05816 (0.118s) | train acc: 0.562, train AUC: 0.869\n",
      "Batch 30/30 | loss: 1.26989 (0.118s) | train acc: 0.537, train AUC: 0.854\n",
      "Validation: val loss: 1.719 | val acc: 0.400 | val F1: 0.393 | val AUC: 0.693\n",
      "\n",
      "-----Epoch 52/100-----\n",
      "Batch 10/30 | loss: 1.18513 (0.119s) | train acc: 0.525, train AUC: 0.854\n",
      "Batch 20/30 | loss: 0.99350 (0.118s) | train acc: 0.581, train AUC: 0.885\n",
      "Batch 30/30 | loss: 1.04187 (0.118s) | train acc: 0.596, train AUC: 0.886\n",
      "Validation: val loss: 1.785 | val acc: 0.350 | val F1: 0.334 | val AUC: 0.710\n",
      "\n",
      "-----Epoch 53/100-----\n",
      "Batch 10/30 | loss: 1.15505 (0.118s) | train acc: 0.588, train AUC: 0.865\n",
      "Batch 20/30 | loss: 1.12487 (0.118s) | train acc: 0.581, train AUC: 0.866\n",
      "Batch 30/30 | loss: 1.19530 (0.118s) | train acc: 0.550, train AUC: 0.857\n",
      "Validation: val loss: 1.731 | val acc: 0.367 | val F1: 0.364 | val AUC: 0.726\n",
      "\n",
      "-----Epoch 54/100-----\n",
      "Batch 10/30 | loss: 1.08799 (0.118s) | train acc: 0.613, train AUC: 0.888\n",
      "Batch 20/30 | loss: 1.08545 (0.118s) | train acc: 0.606, train AUC: 0.888\n",
      "Batch 30/30 | loss: 1.09422 (0.118s) | train acc: 0.600, train AUC: 0.884\n",
      "Validation: val loss: 1.821 | val acc: 0.383 | val F1: 0.371 | val AUC: 0.707\n",
      "\n",
      "-----Epoch 55/100-----\n",
      "Batch 10/30 | loss: 1.01615 (0.118s) | train acc: 0.625, train AUC: 0.889\n",
      "Batch 20/30 | loss: 1.09868 (0.118s) | train acc: 0.575, train AUC: 0.884\n",
      "Batch 30/30 | loss: 1.33176 (0.118s) | train acc: 0.537, train AUC: 0.861\n",
      "Validation: val loss: 1.892 | val acc: 0.367 | val F1: 0.362 | val AUC: 0.683\n",
      "\n",
      "-----Epoch 56/100-----\n",
      "Batch 10/30 | loss: 1.00675 (0.118s) | train acc: 0.600, train AUC: 0.899\n",
      "Batch 20/30 | loss: 0.99253 (0.118s) | train acc: 0.637, train AUC: 0.903\n",
      "Batch 30/30 | loss: 1.07689 (0.118s) | train acc: 0.600, train AUC: 0.896\n",
      "Validation: val loss: 1.963 | val acc: 0.333 | val F1: 0.338 | val AUC: 0.677\n",
      "\n",
      "-----Epoch 57/100-----\n",
      "Batch 10/30 | loss: 0.99294 (0.118s) | train acc: 0.688, train AUC: 0.898\n",
      "Batch 20/30 | loss: 1.16775 (0.118s) | train acc: 0.613, train AUC: 0.875\n",
      "Batch 30/30 | loss: 1.01667 (0.119s) | train acc: 0.604, train AUC: 0.884\n",
      "Validation: val loss: 1.898 | val acc: 0.367 | val F1: 0.362 | val AUC: 0.691\n",
      "\n",
      "-----Epoch 58/100-----\n",
      "Batch 10/30 | loss: 0.92467 (0.118s) | train acc: 0.662, train AUC: 0.908\n",
      "Batch 20/30 | loss: 0.91806 (0.118s) | train acc: 0.675, train AUC: 0.918\n",
      "Batch 30/30 | loss: 1.20630 (0.118s) | train acc: 0.629, train AUC: 0.896\n",
      "Validation: val loss: 1.895 | val acc: 0.367 | val F1: 0.366 | val AUC: 0.707\n",
      "\n",
      "-----Epoch 59/100-----\n",
      "Batch 10/30 | loss: 1.02085 (0.118s) | train acc: 0.613, train AUC: 0.897\n",
      "Batch 20/30 | loss: 1.06650 (0.119s) | train acc: 0.600, train AUC: 0.890\n",
      "Batch 30/30 | loss: 1.12562 (0.118s) | train acc: 0.583, train AUC: 0.882\n",
      "Validation: val loss: 2.010 | val acc: 0.383 | val F1: 0.380 | val AUC: 0.673\n",
      "\n",
      "-----Epoch 60/100-----\n",
      "Batch 10/30 | loss: 1.04580 (0.118s) | train acc: 0.588, train AUC: 0.878\n",
      "Batch 20/30 | loss: 0.91806 (0.118s) | train acc: 0.625, train AUC: 0.896\n",
      "Batch 30/30 | loss: 1.18710 (0.118s) | train acc: 0.604, train AUC: 0.883\n",
      "Validation: val loss: 2.150 | val acc: 0.350 | val F1: 0.326 | val AUC: 0.644\n",
      "\n",
      "-----Epoch 61/100-----\n",
      "Batch 10/30 | loss: 0.87090 (0.118s) | train acc: 0.675, train AUC: 0.919\n",
      "Batch 20/30 | loss: 1.08667 (0.118s) | train acc: 0.600, train AUC: 0.902\n",
      "Batch 30/30 | loss: 0.92381 (0.119s) | train acc: 0.629, train AUC: 0.906\n",
      "Validation: val loss: 1.847 | val acc: 0.367 | val F1: 0.355 | val AUC: 0.698\n",
      "\n",
      "-----Epoch 62/100-----\n",
      "Batch 10/30 | loss: 0.87837 (0.118s) | train acc: 0.650, train AUC: 0.911\n",
      "Batch 20/30 | loss: 0.98941 (0.118s) | train acc: 0.650, train AUC: 0.908\n",
      "Batch 30/30 | loss: 1.06001 (0.119s) | train acc: 0.629, train AUC: 0.898\n",
      "Validation: val loss: 2.036 | val acc: 0.350 | val F1: 0.335 | val AUC: 0.673\n",
      "\n",
      "-----Epoch 63/100-----\n",
      "Batch 10/30 | loss: 0.94974 (0.118s) | train acc: 0.675, train AUC: 0.898\n",
      "Batch 20/30 | loss: 0.91025 (0.119s) | train acc: 0.662, train AUC: 0.912\n",
      "Batch 30/30 | loss: 1.01749 (0.119s) | train acc: 0.642, train AUC: 0.906\n",
      "Validation: val loss: 1.939 | val acc: 0.333 | val F1: 0.299 | val AUC: 0.685\n",
      "\n",
      "-----Epoch 64/100-----\n",
      "Batch 10/30 | loss: 1.00078 (0.119s) | train acc: 0.675, train AUC: 0.898\n",
      "Batch 20/30 | loss: 0.95346 (0.118s) | train acc: 0.669, train AUC: 0.902\n",
      "Batch 30/30 | loss: 1.10223 (0.118s) | train acc: 0.637, train AUC: 0.893\n",
      "Validation: val loss: 2.262 | val acc: 0.333 | val F1: 0.319 | val AUC: 0.623\n",
      "\n",
      "-----Epoch 65/100-----\n",
      "Batch 10/30 | loss: 0.83828 (0.118s) | train acc: 0.738, train AUC: 0.928\n",
      "Batch 20/30 | loss: 0.93759 (0.118s) | train acc: 0.706, train AUC: 0.923\n",
      "Batch 30/30 | loss: 0.95793 (0.118s) | train acc: 0.696, train AUC: 0.915\n",
      "Validation: val loss: 2.194 | val acc: 0.283 | val F1: 0.263 | val AUC: 0.635\n",
      "\n",
      "-----Epoch 66/100-----\n",
      "Batch 10/30 | loss: 0.75835 (0.118s) | train acc: 0.725, train AUC: 0.955\n",
      "Batch 20/30 | loss: 1.00418 (0.118s) | train acc: 0.681, train AUC: 0.924\n",
      "Batch 30/30 | loss: 1.03096 (0.119s) | train acc: 0.667, train AUC: 0.912\n",
      "Validation: val loss: 2.166 | val acc: 0.317 | val F1: 0.315 | val AUC: 0.614\n",
      "\n",
      "-----Epoch 67/100-----\n",
      "Batch 10/30 | loss: 0.99070 (0.118s) | train acc: 0.600, train AUC: 0.902\n",
      "Batch 20/30 | loss: 0.86585 (0.119s) | train acc: 0.644, train AUC: 0.917\n",
      "Batch 30/30 | loss: 0.99216 (0.118s) | train acc: 0.642, train AUC: 0.909\n",
      "Validation: val loss: 1.921 | val acc: 0.383 | val F1: 0.378 | val AUC: 0.706\n",
      "\n",
      "-----Epoch 68/100-----\n",
      "Batch 10/30 | loss: 0.99803 (0.118s) | train acc: 0.562, train AUC: 0.897\n",
      "Batch 20/30 | loss: 0.83039 (0.118s) | train acc: 0.619, train AUC: 0.914\n",
      "Batch 30/30 | loss: 1.06590 (0.118s) | train acc: 0.629, train AUC: 0.902\n",
      "Validation: val loss: 1.933 | val acc: 0.400 | val F1: 0.398 | val AUC: 0.686\n",
      "\n",
      "-----Epoch 69/100-----\n",
      "Batch 10/30 | loss: 0.84036 (0.118s) | train acc: 0.725, train AUC: 0.932\n",
      "Batch 20/30 | loss: 0.97128 (0.118s) | train acc: 0.669, train AUC: 0.923\n",
      "Batch 30/30 | loss: 1.04008 (0.119s) | train acc: 0.646, train AUC: 0.909\n",
      "Validation: val loss: 2.242 | val acc: 0.300 | val F1: 0.283 | val AUC: 0.638\n",
      "\n",
      "-----Epoch 70/100-----\n",
      "Batch 10/30 | loss: 0.97269 (0.119s) | train acc: 0.688, train AUC: 0.903\n",
      "Batch 20/30 | loss: 0.85862 (0.119s) | train acc: 0.669, train AUC: 0.914\n",
      "Batch 30/30 | loss: 1.01848 (0.118s) | train acc: 0.646, train AUC: 0.904\n",
      "Validation: val loss: 1.772 | val acc: 0.417 | val F1: 0.420 | val AUC: 0.709\n",
      "\n",
      "-----Epoch 71/100-----\n",
      "Batch 10/30 | loss: 0.90425 (0.118s) | train acc: 0.637, train AUC: 0.919\n",
      "Batch 20/30 | loss: 1.02860 (0.118s) | train acc: 0.613, train AUC: 0.906\n",
      "Batch 30/30 | loss: 0.79565 (0.118s) | train acc: 0.650, train AUC: 0.916\n",
      "Validation: val loss: 1.977 | val acc: 0.350 | val F1: 0.357 | val AUC: 0.667\n",
      "\n",
      "-----Epoch 72/100-----\n",
      "Batch 10/30 | loss: 0.96067 (0.118s) | train acc: 0.613, train AUC: 0.899\n",
      "Batch 20/30 | loss: 0.94260 (0.118s) | train acc: 0.594, train AUC: 0.904\n",
      "Batch 30/30 | loss: 0.69987 (0.118s) | train acc: 0.650, train AUC: 0.924\n",
      "Validation: val loss: 1.928 | val acc: 0.400 | val F1: 0.415 | val AUC: 0.689\n",
      "\n",
      "-----Epoch 73/100-----\n",
      "Batch 10/30 | loss: 0.76758 (0.118s) | train acc: 0.713, train AUC: 0.938\n",
      "Batch 20/30 | loss: 0.76627 (0.119s) | train acc: 0.725, train AUC: 0.938\n",
      "Batch 30/30 | loss: 0.83406 (0.118s) | train acc: 0.733, train AUC: 0.937\n",
      "Validation: val loss: 2.056 | val acc: 0.400 | val F1: 0.380 | val AUC: 0.701\n",
      "\n",
      "-----Epoch 74/100-----\n",
      "Batch 10/30 | loss: 0.87040 (0.118s) | train acc: 0.675, train AUC: 0.928\n",
      "Batch 20/30 | loss: 0.86331 (0.118s) | train acc: 0.669, train AUC: 0.919\n",
      "Batch 30/30 | loss: 0.95234 (0.118s) | train acc: 0.675, train AUC: 0.914\n",
      "Validation: val loss: 2.064 | val acc: 0.350 | val F1: 0.350 | val AUC: 0.680\n",
      "\n",
      "-----Epoch 75/100-----\n",
      "Batch 10/30 | loss: 0.56261 (0.118s) | train acc: 0.825, train AUC: 0.979\n",
      "Batch 20/30 | loss: 0.78944 (0.118s) | train acc: 0.781, train AUC: 0.956\n",
      "Batch 30/30 | loss: 0.91956 (0.119s) | train acc: 0.742, train AUC: 0.943\n",
      "Validation: val loss: 1.952 | val acc: 0.367 | val F1: 0.372 | val AUC: 0.691\n",
      "\n",
      "-----Epoch 76/100-----\n",
      "Batch 10/30 | loss: 0.85652 (0.118s) | train acc: 0.700, train AUC: 0.931\n",
      "Batch 20/30 | loss: 0.79696 (0.118s) | train acc: 0.700, train AUC: 0.934\n",
      "Batch 30/30 | loss: 0.83304 (0.118s) | train acc: 0.696, train AUC: 0.930\n",
      "Validation: val loss: 1.959 | val acc: 0.367 | val F1: 0.368 | val AUC: 0.692\n",
      "\n",
      "-----Epoch 77/100-----\n",
      "Batch 10/30 | loss: 0.75875 (0.119s) | train acc: 0.725, train AUC: 0.941\n",
      "Batch 20/30 | loss: 0.80490 (0.118s) | train acc: 0.706, train AUC: 0.940\n",
      "Batch 30/30 | loss: 0.92325 (0.118s) | train acc: 0.696, train AUC: 0.930\n",
      "Validation: val loss: 2.039 | val acc: 0.400 | val F1: 0.413 | val AUC: 0.682\n",
      "\n",
      "-----Epoch 78/100-----\n",
      "Batch 10/30 | loss: 0.69952 (0.118s) | train acc: 0.750, train AUC: 0.952\n",
      "Batch 20/30 | loss: 0.84149 (0.118s) | train acc: 0.719, train AUC: 0.941\n",
      "Batch 30/30 | loss: 0.74840 (0.118s) | train acc: 0.729, train AUC: 0.944\n",
      "Validation: val loss: 2.267 | val acc: 0.417 | val F1: 0.418 | val AUC: 0.673\n",
      "\n",
      "-----Epoch 79/100-----\n",
      "Batch 10/30 | loss: 0.77490 (0.118s) | train acc: 0.738, train AUC: 0.944\n",
      "Batch 20/30 | loss: 0.87620 (0.118s) | train acc: 0.706, train AUC: 0.933\n",
      "Batch 30/30 | loss: 0.67048 (0.118s) | train acc: 0.721, train AUC: 0.942\n",
      "Validation: val loss: 2.090 | val acc: 0.283 | val F1: 0.274 | val AUC: 0.675\n",
      "\n",
      "-----Epoch 80/100-----\n",
      "Batch 10/30 | loss: 0.65867 (0.118s) | train acc: 0.850, train AUC: 0.950\n",
      "Batch 20/30 | loss: 0.93496 (0.118s) | train acc: 0.738, train AUC: 0.928\n",
      "Batch 30/30 | loss: 0.79995 (0.118s) | train acc: 0.729, train AUC: 0.933\n",
      "Validation: val loss: 2.280 | val acc: 0.350 | val F1: 0.351 | val AUC: 0.682\n",
      "\n",
      "-----Epoch 81/100-----\n",
      "Batch 10/30 | loss: 0.73882 (0.118s) | train acc: 0.762, train AUC: 0.941\n",
      "Batch 20/30 | loss: 0.85881 (0.118s) | train acc: 0.719, train AUC: 0.931\n",
      "Batch 30/30 | loss: 0.68130 (0.118s) | train acc: 0.750, train AUC: 0.939\n",
      "Validation: val loss: 2.301 | val acc: 0.367 | val F1: 0.349 | val AUC: 0.664\n",
      "\n",
      "-----Epoch 82/100-----\n",
      "Batch 10/30 | loss: 0.61976 (0.118s) | train acc: 0.787, train AUC: 0.966\n",
      "Batch 20/30 | loss: 0.73308 (0.118s) | train acc: 0.750, train AUC: 0.953\n",
      "Batch 30/30 | loss: 0.75457 (0.118s) | train acc: 0.725, train AUC: 0.949\n",
      "Validation: val loss: 2.135 | val acc: 0.400 | val F1: 0.404 | val AUC: 0.687\n",
      "\n",
      "-----Epoch 83/100-----\n",
      "Batch 10/30 | loss: 0.55993 (0.118s) | train acc: 0.775, train AUC: 0.975\n",
      "Batch 20/30 | loss: 0.64580 (0.118s) | train acc: 0.762, train AUC: 0.968\n",
      "Batch 30/30 | loss: 0.73746 (0.118s) | train acc: 0.742, train AUC: 0.960\n",
      "Validation: val loss: 2.173 | val acc: 0.450 | val F1: 0.442 | val AUC: 0.700\n",
      "\n",
      "-----Epoch 84/100-----\n",
      "Batch 10/30 | loss: 0.53749 (0.118s) | train acc: 0.800, train AUC: 0.979\n",
      "Batch 20/30 | loss: 0.66923 (0.118s) | train acc: 0.787, train AUC: 0.966\n",
      "Batch 30/30 | loss: 0.65785 (0.118s) | train acc: 0.783, train AUC: 0.961\n",
      "Validation: val loss: 2.037 | val acc: 0.417 | val F1: 0.407 | val AUC: 0.731\n",
      "\n",
      "-----Epoch 85/100-----\n",
      "Batch 10/30 | loss: 0.79817 (0.118s) | train acc: 0.700, train AUC: 0.930\n",
      "Batch 20/30 | loss: 0.52381 (0.118s) | train acc: 0.769, train AUC: 0.952\n",
      "Batch 30/30 | loss: 0.92229 (0.118s) | train acc: 0.742, train AUC: 0.940\n",
      "Validation: val loss: 1.951 | val acc: 0.450 | val F1: 0.450 | val AUC: 0.716\n",
      "\n",
      "-----Epoch 86/100-----\n",
      "Batch 10/30 | loss: 0.74177 (0.118s) | train acc: 0.713, train AUC: 0.941\n",
      "Batch 20/30 | loss: 0.52265 (0.118s) | train acc: 0.769, train AUC: 0.962\n",
      "Batch 30/30 | loss: 0.87466 (0.118s) | train acc: 0.729, train AUC: 0.948\n",
      "Validation: val loss: 1.996 | val acc: 0.383 | val F1: 0.388 | val AUC: 0.705\n",
      "\n",
      "-----Epoch 87/100-----\n",
      "Batch 10/30 | loss: 0.83544 (0.118s) | train acc: 0.750, train AUC: 0.928\n",
      "Batch 20/30 | loss: 0.63181 (0.118s) | train acc: 0.750, train AUC: 0.944\n",
      "Batch 30/30 | loss: 0.94009 (0.118s) | train acc: 0.721, train AUC: 0.932\n",
      "Validation: val loss: 2.222 | val acc: 0.367 | val F1: 0.351 | val AUC: 0.668\n",
      "\n",
      "-----Epoch 88/100-----\n",
      "Batch 10/30 | loss: 0.62804 (0.118s) | train acc: 0.775, train AUC: 0.957\n",
      "Batch 20/30 | loss: 0.59933 (0.118s) | train acc: 0.775, train AUC: 0.960\n",
      "Batch 30/30 | loss: 0.94814 (0.118s) | train acc: 0.750, train AUC: 0.946\n",
      "Validation: val loss: 2.245 | val acc: 0.417 | val F1: 0.409 | val AUC: 0.682\n",
      "\n",
      "-----Epoch 89/100-----\n",
      "Batch 10/30 | loss: 0.77118 (0.118s) | train acc: 0.762, train AUC: 0.944\n",
      "Batch 20/30 | loss: 0.65755 (0.118s) | train acc: 0.762, train AUC: 0.948\n",
      "Batch 30/30 | loss: 0.78663 (0.118s) | train acc: 0.758, train AUC: 0.944\n",
      "Validation: val loss: 2.494 | val acc: 0.383 | val F1: 0.380 | val AUC: 0.664\n",
      "\n",
      "-----Epoch 90/100-----\n",
      "Batch 10/30 | loss: 0.64767 (0.118s) | train acc: 0.800, train AUC: 0.953\n",
      "Batch 20/30 | loss: 0.65569 (0.118s) | train acc: 0.800, train AUC: 0.956\n",
      "Batch 30/30 | loss: 0.59996 (0.118s) | train acc: 0.800, train AUC: 0.957\n",
      "Validation: val loss: 2.030 | val acc: 0.450 | val F1: 0.459 | val AUC: 0.689\n",
      "\n",
      "-----Epoch 91/100-----\n",
      "Batch 10/30 | loss: 0.61379 (0.118s) | train acc: 0.750, train AUC: 0.963\n",
      "Batch 20/30 | loss: 0.57878 (0.118s) | train acc: 0.781, train AUC: 0.966\n",
      "Batch 30/30 | loss: 0.87927 (0.118s) | train acc: 0.746, train AUC: 0.950\n",
      "Validation: val loss: 2.174 | val acc: 0.367 | val F1: 0.370 | val AUC: 0.683\n",
      "\n",
      "-----Epoch 92/100-----\n",
      "Batch 10/30 | loss: 0.60055 (0.118s) | train acc: 0.787, train AUC: 0.967\n",
      "Batch 20/30 | loss: 0.70661 (0.118s) | train acc: 0.800, train AUC: 0.952\n",
      "Batch 30/30 | loss: 0.67493 (0.118s) | train acc: 0.779, train AUC: 0.952\n",
      "Validation: val loss: 1.987 | val acc: 0.433 | val F1: 0.425 | val AUC: 0.693\n",
      "\n",
      "-----Epoch 93/100-----\n",
      "Batch 10/30 | loss: 0.63386 (0.118s) | train acc: 0.775, train AUC: 0.958\n",
      "Batch 20/30 | loss: 0.48817 (0.118s) | train acc: 0.806, train AUC: 0.970\n",
      "Batch 30/30 | loss: 0.67374 (0.118s) | train acc: 0.792, train AUC: 0.965\n",
      "Validation: val loss: 2.127 | val acc: 0.383 | val F1: 0.381 | val AUC: 0.703\n",
      "\n",
      "-----Epoch 94/100-----\n",
      "Batch 10/30 | loss: 0.59345 (0.118s) | train acc: 0.800, train AUC: 0.970\n",
      "Batch 20/30 | loss: 0.58664 (0.118s) | train acc: 0.794, train AUC: 0.968\n",
      "Batch 30/30 | loss: 0.60810 (0.118s) | train acc: 0.796, train AUC: 0.964\n",
      "Validation: val loss: 2.135 | val acc: 0.350 | val F1: 0.325 | val AUC: 0.688\n",
      "\n",
      "-----Epoch 95/100-----\n",
      "Batch 10/30 | loss: 0.48655 (0.118s) | train acc: 0.863, train AUC: 0.981\n",
      "Batch 20/30 | loss: 0.47595 (0.118s) | train acc: 0.856, train AUC: 0.980\n",
      "Batch 30/30 | loss: 0.54894 (0.118s) | train acc: 0.838, train AUC: 0.976\n",
      "Validation: val loss: 2.217 | val acc: 0.467 | val F1: 0.464 | val AUC: 0.673\n",
      "\n",
      "-----Epoch 96/100-----\n",
      "Batch 10/30 | loss: 0.57658 (0.118s) | train acc: 0.775, train AUC: 0.966\n",
      "Batch 20/30 | loss: 0.42096 (0.118s) | train acc: 0.825, train AUC: 0.978\n",
      "Batch 30/30 | loss: 0.62184 (0.118s) | train acc: 0.812, train AUC: 0.971\n",
      "Validation: val loss: 2.213 | val acc: 0.450 | val F1: 0.447 | val AUC: 0.698\n",
      "\n",
      "-----Epoch 97/100-----\n",
      "Batch 10/30 | loss: 0.55613 (0.118s) | train acc: 0.800, train AUC: 0.965\n",
      "Batch 20/30 | loss: 0.59698 (0.118s) | train acc: 0.806, train AUC: 0.960\n",
      "Batch 30/30 | loss: 0.75705 (0.118s) | train acc: 0.792, train AUC: 0.955\n",
      "Validation: val loss: 2.862 | val acc: 0.317 | val F1: 0.285 | val AUC: 0.614\n",
      "\n",
      "-----Epoch 98/100-----\n",
      "Batch 10/30 | loss: 0.62603 (0.118s) | train acc: 0.800, train AUC: 0.958\n",
      "Batch 20/30 | loss: 0.41989 (0.118s) | train acc: 0.831, train AUC: 0.972\n",
      "Batch 30/30 | loss: 0.61728 (0.118s) | train acc: 0.800, train AUC: 0.969\n",
      "Validation: val loss: 2.283 | val acc: 0.417 | val F1: 0.392 | val AUC: 0.698\n",
      "\n",
      "-----Epoch 99/100-----\n",
      "Batch 10/30 | loss: 0.55825 (0.118s) | train acc: 0.762, train AUC: 0.969\n",
      "Batch 20/30 | loss: 0.56497 (0.118s) | train acc: 0.787, train AUC: 0.966\n",
      "Batch 30/30 | loss: 0.64977 (0.118s) | train acc: 0.787, train AUC: 0.964\n",
      "Validation: val loss: 2.593 | val acc: 0.300 | val F1: 0.297 | val AUC: 0.631\n",
      "\n",
      "-----Epoch 100/100-----\n",
      "Batch 10/30 | loss: 0.48889 (0.118s) | train acc: 0.838, train AUC: 0.976\n",
      "Batch 20/30 | loss: 0.53856 (0.117s) | train acc: 0.838, train AUC: 0.974\n",
      "Batch 30/30 | loss: 0.48760 (0.118s) | train acc: 0.833, train AUC: 0.976\n",
      "Validation: val loss: 2.045 | val acc: 0.400 | val F1: 0.401 | val AUC: 0.723\n"
     ]
    }
   ],
   "source": [
    "# Create and train shadow model\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "weight_decay = 1e-4\n",
    "s_model = GATProteinsModel(num_feat=num_feat, num_classes=n_categories).to(DEVICE)\n",
    "optimizer = optim.Adam(s_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "s_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                       factor=0.5,\n",
    "                                                       patience=50,\n",
    "                                                       min_lr=1e-6,\n",
    "                                                       verbose=True)\n",
    "weight = compute_class_weight('balanced', classes=np.unique(s_dataset_train.y.argmax(dim=1)), y=s_dataset_train.y.argmax(dim=1).numpy())\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weight).to(DEVICE))\n",
    "# s_save_path = 'mia-models/s_model_proteins.pth'\n",
    "s_save_path = None\n",
    "\n",
    "train_model_multi_graph(s_model, optimizer, s_dataset_train, loss_fn, epochs, batch_size, val_dataset=s_dataset_test, save_path=s_save_path, save_freq=10, device=DEVICE)\n",
    "# s_model = load_model(s_model, s_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfd41411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_scaler(s_model, s_dataset_train, s_dataset_test, n_perturb_per_graph=1000, scaler_min=0.1, scaler_max=1):\n",
    "    for scaler in np.arange(scaler_min, scaler_max, 0.1):\n",
    "        scores_train = calculate_robustness_scores(s_model, s_dataset_train, n_perturb_per_graph=n_perturb_per_graph, scaler=scaler, device=DEVICE)\n",
    "        scores_test = calculate_robustness_scores(s_model, s_dataset_test, n_perturb_per_graph=n_perturb_per_graph, scaler=scaler, device=DEVICE)\n",
    "        \n",
    "        is_member = np.concatenate((np.ones_like(scores_train), np.zeros_like(scores_test)))\n",
    "        robust_scores = np.concatenate((scores_train, scores_test))\n",
    "        fpr, tpr, thresholds = roc_curve(is_member, robust_scores)\n",
    "\n",
    "#         j = tpr-fpr\n",
    "#         t = thresholds[j.argmax()]\n",
    "        t = max(thresholds, key=lambda x: roc_auc_score(is_member, robust_scores>x))\n",
    "        \n",
    "        pred_member = (robust_scores > t).astype(int)\n",
    "        acc = accuracy_score(is_member, pred_member)\n",
    "        auroc = roc_auc_score(is_member, robust_scores)\n",
    "        f1 = f1_score(is_member, pred_member)\n",
    "        print(f's={scaler:.3f}:  t={t:.4f}, acc={acc:.4f}, AUC={auroc:.4f}, F1={f1:.4f}')\n",
    "        \n",
    "        \n",
    "def pred_attack(model, dataset, threshold):\n",
    "    '''\n",
    "    Get predictions for membership inference predictions for a model.\n",
    "    \n",
    "    model: the target model\n",
    "    dataset: A dataset to perform membership inference with. That is, predict which examples in it were used to train the model\n",
    "    threshold: Robustness threshold; graphs above this threshold are predicted to be a member\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96d00d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s=0.100:  t=inf, acc=0.2000, AUC=0.7677, F1=0.0000\n",
      "s=0.200:  t=inf, acc=0.2000, AUC=0.8289, F1=0.0000\n",
      "s=0.300:  t=inf, acc=0.2000, AUC=0.8252, F1=0.0000\n",
      "s=0.400:  t=inf, acc=0.2000, AUC=0.8026, F1=0.0000\n",
      "s=0.500:  t=inf, acc=0.2000, AUC=0.7926, F1=0.0000\n",
      "s=0.600:  t=inf, acc=0.2000, AUC=0.7754, F1=0.0000\n",
      "s=0.700:  t=inf, acc=0.2000, AUC=0.7585, F1=0.0000\n",
      "s=0.800:  t=inf, acc=0.2000, AUC=0.7307, F1=0.0000\n",
      "s=0.900:  t=inf, acc=0.2000, AUC=0.6988, F1=0.0000\n",
      "s=1.000:  t=inf, acc=0.2000, AUC=0.6578, F1=0.0000\n",
      "s=1.100:  t=inf, acc=0.2000, AUC=0.6203, F1=0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msearch_scaler\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_dataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_dataset_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_perturb_per_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m, in \u001b[0;36msearch_scaler\u001b[0;34m(s_model, s_dataset_train, s_dataset_test, n_perturb_per_graph, scaler_min, scaler_max)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scaler \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(scaler_min, scaler_max, \u001b[38;5;241m0.1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     scores_train \u001b[38;5;241m=\u001b[39m calculate_robustness_scores(s_model, s_dataset_train, n_perturb_per_graph\u001b[38;5;241m=\u001b[39mn_perturb_per_graph, scaler\u001b[38;5;241m=\u001b[39mscaler, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m----> 4\u001b[0m     scores_test \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_robustness_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_dataset_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_perturb_per_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_perturb_per_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     is_member \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mones_like(scores_train), np\u001b[38;5;241m.\u001b[39mzeros_like(scores_test)))\n\u001b[1;32m      7\u001b[0m     robust_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((scores_train, scores_test))\n",
      "File \u001b[0;32m~/CSE-8803-MLG/biomedical-gnn-privacy/membership_inference_attack/util.py:138\u001b[0m, in \u001b[0;36mcalculate_robustness_scores\u001b[0;34m(model, dataset, n_perturb_per_graph, scaler, device)\u001b[0m\n\u001b[1;32m    136\u001b[0m         gbatch \u001b[38;5;241m=\u001b[39m gbatch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    137\u001b[0m         x_p \u001b[38;5;241m=\u001b[39m create_perturbed_graphs(gbatch\u001b[38;5;241m.\u001b[39mx, num\u001b[38;5;241m=\u001b[39mn_perturb_per_graph, scaler\u001b[38;5;241m=\u001b[39mscaler, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 138\u001b[0m         pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([model(x_pi, gbatch\u001b[38;5;241m.\u001b[39medge_index, gbatch\u001b[38;5;241m.\u001b[39mbatch)\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;28;01mfor\u001b[39;00m x_pi \u001b[38;5;129;01min\u001b[39;00m x_p])\n\u001b[1;32m    139\u001b[0m         scores\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39matleast_1d((pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m gbatch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(scores)\n",
      "File \u001b[0;32m~/CSE-8803-MLG/biomedical-gnn-privacy/membership_inference_attack/util.py:138\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m         gbatch \u001b[38;5;241m=\u001b[39m gbatch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    137\u001b[0m         x_p \u001b[38;5;241m=\u001b[39m create_perturbed_graphs(gbatch\u001b[38;5;241m.\u001b[39mx, num\u001b[38;5;241m=\u001b[39mn_perturb_per_graph, scaler\u001b[38;5;241m=\u001b[39mscaler, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 138\u001b[0m         pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_pi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;28;01mfor\u001b[39;00m x_pi \u001b[38;5;129;01min\u001b[39;00m x_p])\n\u001b[1;32m    139\u001b[0m         scores\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39matleast_1d((pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m gbatch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(scores)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/CSE-8803-MLG/biomedical-gnn-privacy/membership_inference_attack/ml_util.py:394\u001b[0m, in \u001b[0;36mGATProteinsModel.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m    392\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, gat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat_layers):\n\u001b[0;32m--> 394\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mgat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm:\n\u001b[1;32m    396\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbns[i](out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gat_conv.py:362\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_updater\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, alpha\u001b[38;5;241m=\u001b[39malpha, size\u001b[38;5;241m=\u001b[39msize)\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gat_conv_GATConv_edge_updater_qumx6641.py:176\u001b[0m, in \u001b[0;36medge_updater\u001b[0;34m(self, edge_index, alpha, edge_attr, size)\u001b[0m\n\u001b[1;32m    166\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    167\u001b[0m                 alpha_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    168\u001b[0m                 alpha_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    173\u001b[0m             )\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# End Edge Update Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Begin Edge Update Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/nn/conv/gat_conv.py:409\u001b[0m, in \u001b[0;36mGATConv.edge_update\u001b[0;34m(self, alpha_j, alpha_i, edge_attr, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    406\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m+\u001b[39m alpha_edge\n\u001b[1;32m    408\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(alpha, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slope)\n\u001b[0;32m--> 409\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(alpha, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/utils/_softmax.py:78\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(src, index, ptr, num_nodes, dim)\u001b[0m\n\u001b[1;32m     76\u001b[0m     out \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m-\u001b[39m src_max\u001b[38;5;241m.\u001b[39mindex_select(dim, index)\n\u001b[1;32m     77\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m---> 78\u001b[0m     out_sum \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-16\u001b[39m\n\u001b[1;32m     79\u001b[0m     out_sum \u001b[38;5;241m=\u001b[39m out_sum\u001b[38;5;241m.\u001b[39mindex_select(dim, index)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch_geometric/utils/_scatter.py:70\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     69\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_scaler(s_model, s_dataset_train, s_dataset_test, n_perturb_per_graph=1000, scaler_max=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dcf0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.3\n",
    "threshold = 0.8920\n",
    "pred_scores_train = calculate_robustness_scores(t_model, t_dataset_train, scaler=s, device=DEVICE)\n",
    "pred_scores_test = calculate_robustness_scores(t_model, t_dataset_test, scaler=s, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f3442bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7936507936507936\n"
     ]
    }
   ],
   "source": [
    "pred_scores = np.concatenate([pred_scores_train, pred_scores_test])\n",
    "true_scores = np.concatenate([np.ones_like(pred_scores_train), np.zeros_like(pred_scores_test)])\n",
    "\n",
    "print(f1_score(true_scores, pred_scores>threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "816f6a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(true_scores, pred_scores>threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f644a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8247111111111112\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(true_scores, pred_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c436975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x1552007c3c10>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "fpr, tpr, thresholds = roc_curve(true_scores, pred_scores)\n",
    "\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c31fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7397d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
