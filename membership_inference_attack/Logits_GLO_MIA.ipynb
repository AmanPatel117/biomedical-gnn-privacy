{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfbde8e",
   "metadata": {},
   "source": [
    "# GLO-MIA adapted to use logits instead of label only\n",
    "Relaxes the constraint that we only get labels, providing logits or probabilities to the model. (Is getting probabilities back a realistic situation?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c662fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5addc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch_geometric.loader import DataLoader as GDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from ml_util import CustomGATModel, train_model_multi_graph, train_model, load_model, predict, get_auroc_score\n",
    "from util import onehot_transform, graph_train_test_split, create_perturbed_graphs\n",
    "from train_models import get_dataset, shadow_target_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdbc1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = ('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e930acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ogbg-molhiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "475cb695",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(dataset_name)\n",
    "num_feat = dataset[0].x.shape[1]\n",
    "num_categories = dataset[0].y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71d53d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset_train, t_dataset_test, s_dataset_train, s_dataset_test = shadow_target_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "964918b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fetch target model\n",
    "t_model = CustomGATModel(num_feat=num_feat, num_classes=num_categories).to(DEVICE)\n",
    "t_save_path = f'mia-models/t_model_gat_{dataset_name}.pth'\n",
    "\n",
    "t_model, t_dataset_train, t_dataset_test = load_model(t_model, t_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39ca10c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fetch shadow model\n",
    "s_model = CustomGATModel(num_feat=num_feat, num_classes=num_categories).to(DEVICE)\n",
    "s_save_path = f'mia-models/s_model_gat_{dataset_name}.pth'\n",
    "\n",
    "s_model, s_dataset_train, s_dataset_test = load_model(s_model, s_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77da6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attack_dataset(model, train_dataset, test_dataset, scaler=0.4, n_perturb_per_graph=10, device='cpu'):\n",
    "    '''Create an attack dataset containing logits that are generated from perturbed graphs'''\n",
    "    \n",
    "    model.eval()\n",
    "    chunksize = 250\n",
    "    train_loader = GDataLoader(train_dataset, batch_size=chunksize, shuffle=False)\n",
    "    test_loader = GDataLoader(test_dataset, batch_size=chunksize, shuffle=False)\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Enumerate over each dataset\n",
    "        for i, loader in enumerate((train_loader, test_loader)):\n",
    "            label = [1,0] if i == 0 else [0,1]\n",
    "            for i, gbatch in enumerate(loader):\n",
    "                gbatch = gbatch.to(device)\n",
    "                x_p = create_perturbed_graphs(gbatch.x, num=n_perturb_per_graph, scaler=scaler, device=device).to(device)\n",
    "                pred = torch.stack([model(x_pi, gbatch.edge_index, gbatch.batch).squeeze() for x_pi in x_p])\n",
    "                all_logits.append((pred.flatten(end_dim=1).sort(dim=1)[0]))\n",
    "                all_labels.append(torch.Tensor([label] * n_perturb_per_graph * len(gbatch)))        \n",
    "        \n",
    "            \n",
    "    return torch.cat(all_logits), torch.cat(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "049d0c54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mF\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "788f93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericAttackModel(nn.Module):\n",
    "    def __init__(self, num_feat):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_feat, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e68a34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_logits, s_labels = create_attack_dataset(s_model, s_dataset_train, s_dataset_test, scaler=1, device=DEVICE)\n",
    "t_logits, t_labels = create_attack_dataset(t_model, t_dataset_train, t_dataset_test, scaler=1, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ec1a356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.002\n",
      "No learning rate scheduling!\n",
      "Training for 150 epochs, with batch size=32\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/150-----\n",
      "Batch 50/6427 | loss: 0.7057300090789795 (0.068s) | train acc: 0.5594 | train auc: 0.492446\n",
      "Batch 100/6427 | loss: 0.6821008622646332 (0.066s) | train acc: 0.5813 | train auc: 0.492618\n",
      "Batch 150/6427 | loss: 0.6890039873123169 (0.066s) | train acc: 0.5842 | train auc: 0.492127\n",
      "Batch 200/6427 | loss: 0.6775495886802674 (0.066s) | train acc: 0.5887 | train auc: 0.491620\n",
      "Batch 250/6427 | loss: 0.6854936265945435 (0.066s) | train acc: 0.5889 | train auc: 0.489709\n",
      "Batch 300/6427 | loss: 0.6816746819019318 (0.066s) | train acc: 0.5889 | train auc: 0.488490\n",
      "Batch 350/6427 | loss: 0.6710280823707581 (0.066s) | train acc: 0.5929 | train auc: 0.489173\n",
      "Batch 400/6427 | loss: 0.673120174407959 (0.066s) | train acc: 0.5947 | train auc: 0.489050\n",
      "Batch 450/6427 | loss: 0.6799440717697144 (0.066s) | train acc: 0.5948 | train auc: 0.490677\n",
      "Batch 500/6427 | loss: 0.6757307815551757 (0.067s) | train acc: 0.5949 | train auc: 0.491658\n",
      "Batch 550/6427 | loss: 0.6792677295207977 (0.066s) | train acc: 0.5949 | train auc: 0.491132\n",
      "Batch 600/6427 | loss: 0.6870257830619813 (0.066s) | train acc: 0.5935 | train auc: 0.491594\n",
      "Batch 650/6427 | loss: 0.6814472913742066 (0.066s) | train acc: 0.5927 | train auc: 0.491732\n",
      "Batch 700/6427 | loss: 0.6665107703208923 (0.066s) | train acc: 0.5951 | train auc: 0.492496\n",
      "Batch 750/6427 | loss: 0.6788629484176636 (0.066s) | train acc: 0.5951 | train auc: 0.492360\n",
      "Batch 800/6427 | loss: 0.673316035270691 (0.066s) | train acc: 0.5953 | train auc: 0.492820\n",
      "Batch 850/6427 | loss: 0.6631711983680725 (0.066s) | train acc: 0.5973 | train auc: 0.494150\n",
      "Batch 900/6427 | loss: 0.6763538789749145 (0.066s) | train acc: 0.5974 | train auc: 0.494405\n",
      "Batch 950/6427 | loss: 0.6738529872894287 (0.066s) | train acc: 0.5976 | train auc: 0.494779\n",
      "Batch 1000/6427 | loss: 0.6725066447257996 (0.066s) | train acc: 0.5979 | train auc: 0.495424\n",
      "Batch 1050/6427 | loss: 0.6794572496414184 (0.066s) | train acc: 0.5975 | train auc: 0.495101\n",
      "Batch 1100/6427 | loss: 0.6805387496948242 (0.066s) | train acc: 0.5969 | train auc: 0.495981\n",
      "Batch 1150/6427 | loss: 0.6719991552829743 (0.066s) | train acc: 0.5975 | train auc: 0.495386\n",
      "Batch 1200/6427 | loss: 0.6790614867210388 (0.066s) | train acc: 0.5969 | train auc: 0.496080\n",
      "Batch 1250/6427 | loss: 0.6773488450050354 (0.066s) | train acc: 0.5967 | train auc: 0.496762\n",
      "Batch 1300/6427 | loss: 0.677997008562088 (0.066s) | train acc: 0.5966 | train auc: 0.496614\n",
      "Batch 1350/6427 | loss: 0.6732962024211884 (0.066s) | train acc: 0.5969 | train auc: 0.497065\n",
      "Batch 1400/6427 | loss: 0.6714195549488068 (0.066s) | train acc: 0.5973 | train auc: 0.497164\n",
      "Batch 1450/6427 | loss: 0.6755002856254577 (0.066s) | train acc: 0.5974 | train auc: 0.496633\n",
      "Batch 1500/6427 | loss: 0.6680686795711517 (0.066s) | train acc: 0.5979 | train auc: 0.496793\n",
      "Batch 1550/6427 | loss: 0.6746148633956909 (0.066s) | train acc: 0.5981 | train auc: 0.496789\n",
      "Batch 1600/6427 | loss: 0.6764657509326935 (0.066s) | train acc: 0.5980 | train auc: 0.496654\n",
      "Batch 1650/6427 | loss: 0.6798748564720154 (0.066s) | train acc: 0.5976 | train auc: 0.496728\n",
      "Batch 1700/6427 | loss: 0.6753488600254058 (0.066s) | train acc: 0.5977 | train auc: 0.496467\n",
      "Batch 1750/6427 | loss: 0.6816792929172516 (0.066s) | train acc: 0.5972 | train auc: 0.496605\n",
      "Batch 1800/6427 | loss: 0.6795845675468445 (0.066s) | train acc: 0.5969 | train auc: 0.496516\n",
      "Batch 1850/6427 | loss: 0.6659662771224976 (0.066s) | train acc: 0.5975 | train auc: 0.496896\n",
      "Batch 1900/6427 | loss: 0.6702129018306732 (0.066s) | train acc: 0.5978 | train auc: 0.497169\n",
      "Batch 1950/6427 | loss: 0.6747940373420716 (0.066s) | train acc: 0.5978 | train auc: 0.496899\n",
      "Batch 2000/6427 | loss: 0.673301236629486 (0.066s) | train acc: 0.5980 | train auc: 0.496545\n",
      "Batch 2050/6427 | loss: 0.6706669878959656 (0.066s) | train acc: 0.5982 | train auc: 0.496510\n",
      "Batch 2100/6427 | loss: 0.6723685979843139 (0.066s) | train acc: 0.5983 | train auc: 0.496898\n",
      "Batch 2150/6427 | loss: 0.6769122469425202 (0.066s) | train acc: 0.5982 | train auc: 0.496836\n",
      "Batch 2200/6427 | loss: 0.6761290204524993 (0.066s) | train acc: 0.5981 | train auc: 0.496846\n",
      "Batch 2250/6427 | loss: 0.6803089225292206 (0.066s) | train acc: 0.5978 | train auc: 0.497010\n",
      "Batch 2300/6427 | loss: 0.6746390008926392 (0.066s) | train acc: 0.5978 | train auc: 0.496792\n",
      "Batch 2350/6427 | loss: 0.6775581514835358 (0.066s) | train acc: 0.5976 | train auc: 0.497194\n",
      "Batch 2400/6427 | loss: 0.6762548100948333 (0.066s) | train acc: 0.5975 | train auc: 0.497308\n",
      "Batch 2450/6427 | loss: 0.675562914609909 (0.066s) | train acc: 0.5974 | train auc: 0.497391\n",
      "Batch 2500/6427 | loss: 0.6796094346046447 (0.066s) | train acc: 0.5971 | train auc: 0.497562\n",
      "Batch 2550/6427 | loss: 0.6771015405654908 (0.066s) | train acc: 0.5970 | train auc: 0.497552\n",
      "Batch 2600/6427 | loss: 0.670012639760971 (0.066s) | train acc: 0.5973 | train auc: 0.497543\n",
      "Batch 2650/6427 | loss: 0.6716298413276672 (0.066s) | train acc: 0.5974 | train auc: 0.497604\n",
      "Batch 2700/6427 | loss: 0.6656524872779847 (0.066s) | train acc: 0.5977 | train auc: 0.497987\n",
      "Batch 2750/6427 | loss: 0.6690428340435028 (0.066s) | train acc: 0.5980 | train auc: 0.498115\n",
      "Batch 2800/6427 | loss: 0.6866817402839661 (0.066s) | train acc: 0.5974 | train auc: 0.498093\n",
      "Batch 2850/6427 | loss: 0.6769173789024353 (0.066s) | train acc: 0.5973 | train auc: 0.498079\n",
      "Batch 2900/6427 | loss: 0.6762753844261169 (0.066s) | train acc: 0.5972 | train auc: 0.498101\n",
      "Batch 2950/6427 | loss: 0.6761737775802612 (0.066s) | train acc: 0.5971 | train auc: 0.498398\n",
      "Batch 3000/6427 | loss: 0.6699901926517486 (0.066s) | train acc: 0.5974 | train auc: 0.498227\n",
      "Batch 3050/6427 | loss: 0.6787798380851746 (0.066s) | train acc: 0.5972 | train auc: 0.498048\n",
      "Batch 3100/6427 | loss: 0.6691355621814727 (0.066s) | train acc: 0.5974 | train auc: 0.498064\n",
      "Batch 3150/6427 | loss: 0.6750626635551452 (0.066s) | train acc: 0.5974 | train auc: 0.498009\n",
      "Batch 3200/6427 | loss: 0.675530561208725 (0.066s) | train acc: 0.5973 | train auc: 0.497944\n",
      "Batch 3250/6427 | loss: 0.6658456814289093 (0.066s) | train acc: 0.5977 | train auc: 0.497962\n",
      "Batch 3300/6427 | loss: 0.6700809240341187 (0.066s) | train acc: 0.5978 | train auc: 0.498131\n",
      "Batch 3350/6427 | loss: 0.6729893517494202 (0.066s) | train acc: 0.5978 | train auc: 0.498233\n",
      "Batch 3400/6427 | loss: 0.6669128930568695 (0.066s) | train acc: 0.5981 | train auc: 0.498406\n",
      "Batch 3450/6427 | loss: 0.6785215616226197 (0.066s) | train acc: 0.5979 | train auc: 0.498277\n",
      "Batch 3500/6427 | loss: 0.6758321726322174 (0.066s) | train acc: 0.5979 | train auc: 0.498197\n",
      "Batch 3550/6427 | loss: 0.6734424412250519 (0.066s) | train acc: 0.5979 | train auc: 0.498160\n",
      "Batch 3600/6427 | loss: 0.6759194505214691 (0.066s) | train acc: 0.5978 | train auc: 0.498071\n",
      "Batch 3650/6427 | loss: 0.668607314825058 (0.066s) | train acc: 0.5980 | train auc: 0.498076\n",
      "Batch 3700/6427 | loss: 0.6637147283554077 (0.066s) | train acc: 0.5983 | train auc: 0.498413\n",
      "Batch 3750/6427 | loss: 0.6613795197010041 (0.066s) | train acc: 0.5987 | train auc: 0.498798\n",
      "Batch 3800/6427 | loss: 0.6683793437480926 (0.066s) | train acc: 0.5989 | train auc: 0.498994\n",
      "Batch 3850/6427 | loss: 0.6731816852092742 (0.066s) | train acc: 0.5989 | train auc: 0.499031\n",
      "Batch 3900/6427 | loss: 0.6683305943012238 (0.066s) | train acc: 0.5991 | train auc: 0.499141\n",
      "Batch 3950/6427 | loss: 0.6749903786182404 (0.066s) | train acc: 0.5991 | train auc: 0.499002\n",
      "Batch 4000/6427 | loss: 0.6661479926109314 (0.066s) | train acc: 0.5993 | train auc: 0.499076\n",
      "Batch 4050/6427 | loss: 0.6767432034015656 (0.066s) | train acc: 0.5992 | train auc: 0.498975\n",
      "Batch 4100/6427 | loss: 0.6850298726558686 (0.066s) | train acc: 0.5988 | train auc: 0.499083\n",
      "Batch 4150/6427 | loss: 0.6713882124423981 (0.066s) | train acc: 0.5989 | train auc: 0.498964\n",
      "Batch 4200/6427 | loss: 0.6774829840660095 (0.066s) | train acc: 0.5988 | train auc: 0.499051\n",
      "Batch 4250/6427 | loss: 0.6802553606033325 (0.066s) | train acc: 0.5986 | train auc: 0.499280\n",
      "Batch 4300/6427 | loss: 0.6784130620956421 (0.066s) | train acc: 0.5984 | train auc: 0.499444\n",
      "Batch 4350/6427 | loss: 0.6691995692253113 (0.066s) | train acc: 0.5986 | train auc: 0.499430\n",
      "Batch 4400/6427 | loss: 0.6747309768199921 (0.066s) | train acc: 0.5985 | train auc: 0.499443\n",
      "Batch 4450/6427 | loss: 0.6710624349117279 (0.066s) | train acc: 0.5986 | train auc: 0.499459\n",
      "Batch 4500/6427 | loss: 0.6744057369232178 (0.066s) | train acc: 0.5986 | train auc: 0.499436\n",
      "Batch 4550/6427 | loss: 0.6777568185329437 (0.066s) | train acc: 0.5985 | train auc: 0.499451\n",
      "Batch 4600/6427 | loss: 0.6729878067970276 (0.066s) | train acc: 0.5985 | train auc: 0.499334\n",
      "Batch 4650/6427 | loss: 0.6689955449104309 (0.066s) | train acc: 0.5986 | train auc: 0.499310\n",
      "Batch 4700/6427 | loss: 0.6794803130626679 (0.066s) | train acc: 0.5985 | train auc: 0.499250\n",
      "Batch 4750/6427 | loss: 0.6774907505512238 (0.066s) | train acc: 0.5984 | train auc: 0.499269\n",
      "Batch 4800/6427 | loss: 0.6664449965953827 (0.066s) | train acc: 0.5986 | train auc: 0.499188\n",
      "Batch 4850/6427 | loss: 0.6704592859745026 (0.066s) | train acc: 0.5987 | train auc: 0.499228\n",
      "Batch 4900/6427 | loss: 0.674247385263443 (0.066s) | train acc: 0.5986 | train auc: 0.499188\n",
      "Batch 4950/6427 | loss: 0.6799672341346741 (0.066s) | train acc: 0.5985 | train auc: 0.499198\n",
      "Batch 5000/6427 | loss: 0.6737144899368286 (0.066s) | train acc: 0.5985 | train auc: 0.499200\n",
      "Batch 5050/6427 | loss: 0.6717245817184448 (0.066s) | train acc: 0.5985 | train auc: 0.499142\n",
      "Batch 5100/6427 | loss: 0.6734090614318847 (0.066s) | train acc: 0.5985 | train auc: 0.499099\n",
      "Batch 5150/6427 | loss: 0.6644728934764862 (0.066s) | train acc: 0.5988 | train auc: 0.499238\n",
      "Batch 5200/6427 | loss: 0.67355593085289 (0.066s) | train acc: 0.5988 | train auc: 0.499208\n",
      "Batch 5250/6427 | loss: 0.6797469222545623 (0.066s) | train acc: 0.5986 | train auc: 0.499138\n",
      "Batch 5300/6427 | loss: 0.6733863008022308 (0.066s) | train acc: 0.5986 | train auc: 0.499105\n",
      "Batch 5350/6427 | loss: 0.6771239018440247 (0.066s) | train acc: 0.5986 | train auc: 0.499103\n",
      "Batch 5400/6427 | loss: 0.6719158828258515 (0.066s) | train acc: 0.5986 | train auc: 0.499079\n",
      "Batch 5450/6427 | loss: 0.6653806436061859 (0.067s) | train acc: 0.5988 | train auc: 0.499141\n",
      "Batch 5500/6427 | loss: 0.6706021797657012 (0.066s) | train acc: 0.5988 | train auc: 0.499207\n",
      "Batch 5550/6427 | loss: 0.6799768674373626 (0.066s) | train acc: 0.5987 | train auc: 0.499094\n",
      "Batch 5600/6427 | loss: 0.6796157324314117 (0.066s) | train acc: 0.5986 | train auc: 0.499130\n",
      "Batch 5650/6427 | loss: 0.6747958278656006 (0.066s) | train acc: 0.5986 | train auc: 0.499165\n",
      "Batch 5700/6427 | loss: 0.6659868621826172 (0.066s) | train acc: 0.5987 | train auc: 0.499141\n",
      "Batch 5750/6427 | loss: 0.6686473560333251 (0.066s) | train acc: 0.5988 | train auc: 0.499238\n",
      "Batch 5800/6427 | loss: 0.6723474001884461 (0.066s) | train acc: 0.5989 | train auc: 0.499252\n",
      "Batch 5850/6427 | loss: 0.6678397142887116 (0.066s) | train acc: 0.5990 | train auc: 0.499328\n",
      "Batch 5900/6427 | loss: 0.6712633216381073 (0.066s) | train acc: 0.5990 | train auc: 0.499366\n",
      "Batch 5950/6427 | loss: 0.67500537276268 (0.066s) | train acc: 0.5990 | train auc: 0.499314\n",
      "Batch 6000/6427 | loss: 0.6660233187675476 (0.067s) | train acc: 0.5991 | train auc: 0.499400\n",
      "Batch 6050/6427 | loss: 0.6715481185913086 (0.066s) | train acc: 0.5992 | train auc: 0.499420\n",
      "Batch 6100/6427 | loss: 0.6763056480884552 (0.067s) | train acc: 0.5991 | train auc: 0.499359\n",
      "Batch 6150/6427 | loss: 0.6794205272197723 (0.067s) | train acc: 0.5990 | train auc: 0.499394\n",
      "Batch 6200/6427 | loss: 0.6717712640762329 (0.067s) | train acc: 0.5990 | train auc: 0.499343\n",
      "Batch 6250/6427 | loss: 0.6701267731189727 (0.066s) | train acc: 0.5991 | train auc: 0.499316\n",
      "Batch 6300/6427 | loss: 0.6713250148296356 (0.067s) | train acc: 0.5991 | train auc: 0.499312\n",
      "Batch 6350/6427 | loss: 0.6678315424919128 (0.067s) | train acc: 0.5993 | train auc: 0.499375\n",
      "Batch 6400/6427 | loss: 0.6693956077098846 (0.067s) | train acc: 0.5993 | train auc: 0.499444\n",
      "Batch 6427/6427 | loss: 0.6788984934488932 (0.038s) | train acc: 0.5993 | train auc: 0.499389\n",
      "Validation: val loss: 0.673 | val acc: 0.600 | val F1: 0.000 | val AUC: 0.500\n",
      "\n",
      "-----Epoch 2/150-----\n",
      "Batch 50/6427 | loss: 0.6721357607841492 (0.068s) | train acc: 0.6025 | train auc: 0.466701\n",
      "Batch 100/6427 | loss: 0.6711009550094604 (0.065s) | train acc: 0.6038 | train auc: 0.470466\n",
      "Batch 150/6427 | loss: 0.6742134332656861 (0.066s) | train acc: 0.6017 | train auc: 0.476002\n",
      "Batch 200/6427 | loss: 0.6739811825752259 (0.065s) | train acc: 0.6008 | train auc: 0.486474\n",
      "Batch 250/6427 | loss: 0.6686581134796142 (0.065s) | train acc: 0.6030 | train auc: 0.483003\n",
      "Batch 300/6427 | loss: 0.6673236644268036 (0.066s) | train acc: 0.6048 | train auc: 0.488790\n",
      "Batch 350/6427 | loss: 0.6704514408111573 (0.065s) | train acc: 0.6051 | train auc: 0.486122\n",
      "Batch 400/6427 | loss: 0.6757913339138031 (0.065s) | train acc: 0.6037 | train auc: 0.485363\n",
      "Batch 450/6427 | loss: 0.671796658039093 (0.066s) | train acc: 0.6036 | train auc: 0.485541\n",
      "Batch 500/6427 | loss: 0.671089768409729 (0.066s) | train acc: 0.6038 | train auc: 0.484395\n",
      "Batch 550/6427 | loss: 0.6718311405181885 (0.066s) | train acc: 0.6037 | train auc: 0.484873\n",
      "Batch 600/6427 | loss: 0.6734415447711944 (0.066s) | train acc: 0.6033 | train auc: 0.485050\n",
      "Batch 650/6427 | loss: 0.6675648546218872 (0.066s) | train acc: 0.6041 | train auc: 0.485555\n",
      "Batch 700/6427 | loss: 0.6770489132404327 (0.066s) | train acc: 0.6031 | train auc: 0.485475\n",
      "Batch 750/6427 | loss: 0.676698203086853 (0.066s) | train acc: 0.6023 | train auc: 0.486726\n",
      "Batch 800/6427 | loss: 0.6723934614658356 (0.068s) | train acc: 0.6023 | train auc: 0.488098\n",
      "Batch 850/6427 | loss: 0.6765214335918427 (0.069s) | train acc: 0.6017 | train auc: 0.490296\n",
      "Batch 900/6427 | loss: 0.6743406724929809 (0.066s) | train acc: 0.6014 | train auc: 0.491658\n",
      "Batch 950/6427 | loss: 0.665766795873642 (0.066s) | train acc: 0.6023 | train auc: 0.491600\n",
      "Batch 1000/6427 | loss: 0.6684474575519562 (0.066s) | train acc: 0.6027 | train auc: 0.492695\n",
      "Batch 1050/6427 | loss: 0.6761116576194763 (0.066s) | train acc: 0.6023 | train auc: 0.492491\n",
      "Batch 1100/6427 | loss: 0.6712459862232208 (0.066s) | train acc: 0.6024 | train auc: 0.492663\n",
      "Batch 1150/6427 | loss: 0.6720463061332702 (0.066s) | train acc: 0.6024 | train auc: 0.493009\n",
      "Batch 1200/6427 | loss: 0.6748621034622192 (0.066s) | train acc: 0.6021 | train auc: 0.493355\n",
      "Batch 1250/6427 | loss: 0.6820370054244995 (0.066s) | train acc: 0.6011 | train auc: 0.495259\n",
      "Batch 1300/6427 | loss: 0.6699952614307404 (0.066s) | train acc: 0.6014 | train auc: 0.495200\n",
      "Batch 1350/6427 | loss: 0.669692120552063 (0.066s) | train acc: 0.6016 | train auc: 0.495360\n",
      "Batch 1400/6427 | loss: 0.6699431788921356 (0.066s) | train acc: 0.6019 | train auc: 0.495517\n",
      "Batch 1450/6427 | loss: 0.6777283346652985 (0.066s) | train acc: 0.6014 | train auc: 0.495479\n",
      "Batch 1500/6427 | loss: 0.6703003418445587 (0.066s) | train acc: 0.6016 | train auc: 0.494793\n",
      "Batch 1550/6427 | loss: 0.6692646563053131 (0.066s) | train acc: 0.6019 | train auc: 0.494848\n",
      "Batch 1600/6427 | loss: 0.6686477541923523 (0.066s) | train acc: 0.6022 | train auc: 0.495101\n",
      "Batch 1650/6427 | loss: 0.6782125675678253 (0.066s) | train acc: 0.6017 | train auc: 0.494793\n",
      "Batch 1700/6427 | loss: 0.6708098769187927 (0.066s) | train acc: 0.6019 | train auc: 0.494642\n",
      "Batch 1750/6427 | loss: 0.6740778362751008 (0.066s) | train acc: 0.6017 | train auc: 0.494831\n",
      "Batch 1800/6427 | loss: 0.6647853446006775 (0.066s) | train acc: 0.6022 | train auc: 0.494904\n",
      "Batch 1850/6427 | loss: 0.6672455978393554 (0.066s) | train acc: 0.6026 | train auc: 0.495504\n",
      "Batch 1900/6427 | loss: 0.6844902741909027 (0.066s) | train acc: 0.6018 | train auc: 0.495291\n",
      "Batch 1950/6427 | loss: 0.6831774473190307 (0.066s) | train acc: 0.6010 | train auc: 0.497016\n",
      "Batch 2000/6427 | loss: 0.6689207220077514 (0.066s) | train acc: 0.6013 | train auc: 0.496608\n",
      "Batch 2050/6427 | loss: 0.6772108602523804 (0.066s) | train acc: 0.6011 | train auc: 0.496553\n",
      "Batch 2100/6427 | loss: 0.6761767983436584 (0.066s) | train acc: 0.6008 | train auc: 0.497056\n",
      "Batch 2150/6427 | loss: 0.6781707382202149 (0.066s) | train acc: 0.6005 | train auc: 0.497359\n",
      "Batch 2200/6427 | loss: 0.6720154583454132 (0.066s) | train acc: 0.6006 | train auc: 0.496628\n",
      "Batch 2250/6427 | loss: 0.6721582806110382 (0.066s) | train acc: 0.6007 | train auc: 0.496637\n",
      "Batch 2300/6427 | loss: 0.6732686030864715 (0.066s) | train acc: 0.6007 | train auc: 0.496525\n",
      "Batch 2350/6427 | loss: 0.6614440190792084 (0.066s) | train acc: 0.6012 | train auc: 0.497059\n",
      "Batch 2400/6427 | loss: 0.6751528894901275 (0.066s) | train acc: 0.6011 | train auc: 0.496822\n",
      "Batch 2450/6427 | loss: 0.6715796911716461 (0.066s) | train acc: 0.6012 | train auc: 0.496801\n",
      "Batch 2500/6427 | loss: 0.6728495824337005 (0.066s) | train acc: 0.6012 | train auc: 0.496733\n",
      "Batch 2550/6427 | loss: 0.673908245563507 (0.066s) | train acc: 0.6011 | train auc: 0.496630\n",
      "Batch 2600/6427 | loss: 0.6727152705192566 (0.066s) | train acc: 0.6011 | train auc: 0.496360\n",
      "Batch 2650/6427 | loss: 0.6793287014961242 (0.066s) | train acc: 0.6008 | train auc: 0.496741\n",
      "Batch 2700/6427 | loss: 0.6780892312526703 (0.066s) | train acc: 0.6006 | train auc: 0.497045\n",
      "Batch 2750/6427 | loss: 0.6697139179706574 (0.066s) | train acc: 0.6007 | train auc: 0.496861\n",
      "Batch 2800/6427 | loss: 0.6737667179107666 (0.066s) | train acc: 0.6007 | train auc: 0.496820\n",
      "Batch 2850/6427 | loss: 0.6759284818172455 (0.066s) | train acc: 0.6005 | train auc: 0.496768\n",
      "Batch 2900/6427 | loss: 0.6709426403045654 (0.066s) | train acc: 0.6006 | train auc: 0.496519\n",
      "Batch 2950/6427 | loss: 0.664508410692215 (0.066s) | train acc: 0.6010 | train auc: 0.496703\n",
      "Batch 3000/6427 | loss: 0.6688592875003815 (0.066s) | train acc: 0.6011 | train auc: 0.496948\n",
      "Batch 3050/6427 | loss: 0.6738256621360779 (0.066s) | train acc: 0.6011 | train auc: 0.496850\n",
      "Batch 3100/6427 | loss: 0.666481374502182 (0.066s) | train acc: 0.6013 | train auc: 0.497029\n",
      "Batch 3150/6427 | loss: 0.667644648551941 (0.066s) | train acc: 0.6015 | train auc: 0.497384\n",
      "Batch 3200/6427 | loss: 0.6668454480171203 (0.066s) | train acc: 0.6017 | train auc: 0.497830\n",
      "Batch 3250/6427 | loss: 0.6736395764350891 (0.066s) | train acc: 0.6017 | train auc: 0.497769\n",
      "Batch 3300/6427 | loss: 0.6771270155906677 (0.066s) | train acc: 0.6015 | train auc: 0.497579\n",
      "Batch 3350/6427 | loss: 0.6748499941825866 (0.066s) | train acc: 0.6014 | train auc: 0.497634\n",
      "Batch 3400/6427 | loss: 0.6763445305824279 (0.066s) | train acc: 0.6013 | train auc: 0.497783\n",
      "Batch 3450/6427 | loss: 0.6753655469417572 (0.066s) | train acc: 0.6012 | train auc: 0.497890\n",
      "Batch 3500/6427 | loss: 0.6753768301010132 (0.066s) | train acc: 0.6011 | train auc: 0.498033\n",
      "Batch 3550/6427 | loss: 0.6731713676452636 (0.066s) | train acc: 0.6011 | train auc: 0.498006\n",
      "Batch 3600/6427 | loss: 0.6654354119300843 (0.066s) | train acc: 0.6013 | train auc: 0.497962\n",
      "Batch 3650/6427 | loss: 0.6714713931083679 (0.066s) | train acc: 0.6013 | train auc: 0.497987\n",
      "Batch 3700/6427 | loss: 0.681694393157959 (0.066s) | train acc: 0.6010 | train auc: 0.498058\n",
      "Batch 3750/6427 | loss: 0.6617577695846557 (0.066s) | train acc: 0.6014 | train auc: 0.497871\n",
      "Batch 3800/6427 | loss: 0.6632220625877381 (0.066s) | train acc: 0.6017 | train auc: 0.498337\n",
      "Batch 3850/6427 | loss: 0.6737910079956054 (0.066s) | train acc: 0.6017 | train auc: 0.498233\n",
      "Batch 3900/6427 | loss: 0.6640931236743927 (0.066s) | train acc: 0.6019 | train auc: 0.498628\n",
      "Batch 3950/6427 | loss: 0.6749953055381774 (0.066s) | train acc: 0.6018 | train auc: 0.498495\n",
      "Batch 4000/6427 | loss: 0.6834128093719483 (0.066s) | train acc: 0.6015 | train auc: 0.498488\n",
      "Batch 4050/6427 | loss: 0.6676646089553833 (0.066s) | train acc: 0.6017 | train auc: 0.498302\n",
      "Batch 4100/6427 | loss: 0.6726608061790467 (0.066s) | train acc: 0.6017 | train auc: 0.498197\n",
      "Batch 4150/6427 | loss: 0.6739192831516266 (0.066s) | train acc: 0.6016 | train auc: 0.498117\n",
      "Batch 4200/6427 | loss: 0.6687023878097534 (0.066s) | train acc: 0.6017 | train auc: 0.498118\n",
      "Batch 4250/6427 | loss: 0.6773198652267456 (0.066s) | train acc: 0.6016 | train auc: 0.498029\n",
      "Batch 4300/6427 | loss: 0.6708150279521942 (0.066s) | train acc: 0.6016 | train auc: 0.497988\n",
      "Batch 4350/6427 | loss: 0.6731488120555877 (0.066s) | train acc: 0.6016 | train auc: 0.497922\n",
      "Batch 4400/6427 | loss: 0.6751383781433106 (0.065s) | train acc: 0.6015 | train auc: 0.497924\n",
      "Batch 4450/6427 | loss: 0.6720430278778076 (0.066s) | train acc: 0.6015 | train auc: 0.497907\n",
      "Batch 4500/6427 | loss: 0.6816810631752014 (0.066s) | train acc: 0.6013 | train auc: 0.498171\n",
      "Batch 4550/6427 | loss: 0.6676932847499848 (0.065s) | train acc: 0.6014 | train auc: 0.497956\n",
      "Batch 4600/6427 | loss: 0.6702654778957366 (0.066s) | train acc: 0.6015 | train auc: 0.497922\n",
      "Batch 4650/6427 | loss: 0.6772431409358979 (0.066s) | train acc: 0.6014 | train auc: 0.497804\n",
      "Batch 4700/6427 | loss: 0.670551962852478 (0.065s) | train acc: 0.6014 | train auc: 0.497759\n",
      "Batch 4750/6427 | loss: 0.6751866042613983 (0.065s) | train acc: 0.6014 | train auc: 0.497697\n",
      "Batch 4800/6427 | loss: 0.6735952198505402 (0.066s) | train acc: 0.6013 | train auc: 0.497660\n",
      "Batch 4850/6427 | loss: 0.6741500782966614 (0.065s) | train acc: 0.6013 | train auc: 0.497621\n",
      "Batch 4900/6427 | loss: 0.6838629102706909 (0.065s) | train acc: 0.6010 | train auc: 0.498061\n",
      "Batch 4950/6427 | loss: 0.6804156148433685 (0.066s) | train acc: 0.6008 | train auc: 0.498532\n",
      "Batch 5000/6427 | loss: 0.6673254096508026 (0.066s) | train acc: 0.6009 | train auc: 0.498238\n",
      "Batch 5050/6427 | loss: 0.6718089091777801 (0.066s) | train acc: 0.6010 | train auc: 0.498208\n",
      "Batch 5100/6427 | loss: 0.6748931384086609 (0.066s) | train acc: 0.6009 | train auc: 0.498220\n",
      "Batch 5150/6427 | loss: 0.6672930765151978 (0.066s) | train acc: 0.6010 | train auc: 0.498139\n",
      "Batch 5200/6427 | loss: 0.6798810291290284 (0.065s) | train acc: 0.6009 | train auc: 0.498133\n",
      "Batch 5250/6427 | loss: 0.6767675924301148 (0.066s) | train acc: 0.6008 | train auc: 0.498179\n",
      "Batch 5300/6427 | loss: 0.6785180699825287 (0.066s) | train acc: 0.6006 | train auc: 0.498456\n",
      "Batch 5350/6427 | loss: 0.6813146507740021 (0.065s) | train acc: 0.6004 | train auc: 0.498915\n",
      "Batch 5400/6427 | loss: 0.6857239460945129 (0.066s) | train acc: 0.6001 | train auc: 0.499639\n",
      "Batch 5450/6427 | loss: 0.6669497418403626 (0.066s) | train acc: 0.6003 | train auc: 0.499368\n",
      "Batch 5500/6427 | loss: 0.6813290548324585 (0.066s) | train acc: 0.6001 | train auc: 0.499256\n",
      "Batch 5550/6427 | loss: 0.6699450516700745 (0.066s) | train acc: 0.6002 | train auc: 0.499203\n",
      "Batch 5600/6427 | loss: 0.6737630414962769 (0.065s) | train acc: 0.6002 | train auc: 0.499084\n",
      "Batch 5650/6427 | loss: 0.6702362215518951 (0.066s) | train acc: 0.6002 | train auc: 0.498966\n",
      "Batch 5700/6427 | loss: 0.6674931406974792 (0.066s) | train acc: 0.6003 | train auc: 0.499062\n",
      "Batch 5750/6427 | loss: 0.6767237675189972 (0.065s) | train acc: 0.6003 | train auc: 0.499056\n",
      "Batch 5800/6427 | loss: 0.6785186147689819 (0.066s) | train acc: 0.6001 | train auc: 0.499297\n",
      "Batch 5850/6427 | loss: 0.6859413063526154 (0.066s) | train acc: 0.5998 | train auc: 0.499919\n",
      "Batch 5900/6427 | loss: 0.6726601922512054 (0.066s) | train acc: 0.5999 | train auc: 0.499841\n",
      "Batch 5950/6427 | loss: 0.669938131570816 (0.066s) | train acc: 0.5999 | train auc: 0.499774\n",
      "Batch 6000/6427 | loss: 0.6697528314590454 (0.066s) | train acc: 0.6000 | train auc: 0.499860\n",
      "Batch 6050/6427 | loss: 0.67313068151474 (0.065s) | train acc: 0.6000 | train auc: 0.499811\n",
      "Batch 6100/6427 | loss: 0.6785139262676239 (0.066s) | train acc: 0.5999 | train auc: 0.499877\n",
      "Batch 6150/6427 | loss: 0.6758228027820588 (0.065s) | train acc: 0.5998 | train auc: 0.499936\n",
      "Batch 6200/6427 | loss: 0.6742029452323913 (0.066s) | train acc: 0.5998 | train auc: 0.499895\n",
      "Batch 6250/6427 | loss: 0.6711441063880921 (0.065s) | train acc: 0.5999 | train auc: 0.499822\n",
      "Batch 6300/6427 | loss: 0.6747348988056183 (0.065s) | train acc: 0.5998 | train auc: 0.499781\n",
      "Batch 6350/6427 | loss: 0.6716897594928741 (0.065s) | train acc: 0.5999 | train auc: 0.499703\n",
      "Batch 6400/6427 | loss: 0.6692177879810334 (0.066s) | train acc: 0.5999 | train auc: 0.499698\n",
      "Batch 6427/6427 | loss: 0.6635335551367866 (0.037s) | train acc: 0.6000 | train auc: 0.499718\n",
      "Validation: val loss: 0.673 | val acc: 0.600 | val F1: 0.000 | val AUC: 0.500\n",
      "\n",
      "-----Epoch 3/150-----\n",
      "Batch 50/6427 | loss: 0.6669100964069367 (0.067s) | train acc: 0.6144 | train auc: 0.485939\n",
      "Batch 100/6427 | loss: 0.6729043209552765 (0.065s) | train acc: 0.6078 | train auc: 0.491813\n",
      "Batch 150/6427 | loss: 0.6744950699806214 (0.066s) | train acc: 0.6042 | train auc: 0.495244\n",
      "Batch 200/6427 | loss: 0.667787857055664 (0.065s) | train acc: 0.6064 | train auc: 0.492814\n",
      "Batch 250/6427 | loss: 0.6687953293323516 (0.065s) | train acc: 0.6071 | train auc: 0.494427\n",
      "Batch 300/6427 | loss: 0.6710734033584594 (0.065s) | train acc: 0.6068 | train auc: 0.493860\n",
      "Batch 350/6427 | loss: 0.6767516779899597 (0.065s) | train acc: 0.6046 | train auc: 0.496703\n",
      "Batch 400/6427 | loss: 0.6775230813026428 (0.065s) | train acc: 0.6026 | train auc: 0.500704\n",
      "Batch 450/6427 | loss: 0.6803840553760528 (0.065s) | train acc: 0.6001 | train auc: 0.504646\n",
      "Batch 500/6427 | loss: 0.6762462055683136 (0.067s) | train acc: 0.5993 | train auc: 0.504833\n",
      "Batch 550/6427 | loss: 0.6748864626884461 (0.066s) | train acc: 0.5990 | train auc: 0.504243\n",
      "Batch 600/6427 | loss: 0.67419757604599 (0.067s) | train acc: 0.5989 | train auc: 0.504017\n",
      "Batch 650/6427 | loss: 0.6614970350265503 (0.066s) | train acc: 0.6012 | train auc: 0.504443\n",
      "Batch 700/6427 | loss: 0.6730234730243683 (0.066s) | train acc: 0.6012 | train auc: 0.503265\n",
      "Batch 750/6427 | loss: 0.6746924483776092 (0.066s) | train acc: 0.6008 | train auc: 0.503034\n",
      "Batch 800/6427 | loss: 0.6837276256084442 (0.066s) | train acc: 0.5991 | train auc: 0.504455\n",
      "Batch 850/6427 | loss: 0.6771108210086823 (0.066s) | train acc: 0.5985 | train auc: 0.504365\n",
      "Batch 900/6427 | loss: 0.676977127790451 (0.066s) | train acc: 0.5981 | train auc: 0.504236\n",
      "Batch 950/6427 | loss: 0.6762908816337585 (0.066s) | train acc: 0.5977 | train auc: 0.504073\n",
      "Batch 1000/6427 | loss: 0.6842153716087341 (0.066s) | train acc: 0.5964 | train auc: 0.505327\n",
      "Batch 1050/6427 | loss: 0.6767859649658203 (0.066s) | train acc: 0.5961 | train auc: 0.505164\n",
      "Batch 1100/6427 | loss: 0.6724077820777893 (0.066s) | train acc: 0.5964 | train auc: 0.504516\n",
      "Batch 1150/6427 | loss: 0.6685535347461701 (0.066s) | train acc: 0.5970 | train auc: 0.504767\n",
      "Batch 1200/6427 | loss: 0.6669245707988739 (0.066s) | train acc: 0.5978 | train auc: 0.505481\n",
      "Batch 1250/6427 | loss: 0.6735207939147949 (0.067s) | train acc: 0.5979 | train auc: 0.505173\n",
      "Batch 1300/6427 | loss: 0.6740986764431 (0.067s) | train acc: 0.5979 | train auc: 0.504930\n",
      "Batch 1350/6427 | loss: 0.6727507627010345 (0.067s) | train acc: 0.5980 | train auc: 0.504770\n",
      "Batch 1400/6427 | loss: 0.6746170735359192 (0.066s) | train acc: 0.5979 | train auc: 0.504747\n",
      "Batch 1450/6427 | loss: 0.6689606380462646 (0.066s) | train acc: 0.5984 | train auc: 0.504860\n",
      "Batch 1500/6427 | loss: 0.6747106981277465 (0.066s) | train acc: 0.5983 | train auc: 0.504648\n",
      "Batch 1550/6427 | loss: 0.6802550411224365 (0.066s) | train acc: 0.5978 | train auc: 0.504364\n",
      "Batch 1600/6427 | loss: 0.6677340316772461 (0.066s) | train acc: 0.5983 | train auc: 0.503954\n",
      "Batch 1650/6427 | loss: 0.6638917744159698 (0.066s) | train acc: 0.5990 | train auc: 0.504579\n",
      "Batch 1700/6427 | loss: 0.6795150530338288 (0.066s) | train acc: 0.5986 | train auc: 0.503745\n",
      "Batch 1750/6427 | loss: 0.6690978145599366 (0.066s) | train acc: 0.5990 | train auc: 0.503485\n",
      "Batch 1800/6427 | loss: 0.6748308265209197 (0.066s) | train acc: 0.5989 | train auc: 0.503479\n",
      "Batch 1850/6427 | loss: 0.6734334993362426 (0.066s) | train acc: 0.5989 | train auc: 0.503407\n",
      "Batch 1900/6427 | loss: 0.6751092374324799 (0.066s) | train acc: 0.5988 | train auc: 0.503381\n",
      "Batch 1950/6427 | loss: 0.677207955121994 (0.066s) | train acc: 0.5985 | train auc: 0.503470\n",
      "Batch 2000/6427 | loss: 0.6750659263134002 (0.066s) | train acc: 0.5985 | train auc: 0.503463\n",
      "Batch 2050/6427 | loss: 0.6698011565208435 (0.066s) | train acc: 0.5987 | train auc: 0.503242\n",
      "Batch 2100/6427 | loss: 0.6761641895771027 (0.066s) | train acc: 0.5986 | train auc: 0.503106\n",
      "Batch 2150/6427 | loss: 0.6691861152648926 (0.066s) | train acc: 0.5988 | train auc: 0.502979\n",
      "Batch 2200/6427 | loss: 0.6712978720664978 (0.066s) | train acc: 0.5989 | train auc: 0.502965\n",
      "Batch 2250/6427 | loss: 0.6774799931049347 (0.066s) | train acc: 0.5988 | train auc: 0.502568\n",
      "Batch 2300/6427 | loss: 0.6760141158103943 (0.066s) | train acc: 0.5986 | train auc: 0.502577\n",
      "Batch 2350/6427 | loss: 0.6736337339878082 (0.066s) | train acc: 0.5986 | train auc: 0.502436\n",
      "Batch 2400/6427 | loss: 0.6819072020053863 (0.066s) | train acc: 0.5982 | train auc: 0.502780\n",
      "Batch 2450/6427 | loss: 0.6709753012657166 (0.066s) | train acc: 0.5983 | train auc: 0.502496\n",
      "Batch 2500/6427 | loss: 0.6804479837417603 (0.066s) | train acc: 0.5980 | train auc: 0.502529\n",
      "Batch 2550/6427 | loss: 0.674899400472641 (0.066s) | train acc: 0.5979 | train auc: 0.502449\n",
      "Batch 2600/6427 | loss: 0.6730431640148162 (0.066s) | train acc: 0.5980 | train auc: 0.502225\n",
      "Batch 2650/6427 | loss: 0.6706308007240296 (0.067s) | train acc: 0.5981 | train auc: 0.502142\n",
      "Batch 2700/6427 | loss: 0.6737686109542846 (0.066s) | train acc: 0.5982 | train auc: 0.501913\n",
      "Batch 2750/6427 | loss: 0.6644949674606323 (0.066s) | train acc: 0.5986 | train auc: 0.502275\n",
      "Batch 2800/6427 | loss: 0.6663236713409424 (0.066s) | train acc: 0.5989 | train auc: 0.502722\n",
      "Batch 2850/6427 | loss: 0.6776520323753357 (0.066s) | train acc: 0.5987 | train auc: 0.502397\n",
      "Batch 2900/6427 | loss: 0.6823357319831849 (0.066s) | train acc: 0.5983 | train auc: 0.502485\n",
      "Batch 2950/6427 | loss: 0.6674472546577453 (0.066s) | train acc: 0.5986 | train auc: 0.502227\n",
      "Batch 3000/6427 | loss: 0.6733834314346313 (0.066s) | train acc: 0.5986 | train auc: 0.502141\n",
      "Batch 3050/6427 | loss: 0.6790066874027252 (0.066s) | train acc: 0.5984 | train auc: 0.502083\n",
      "Batch 3100/6427 | loss: 0.6667514610290527 (0.066s) | train acc: 0.5987 | train auc: 0.501947\n",
      "Batch 3150/6427 | loss: 0.6814017820358277 (0.066s) | train acc: 0.5984 | train auc: 0.501803\n",
      "Batch 3200/6427 | loss: 0.6736333227157593 (0.066s) | train acc: 0.5984 | train auc: 0.501690\n",
      "Batch 3250/6427 | loss: 0.6639520168304444 (0.066s) | train acc: 0.5988 | train auc: 0.501791\n",
      "Batch 3300/6427 | loss: 0.6747483670711517 (0.066s) | train acc: 0.5987 | train auc: 0.501702\n",
      "Batch 3350/6427 | loss: 0.6738663816452026 (0.066s) | train acc: 0.5987 | train auc: 0.501656\n",
      "Batch 3400/6427 | loss: 0.6700105011463166 (0.066s) | train acc: 0.5988 | train auc: 0.501710\n",
      "Batch 3450/6427 | loss: 0.6734183597564697 (0.066s) | train acc: 0.5988 | train auc: 0.501630\n",
      "Batch 3500/6427 | loss: 0.6627480530738831 (0.066s) | train acc: 0.5992 | train auc: 0.502002\n",
      "Batch 3550/6427 | loss: 0.6672327780723571 (0.066s) | train acc: 0.5994 | train auc: 0.502283\n",
      "Batch 3600/6427 | loss: 0.6714001643657684 (0.066s) | train acc: 0.5995 | train auc: 0.502329\n",
      "Batch 3650/6427 | loss: 0.6743399012088775 (0.066s) | train acc: 0.5995 | train auc: 0.502177\n",
      "Batch 3700/6427 | loss: 0.6668106842041016 (0.066s) | train acc: 0.5997 | train auc: 0.502320\n",
      "Batch 3750/6427 | loss: 0.675652093887329 (0.066s) | train acc: 0.5996 | train auc: 0.502175\n",
      "Batch 3800/6427 | loss: 0.6638966846466064 (0.066s) | train acc: 0.5999 | train auc: 0.502418\n",
      "Batch 3850/6427 | loss: 0.6825299823284149 (0.066s) | train acc: 0.5996 | train auc: 0.502183\n",
      "Batch 3900/6427 | loss: 0.673698148727417 (0.066s) | train acc: 0.5996 | train auc: 0.502004\n",
      "Batch 3950/6427 | loss: 0.6796376883983613 (0.066s) | train acc: 0.5994 | train auc: 0.502095\n",
      "Batch 4000/6427 | loss: 0.673253802061081 (0.066s) | train acc: 0.5994 | train auc: 0.502026\n",
      "Batch 4050/6427 | loss: 0.6783481550216675 (0.066s) | train acc: 0.5992 | train auc: 0.502172\n",
      "Batch 4100/6427 | loss: 0.6724755072593689 (0.066s) | train acc: 0.5993 | train auc: 0.502089\n",
      "Batch 4150/6427 | loss: 0.6744147551059723 (0.066s) | train acc: 0.5992 | train auc: 0.501994\n",
      "Batch 4200/6427 | loss: 0.6712518239021301 (0.066s) | train acc: 0.5993 | train auc: 0.501880\n",
      "Batch 4250/6427 | loss: 0.6787802290916443 (0.066s) | train acc: 0.5992 | train auc: 0.501885\n",
      "Batch 4300/6427 | loss: 0.6670581901073456 (0.066s) | train acc: 0.5993 | train auc: 0.501871\n",
      "Batch 4350/6427 | loss: 0.6733120179176331 (0.066s) | train acc: 0.5993 | train auc: 0.501756\n",
      "Batch 4400/6427 | loss: 0.6791309225559234 (0.067s) | train acc: 0.5992 | train auc: 0.501578\n",
      "Batch 4450/6427 | loss: 0.6762364196777344 (0.066s) | train acc: 0.5991 | train auc: 0.501635\n",
      "Batch 4500/6427 | loss: 0.6640711903572083 (0.066s) | train acc: 0.5994 | train auc: 0.501642\n",
      "Batch 4550/6427 | loss: 0.6833570873737336 (0.066s) | train acc: 0.5991 | train auc: 0.501523\n",
      "Batch 4600/6427 | loss: 0.6732752382755279 (0.066s) | train acc: 0.5991 | train auc: 0.501452\n",
      "Batch 4650/6427 | loss: 0.6711170113086701 (0.066s) | train acc: 0.5992 | train auc: 0.501411\n",
      "Batch 4700/6427 | loss: 0.6747135555744171 (0.066s) | train acc: 0.5992 | train auc: 0.501299\n",
      "Batch 4750/6427 | loss: 0.6785286796092987 (0.066s) | train acc: 0.5990 | train auc: 0.501301\n",
      "Batch 4800/6427 | loss: 0.6753233993053436 (0.066s) | train acc: 0.5990 | train auc: 0.501277\n",
      "Batch 4850/6427 | loss: 0.6709348678588867 (0.066s) | train acc: 0.5991 | train auc: 0.501182\n",
      "Batch 4900/6427 | loss: 0.6698417401313782 (0.066s) | train acc: 0.5991 | train auc: 0.501141\n",
      "Batch 4950/6427 | loss: 0.6702309715747833 (0.066s) | train acc: 0.5992 | train auc: 0.501195\n",
      "Batch 5000/6427 | loss: 0.6727817904949188 (0.066s) | train acc: 0.5992 | train auc: 0.501105\n",
      "Batch 5050/6427 | loss: 0.6747374951839447 (0.066s) | train acc: 0.5992 | train auc: 0.501021\n",
      "Batch 5100/6427 | loss: 0.6836616635322571 (0.066s) | train acc: 0.5990 | train auc: 0.501109\n",
      "Batch 5150/6427 | loss: 0.6751157462596893 (0.066s) | train acc: 0.5989 | train auc: 0.501129\n",
      "Batch 5200/6427 | loss: 0.6698731195926666 (0.066s) | train acc: 0.5990 | train auc: 0.500976\n",
      "Batch 5250/6427 | loss: 0.6715652823448182 (0.066s) | train acc: 0.5991 | train auc: 0.500974\n",
      "Batch 5300/6427 | loss: 0.6796385300159454 (0.066s) | train acc: 0.5989 | train auc: 0.500902\n",
      "Batch 5350/6427 | loss: 0.6736636662483215 (0.067s) | train acc: 0.5989 | train auc: 0.500855\n",
      "Batch 5400/6427 | loss: 0.6779014670848846 (0.066s) | train acc: 0.5988 | train auc: 0.500921\n",
      "Batch 5450/6427 | loss: 0.6666322386264801 (0.066s) | train acc: 0.5990 | train auc: 0.500788\n",
      "Batch 5500/6427 | loss: 0.6683153939247132 (0.066s) | train acc: 0.5991 | train auc: 0.500886\n",
      "Batch 5550/6427 | loss: 0.6667886459827423 (0.066s) | train acc: 0.5992 | train auc: 0.501069\n",
      "Batch 5600/6427 | loss: 0.6670828592777253 (0.066s) | train acc: 0.5994 | train auc: 0.501199\n",
      "Batch 5650/6427 | loss: 0.6781897795200348 (0.066s) | train acc: 0.5993 | train auc: 0.501016\n",
      "Batch 5700/6427 | loss: 0.6644310665130615 (0.066s) | train acc: 0.5995 | train auc: 0.501170\n",
      "Batch 5750/6427 | loss: 0.6640568780899048 (0.066s) | train acc: 0.5997 | train auc: 0.501491\n",
      "Batch 5800/6427 | loss: 0.6666182434558868 (0.066s) | train acc: 0.5998 | train auc: 0.501690\n",
      "Batch 5850/6427 | loss: 0.673611832857132 (0.066s) | train acc: 0.5998 | train auc: 0.501645\n",
      "Batch 5900/6427 | loss: 0.6666425275802612 (0.066s) | train acc: 0.5999 | train auc: 0.501797\n",
      "Batch 5950/6427 | loss: 0.6745777344703674 (0.066s) | train acc: 0.5999 | train auc: 0.501638\n",
      "Batch 6000/6427 | loss: 0.6700950145721436 (0.066s) | train acc: 0.6000 | train auc: 0.501652\n",
      "Batch 6050/6427 | loss: 0.6765796792507172 (0.066s) | train acc: 0.5999 | train auc: 0.501560\n",
      "Batch 6100/6427 | loss: 0.6655723249912262 (0.066s) | train acc: 0.6000 | train auc: 0.501623\n",
      "Batch 6150/6427 | loss: 0.6789423370361328 (0.066s) | train acc: 0.5999 | train auc: 0.501500\n",
      "Batch 6200/6427 | loss: 0.6740558922290802 (0.066s) | train acc: 0.5999 | train auc: 0.501486\n",
      "Batch 6250/6427 | loss: 0.6721303081512451 (0.066s) | train acc: 0.5999 | train auc: 0.501438\n",
      "Batch 6300/6427 | loss: 0.6690448236465454 (0.066s) | train acc: 0.6000 | train auc: 0.501416\n",
      "Batch 6350/6427 | loss: 0.6736964046955108 (0.066s) | train acc: 0.6000 | train auc: 0.501372\n",
      "Batch 6400/6427 | loss: 0.6710806035995484 (0.066s) | train acc: 0.6000 | train auc: 0.501349\n",
      "Batch 6427/6427 | loss: 0.6790534898086831 (0.038s) | train acc: 0.6000 | train auc: 0.501299\n",
      "Validation: val loss: 0.673 | val acc: 0.600 | val F1: 0.000 | val AUC: 0.500\n",
      "\n",
      "-----Epoch 4/150-----\n",
      "Batch 50/6427 | loss: 0.6818420779705048 (0.068s) | train acc: 0.5775 | train auc: 0.486680\n",
      "Batch 100/6427 | loss: 0.6655708658695221 (0.065s) | train acc: 0.5991 | train auc: 0.486592\n",
      "Batch 150/6427 | loss: 0.6645828402042389 (0.066s) | train acc: 0.6062 | train auc: 0.498319\n",
      "Batch 200/6427 | loss: 0.6761423635482788 (0.067s) | train acc: 0.6030 | train auc: 0.494911\n",
      "Batch 250/6427 | loss: 0.6712726509571075 (0.067s) | train acc: 0.6032 | train auc: 0.494928\n",
      "Batch 300/6427 | loss: 0.6678772795200348 (0.067s) | train acc: 0.6048 | train auc: 0.494635\n",
      "Batch 350/6427 | loss: 0.6734316718578338 (0.067s) | train acc: 0.6040 | train auc: 0.492930\n",
      "Batch 400/6427 | loss: 0.6692081618309021 (0.066s) | train acc: 0.6047 | train auc: 0.492186\n",
      "Batch 450/6427 | loss: 0.6794524347782135 (0.066s) | train acc: 0.6025 | train auc: 0.492137\n",
      "Batch 500/6427 | loss: 0.6712383949756622 (0.067s) | train acc: 0.6028 | train auc: 0.492823\n",
      "Batch 550/6427 | loss: 0.676353816986084 (0.067s) | train acc: 0.6018 | train auc: 0.493595\n",
      "Batch 600/6427 | loss: 0.6723921132087708 (0.068s) | train acc: 0.6018 | train auc: 0.493321\n",
      "Batch 650/6427 | loss: 0.6708278942108155 (0.066s) | train acc: 0.6021 | train auc: 0.493539\n",
      "Batch 700/6427 | loss: 0.6741707098484039 (0.066s) | train acc: 0.6018 | train auc: 0.493496\n",
      "Batch 750/6427 | loss: 0.6752807474136353 (0.066s) | train acc: 0.6013 | train auc: 0.492779\n",
      "Batch 800/6427 | loss: 0.6708739697933197 (0.066s) | train acc: 0.6016 | train auc: 0.492740\n",
      "Batch 850/6427 | loss: 0.6703227877616882 (0.066s) | train acc: 0.6019 | train auc: 0.492437\n",
      "Batch 900/6427 | loss: 0.6750020015239716 (0.066s) | train acc: 0.6016 | train auc: 0.491815\n",
      "Batch 950/6427 | loss: 0.6662571024894715 (0.066s) | train acc: 0.6023 | train auc: 0.492594\n",
      "Batch 1000/6427 | loss: 0.6812639915943146 (0.066s) | train acc: 0.6012 | train auc: 0.492129\n",
      "Batch 1050/6427 | loss: 0.6719977986812592 (0.066s) | train acc: 0.6013 | train auc: 0.491877\n",
      "Batch 1100/6427 | loss: 0.6731783604621887 (0.066s) | train acc: 0.6013 | train auc: 0.490782\n",
      "Batch 1150/6427 | loss: 0.6713664042949676 (0.066s) | train acc: 0.6014 | train auc: 0.490522\n",
      "Batch 1200/6427 | loss: 0.666055029630661 (0.066s) | train acc: 0.6021 | train auc: 0.490945\n",
      "Batch 1250/6427 | loss: 0.6815754532814026 (0.066s) | train acc: 0.6012 | train auc: 0.490325\n",
      "Batch 1300/6427 | loss: 0.6744346380233764 (0.066s) | train acc: 0.6010 | train auc: 0.491056\n",
      "Batch 1350/6427 | loss: 0.665247130393982 (0.066s) | train acc: 0.6017 | train auc: 0.490970\n",
      "Batch 1400/6427 | loss: 0.6729438269138336 (0.066s) | train acc: 0.6017 | train auc: 0.491283\n",
      "Batch 1450/6427 | loss: 0.6733338832855225 (0.066s) | train acc: 0.6016 | train auc: 0.491370\n",
      "Batch 1500/6427 | loss: 0.6686655879020691 (0.066s) | train acc: 0.6019 | train auc: 0.491525\n",
      "Batch 1550/6427 | loss: 0.6738881838321685 (0.066s) | train acc: 0.6018 | train auc: 0.491382\n",
      "Batch 1600/6427 | loss: 0.6702917766571045 (0.066s) | train acc: 0.6019 | train auc: 0.491465\n",
      "Batch 1650/6427 | loss: 0.6780349266529083 (0.066s) | train acc: 0.6015 | train auc: 0.491357\n",
      "Batch 1700/6427 | loss: 0.6753414714336395 (0.066s) | train acc: 0.6013 | train auc: 0.492016\n",
      "Batch 1750/6427 | loss: 0.6714294421672821 (0.066s) | train acc: 0.6014 | train auc: 0.492113\n",
      "Batch 1800/6427 | loss: 0.6768760085105896 (0.066s) | train acc: 0.6011 | train auc: 0.492427\n",
      "Batch 1850/6427 | loss: 0.6741381335258484 (0.066s) | train acc: 0.6010 | train auc: 0.492566\n",
      "Batch 1900/6427 | loss: 0.6709164488315582 (0.066s) | train acc: 0.6011 | train auc: 0.492586\n",
      "Batch 1950/6427 | loss: 0.6713608849048615 (0.066s) | train acc: 0.6012 | train auc: 0.492545\n",
      "Batch 2000/6427 | loss: 0.6708045196533203 (0.066s) | train acc: 0.6013 | train auc: 0.492445\n",
      "Batch 2050/6427 | loss: 0.6718948447704315 (0.066s) | train acc: 0.6014 | train auc: 0.492380\n",
      "Batch 2100/6427 | loss: 0.6690555667877197 (0.066s) | train acc: 0.6016 | train auc: 0.492238\n",
      "Batch 2150/6427 | loss: 0.6745540463924408 (0.066s) | train acc: 0.6015 | train auc: 0.492112\n",
      "Batch 2200/6427 | loss: 0.6822294914722442 (0.066s) | train acc: 0.6009 | train auc: 0.492400\n",
      "Batch 2250/6427 | loss: 0.671754515171051 (0.066s) | train acc: 0.6010 | train auc: 0.492558\n",
      "Batch 2300/6427 | loss: 0.6821937942504883 (0.066s) | train acc: 0.6004 | train auc: 0.493636\n",
      "Batch 2350/6427 | loss: 0.6767081606388092 (0.066s) | train acc: 0.6002 | train auc: 0.494309\n",
      "Batch 2400/6427 | loss: 0.6680192434787751 (0.066s) | train acc: 0.6005 | train auc: 0.494060\n",
      "Batch 2450/6427 | loss: 0.6738474822044372 (0.066s) | train acc: 0.6005 | train auc: 0.494062\n",
      "Batch 2500/6427 | loss: 0.6782872486114502 (0.066s) | train acc: 0.6002 | train auc: 0.494373\n",
      "Batch 2550/6427 | loss: 0.6752783000469208 (0.066s) | train acc: 0.6001 | train auc: 0.494694\n",
      "Batch 2600/6427 | loss: 0.6751105952262878 (0.067s) | train acc: 0.6000 | train auc: 0.494951\n",
      "Batch 2650/6427 | loss: 0.6609354591369629 (0.066s) | train acc: 0.6005 | train auc: 0.494951\n",
      "Batch 2700/6427 | loss: 0.6744540154933929 (0.066s) | train acc: 0.6005 | train auc: 0.494537\n",
      "Batch 2750/6427 | loss: 0.673207449913025 (0.066s) | train acc: 0.6005 | train auc: 0.494535\n",
      "Batch 2800/6427 | loss: 0.6763321101665497 (0.066s) | train acc: 0.6003 | train auc: 0.494300\n",
      "Batch 2850/6427 | loss: 0.6748271608352661 (0.066s) | train acc: 0.6003 | train auc: 0.494372\n",
      "Batch 2900/6427 | loss: 0.6765184938907624 (0.066s) | train acc: 0.6001 | train auc: 0.494661\n",
      "Batch 2950/6427 | loss: 0.6743616390228272 (0.066s) | train acc: 0.6000 | train auc: 0.494783\n",
      "Batch 3000/6427 | loss: 0.6684162104129792 (0.066s) | train acc: 0.6002 | train auc: 0.494618\n",
      "Batch 3050/6427 | loss: 0.6652461540699005 (0.066s) | train acc: 0.6005 | train auc: 0.495017\n",
      "Batch 3100/6427 | loss: 0.681872740983963 (0.066s) | train acc: 0.6002 | train auc: 0.494715\n",
      "Batch 3150/6427 | loss: 0.6740335881710052 (0.066s) | train acc: 0.6002 | train auc: 0.494565\n",
      "Batch 3200/6427 | loss: 0.6837178421020508 (0.066s) | train acc: 0.5997 | train auc: 0.495563\n",
      "Batch 3250/6427 | loss: 0.6709134268760681 (0.066s) | train acc: 0.5998 | train auc: 0.495456\n",
      "Batch 3300/6427 | loss: 0.6671901893615723 (0.066s) | train acc: 0.6001 | train auc: 0.495512\n",
      "Batch 3350/6427 | loss: 0.6721476745605469 (0.066s) | train acc: 0.6001 | train auc: 0.495597\n",
      "Batch 3400/6427 | loss: 0.6707357227802276 (0.066s) | train acc: 0.6002 | train auc: 0.495662\n",
      "Batch 3450/6427 | loss: 0.6731476891040802 (0.066s) | train acc: 0.6002 | train auc: 0.495564\n",
      "Batch 3500/6427 | loss: 0.6667204380035401 (0.066s) | train acc: 0.6004 | train auc: 0.495861\n",
      "Batch 3550/6427 | loss: 0.6657895505428314 (0.066s) | train acc: 0.6006 | train auc: 0.496423\n",
      "Batch 3600/6427 | loss: 0.6752880096435547 (0.066s) | train acc: 0.6005 | train auc: 0.496331\n",
      "Batch 3650/6427 | loss: 0.6759593796730041 (0.066s) | train acc: 0.6004 | train auc: 0.496228\n",
      "Batch 3700/6427 | loss: 0.6736181330680847 (0.066s) | train acc: 0.6004 | train auc: 0.496186\n",
      "Batch 3750/6427 | loss: 0.6774943232536316 (0.066s) | train acc: 0.6003 | train auc: 0.496355\n",
      "Batch 3800/6427 | loss: 0.6781275022029877 (0.066s) | train acc: 0.6001 | train auc: 0.496756\n",
      "Batch 3850/6427 | loss: 0.6782912290096283 (0.066s) | train acc: 0.5999 | train auc: 0.497168\n",
      "Batch 3900/6427 | loss: 0.6712144613265991 (0.066s) | train acc: 0.6000 | train auc: 0.497064\n",
      "Batch 3950/6427 | loss: 0.6691122055053711 (0.066s) | train acc: 0.6001 | train auc: 0.496982\n",
      "Batch 4000/6427 | loss: 0.6702393460273742 (0.066s) | train acc: 0.6002 | train auc: 0.496987\n",
      "Batch 4050/6427 | loss: 0.6755083417892456 (0.066s) | train acc: 0.6001 | train auc: 0.496837\n",
      "Batch 4100/6427 | loss: 0.6768873023986817 (0.066s) | train acc: 0.6000 | train auc: 0.496821\n",
      "Batch 4150/6427 | loss: 0.6675817155838013 (0.066s) | train acc: 0.6002 | train auc: 0.496724\n",
      "Batch 4200/6427 | loss: 0.6818146967887878 (0.066s) | train acc: 0.5999 | train auc: 0.496865\n",
      "Batch 4250/6427 | loss: 0.6727415311336518 (0.066s) | train acc: 0.5999 | train auc: 0.496793\n",
      "Batch 4300/6427 | loss: 0.6683572471141815 (0.066s) | train acc: 0.6001 | train auc: 0.496729\n",
      "Batch 4350/6427 | loss: 0.6750159180164337 (0.066s) | train acc: 0.6000 | train auc: 0.496577\n",
      "Batch 4400/6427 | loss: 0.680487687587738 (0.066s) | train acc: 0.5998 | train auc: 0.496769\n",
      "Batch 4450/6427 | loss: 0.6741469037532807 (0.066s) | train acc: 0.5998 | train auc: 0.496847\n",
      "Batch 4500/6427 | loss: 0.6757454943656921 (0.066s) | train acc: 0.5997 | train auc: 0.496954\n",
      "Batch 4550/6427 | loss: 0.6705663669109344 (0.066s) | train acc: 0.5998 | train auc: 0.496949\n",
      "Batch 4600/6427 | loss: 0.6657452118396759 (0.066s) | train acc: 0.6000 | train auc: 0.497163\n",
      "Batch 4650/6427 | loss: 0.6834245383739471 (0.066s) | train acc: 0.5997 | train auc: 0.496868\n",
      "Batch 4700/6427 | loss: 0.6734060800075531 (0.066s) | train acc: 0.5997 | train auc: 0.496894\n",
      "Batch 4750/6427 | loss: 0.6810778295993805 (0.066s) | train acc: 0.5995 | train auc: 0.497175\n",
      "Batch 4800/6427 | loss: 0.6716229045391082 (0.066s) | train acc: 0.5995 | train auc: 0.497105\n",
      "Batch 4850/6427 | loss: 0.6677332472801208 (0.066s) | train acc: 0.5997 | train auc: 0.497132\n",
      "Batch 4900/6427 | loss: 0.6734631776809692 (0.066s) | train acc: 0.5997 | train auc: 0.497111\n",
      "Batch 4950/6427 | loss: 0.6783525598049164 (0.066s) | train acc: 0.5996 | train auc: 0.497225\n",
      "Batch 5000/6427 | loss: 0.6717118108272553 (0.066s) | train acc: 0.5996 | train auc: 0.497171\n",
      "Batch 5050/6427 | loss: 0.6789435625076294 (0.066s) | train acc: 0.5995 | train auc: 0.497352\n",
      "Batch 5100/6427 | loss: 0.6727594447135925 (0.066s) | train acc: 0.5995 | train auc: 0.497318\n",
      "Batch 5150/6427 | loss: 0.6778242635726929 (0.066s) | train acc: 0.5994 | train auc: 0.497376\n",
      "Batch 5200/6427 | loss: 0.6695918774604798 (0.066s) | train acc: 0.5995 | train auc: 0.497220\n",
      "Batch 5250/6427 | loss: 0.6606351840496063 (0.066s) | train acc: 0.5998 | train auc: 0.497673\n",
      "Batch 5300/6427 | loss: 0.6705121505260467 (0.066s) | train acc: 0.5998 | train auc: 0.497552\n",
      "Batch 5350/6427 | loss: 0.6644882357120514 (0.066s) | train acc: 0.6000 | train auc: 0.497963\n",
      "Batch 5400/6427 | loss: 0.6717957711219787 (0.066s) | train acc: 0.6001 | train auc: 0.498068\n",
      "Batch 5450/6427 | loss: 0.6710406041145325 (0.066s) | train acc: 0.6001 | train auc: 0.498148\n",
      "Batch 5500/6427 | loss: 0.6752106845378876 (0.066s) | train acc: 0.6001 | train auc: 0.498078\n",
      "Batch 5550/6427 | loss: 0.673891053199768 (0.066s) | train acc: 0.6000 | train auc: 0.498039\n",
      "Batch 5600/6427 | loss: 0.6736884450912476 (0.066s) | train acc: 0.6000 | train auc: 0.497976\n",
      "Batch 5650/6427 | loss: 0.6654622089862824 (0.066s) | train acc: 0.6002 | train auc: 0.498137\n",
      "Batch 5700/6427 | loss: 0.6781396579742431 (0.066s) | train acc: 0.6001 | train auc: 0.498114\n",
      "Batch 5750/6427 | loss: 0.675854834318161 (0.067s) | train acc: 0.6000 | train auc: 0.498155\n",
      "Batch 5800/6427 | loss: 0.667468684911728 (0.066s) | train acc: 0.6002 | train auc: 0.497995\n",
      "Batch 5850/6427 | loss: 0.6716064822673797 (0.066s) | train acc: 0.6002 | train auc: 0.497975\n",
      "Batch 5900/6427 | loss: 0.6691564440727233 (0.066s) | train acc: 0.6003 | train auc: 0.498060\n",
      "Batch 5950/6427 | loss: 0.6734795093536377 (0.066s) | train acc: 0.6003 | train auc: 0.497887\n",
      "Batch 6000/6427 | loss: 0.6717391741275788 (0.067s) | train acc: 0.6003 | train auc: 0.497814\n",
      "Batch 6050/6427 | loss: 0.6754501080513 (0.066s) | train acc: 0.6002 | train auc: 0.497744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(att_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m      9\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matt_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m att_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/CSE-8803-MLG/biomedical-gnn-privacy/membership_inference_attack/ml_util.py:143\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataset, loss_fn, epochs, batch_size, val_dataset, save_freq, save_path, scheduler, device)\u001b[0m\n\u001b[1;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    142\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m--> 143\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    145\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "att_dataset = TensorDataset(s_logits, s_labels)\n",
    "target_dataset = TensorDataset(t_logits, t_labels)\n",
    "lr = 0.002\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "weight_decay = 1e-3\n",
    "att_model = GenericAttackModel(num_feat=2).to(DEVICE)\n",
    "optimizer = optim.Adam(att_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(att_model, optimizer, att_dataset, loss_fn, epochs, batch_size, val_dataset=target_dataset, device=DEVICE)\n",
    "att_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52b939c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5825846698716655"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scores_test = predict(att_model, target_dataset, device=DEVICE, logits=True, return_type='pt').cpu()\n",
    "true_scores_test = target_dataset.tensors[1]\n",
    "get_auroc_score(pred_scores_test, true_scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022658b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e4508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
