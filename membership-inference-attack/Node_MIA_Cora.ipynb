{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b116ae6f",
   "metadata": {},
   "source": [
    "# Old version of membership inference attack (MIA)\n",
    "## Keenan Hom\n",
    "The original choice for MIA is here: https://arxiv.org/pdf/2102.05429. Because there was no provided code, I attempted to reproduce it from the descriptions in the paper, but was not able to achieve the same success that the authors did. In addition, this architecture is poorly suited to our project's needs because this attack method only works on a GNN trained on a single large graph, but all of our datasets are multi-graph.\n",
    "\n",
    "This notebook attempts to reproduce the exact methodology as described in the paper, on the Cora dataset. I can barely reach 62% accuracy during training, which falls to <55% during validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f23ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a64f866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric import nn as gnn, transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from ml_util import train_model, train_model_single_graph, get_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b791fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = ('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32259522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cora_transform(data):\n",
    "    '''\n",
    "    Transform Cora data. Performs the following operations:\n",
    "    - Turn y labels into one-hot vectors\n",
    "    '''\n",
    "    data.y = torch.Tensor(OneHotEncoder().fit_transform(data.y.reshape(-1,1)).todense())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7360bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_zero_hop(model, v_feat):\n",
    "    '''\n",
    "    Perform a zero-hop query on a trained model. The only input is the node and its features, with a self loop\n",
    "    to itself.\n",
    "    \n",
    "    v_feat should be a 1D vector.\n",
    "    '''\n",
    "#     v_tar_x = d.x[0].unsqueeze(0).to(DEVICE)\n",
    "    edge_index = torch.tensor([[0], [0]], dtype=int).to(DEVICE)\n",
    "#     d_tar = Data(x=v_tar_x, edge_index=edge_index).to(DEVICE)\n",
    "    return model(v_feat.unsqueeze(0).to(DEVICE), edge_index)\n",
    "\n",
    "\n",
    "def zero_hop_acc(model, data, mask):\n",
    "    '''Get the accuracy of the model by only inputting 0-hop graphs; i.e. only the features of the node of interest and nothing else'''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = torch.stack([query_zero_hop(model, v_feat).cpu().flatten() for v_feat in data.x[mask]])\n",
    "        y = data.y[mask].cpu()\n",
    "        return get_accuracy(pred, y)\n",
    "    \n",
    "def split_in_half(data):\n",
    "    '''Split a Data object in half. Severs edges between the two halves and preserves edges within each half'''\n",
    "    half_split_transform = T.RandomNodeSplit(split='train_rest', num_val=0.5, num_test=0.)\n",
    "    return half_split_transform(data)\n",
    "    \n",
    "\n",
    "def ind_to_mask(ind, size):\n",
    "    '''Takes a list of indices and returns a boolean mask which is True at every spot specified by the indices'''\n",
    "    return np.isin(np.arange(size), ind)\n",
    "    \n",
    "def target_shadow_split(data):\n",
    "    '''\n",
    "    Split data randomly and equally into two objects, one for target model training and one for shadow model training. Each \n",
    "    Data object is also further split by half, as their train and test sets.\n",
    "    '''\n",
    "    n_nodes = data.x.shape[0]\n",
    "    data_halved = split_in_half(data)\n",
    "    t_ind, s_ind = torch.where(data_halved.train_mask)[0], torch.where(~data_halved.train_mask)[0]\n",
    "    \n",
    "    # Shuffle target indices; then assign first half as train and second half as test\n",
    "    t_ind_shuffle = np.random.choice(t_ind, size=len(t_ind), replace=False)\n",
    "    t_ind_train, t_ind_test = np.sort(t_ind_shuffle[:len(t_ind)//2]), np.sort(t_ind_shuffle[len(t_ind)//2:])\n",
    "    \n",
    "    # Shuffle shadow indices; then assign first half as train and second half as test\n",
    "    s_ind_shuffle = np.random.choice(s_ind, size=len(s_ind), replace=False)\n",
    "    s_ind_train, s_ind_test = np.sort(s_ind_shuffle[:len(s_ind)//2]), np.sort(s_ind_shuffle[len(s_ind)//2:])\n",
    "    \n",
    "    # Assign masks to Data objects\n",
    "    t_data = data.clone()\n",
    "    t_data.train_mask = ind_to_mask(t_ind_train, n_nodes)\n",
    "    t_data.test_mask = ind_to_mask(t_ind_test, n_nodes)\n",
    "    t_data.val_mask = t_data.test_mask\n",
    "    \n",
    "    s_data = data.clone()\n",
    "    s_data.train_mask = ind_to_mask(s_ind_train, n_nodes)\n",
    "    s_data.test_mask = ind_to_mask(s_ind_test, n_nodes)\n",
    "    s_data.val_mask = s_data.test_mask\n",
    "    \n",
    "    return t_data, s_data\n",
    "\n",
    "\n",
    "def create_attack_dataset(model, data):\n",
    "    '''\n",
    "    Creates the attack dataset using 0-hop querying on a trained shadow model. Each data pair consists is generated from a node feature v. The x variable is the posterior\n",
    "    generated by the shadow model for input v, and the label is binary- True if v is in the shadow model's train dataset, and False if it was in the test/val dataset.\n",
    "    \n",
    "    s_model (nn.Module): GNN trained on the shadow dataset\n",
    "    s_data (torch_geometric.data.Data): Data object representing a graph, with appropriate train_mask and test_mask defined.\n",
    "    '''\n",
    "    \n",
    "    feat = data.x[data.train_mask | data.test_mask]    \n",
    "    posteriors = F.softmax(torch.vstack([query_zero_hop(model, v_feat).sort()[0] for v_feat in feat]).detach().cpu(), dim=1)\n",
    "    membership = torch.tensor(np.array([data.train_mask[i] for i in range(data.num_nodes) if (data.train_mask[i] or data.test_mask[i])]), dtype=torch.int)\n",
    "    membership = torch.Tensor(OneHotEncoder().fit_transform(membership.reshape(-1,1)).todense())\n",
    "    return posteriors, membership\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9513821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericGATModel(nn.Module):\n",
    "    def __init__(self, num_feat, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gat1 = gnn.conv.GATConv(num_feat, 32, heads=2, dropout=0.5)\n",
    "        self.gat2 = gnn.conv.GATConv(32 * 2, num_classes, heads=1, dropout=0.5)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        out1 = self.gat1(x, edge_index)\n",
    "        out2 = self.relu(out1)\n",
    "        out3 = self.gat2(out2, edge_index)\n",
    "        \n",
    "        return out3\n",
    "    \n",
    "\n",
    "class GenericAttackModel(nn.Module):\n",
    "    def __init__(self, num_feat):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_feat, 128),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(128, 128),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2),\n",
    "#             nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class GenericDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        '''\n",
    "        Generic dataset for training PyTorch models/\n",
    "        \n",
    "        x should be shape (num_samples, num_features)\n",
    "        y should be shape (num_samples, num_classes) and be one-hot encoded\n",
    "        '''\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30cbe2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/home/hice1/khom9/scratch/CSE-8803-MLG-Data/', name='Citeseer', split='full', transform=cora_transform)\n",
    "\n",
    "d = dataset[0]\n",
    "t_data, s_data = target_shadow_split(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d67572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.003\n",
      "No learning rate scheduling!\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/50-----\n",
      "Loss: 1.8160910606384277 (0.714s), train acc: 0.147, val loss: 1.718, val acc: 0.430\n",
      "\n",
      "-----Epoch 2/50-----\n",
      "Loss: 1.6944130659103394 (0.004s), train acc: 0.446, val loss: 1.633, val acc: 0.635\n",
      "\n",
      "-----Epoch 3/50-----\n",
      "Loss: 1.5997278690338135 (0.003s), train acc: 0.600, val loss: 1.533, val acc: 0.694\n",
      "\n",
      "-----Epoch 4/50-----\n",
      "Loss: 1.5035320520401 (0.003s), train acc: 0.656, val loss: 1.425, val acc: 0.716\n",
      "\n",
      "-----Epoch 5/50-----\n",
      "Loss: 1.3770817518234253 (0.003s), train acc: 0.677, val loss: 1.316, val acc: 0.731\n",
      "\n",
      "-----Epoch 6/50-----\n",
      "Loss: 1.2857917547225952 (0.003s), train acc: 0.693, val loss: 1.217, val acc: 0.737\n",
      "\n",
      "-----Epoch 7/50-----\n",
      "Loss: 1.1790015697479248 (0.003s), train acc: 0.716, val loss: 1.131, val acc: 0.737\n",
      "\n",
      "-----Epoch 8/50-----\n",
      "Loss: 1.0977574586868286 (0.003s), train acc: 0.688, val loss: 1.060, val acc: 0.744\n",
      "\n",
      "-----Epoch 9/50-----\n",
      "Loss: 1.0496230125427246 (0.003s), train acc: 0.702, val loss: 1.004, val acc: 0.749\n",
      "\n",
      "-----Epoch 10/50-----\n",
      "Loss: 0.9938309192657471 (0.003s), train acc: 0.706, val loss: 0.960, val acc: 0.744\n",
      "\n",
      "-----Epoch 11/50-----\n",
      "Loss: 0.9018989205360413 (0.003s), train acc: 0.741, val loss: 0.926, val acc: 0.736\n",
      "\n",
      "-----Epoch 12/50-----\n",
      "Loss: 0.8621159195899963 (0.003s), train acc: 0.732, val loss: 0.899, val acc: 0.736\n",
      "\n",
      "-----Epoch 13/50-----\n",
      "Loss: 0.8494904041290283 (0.003s), train acc: 0.751, val loss: 0.877, val acc: 0.737\n",
      "\n",
      "-----Epoch 14/50-----\n",
      "Loss: 0.8407929539680481 (0.003s), train acc: 0.747, val loss: 0.859, val acc: 0.731\n",
      "\n",
      "-----Epoch 15/50-----\n",
      "Loss: 0.7864024639129639 (0.003s), train acc: 0.758, val loss: 0.845, val acc: 0.727\n",
      "\n",
      "-----Epoch 16/50-----\n",
      "Loss: 0.7957636117935181 (0.003s), train acc: 0.757, val loss: 0.835, val acc: 0.731\n",
      "\n",
      "-----Epoch 17/50-----\n",
      "Loss: 0.7358121275901794 (0.003s), train acc: 0.759, val loss: 0.829, val acc: 0.733\n",
      "\n",
      "-----Epoch 18/50-----\n",
      "Loss: 0.6830822825431824 (0.003s), train acc: 0.777, val loss: 0.826, val acc: 0.731\n",
      "\n",
      "-----Epoch 19/50-----\n",
      "Loss: 0.6701412200927734 (0.003s), train acc: 0.780, val loss: 0.826, val acc: 0.726\n",
      "\n",
      "-----Epoch 20/50-----\n",
      "Loss: 0.6781519651412964 (0.003s), train acc: 0.787, val loss: 0.830, val acc: 0.722\n",
      "\n",
      "-----Epoch 21/50-----\n",
      "Loss: 0.6334705948829651 (0.003s), train acc: 0.781, val loss: 0.834, val acc: 0.721\n",
      "\n",
      "-----Epoch 22/50-----\n",
      "Loss: 0.6508257389068604 (0.003s), train acc: 0.770, val loss: 0.839, val acc: 0.721\n",
      "\n",
      "-----Epoch 23/50-----\n",
      "Loss: 0.628941535949707 (0.003s), train acc: 0.783, val loss: 0.846, val acc: 0.715\n",
      "\n",
      "-----Epoch 24/50-----\n",
      "Loss: 0.6321904063224792 (0.003s), train acc: 0.779, val loss: 0.854, val acc: 0.714\n",
      "\n",
      "-----Epoch 25/50-----\n",
      "Loss: 0.5994032025337219 (0.003s), train acc: 0.791, val loss: 0.863, val acc: 0.710\n",
      "\n",
      "-----Epoch 26/50-----\n",
      "Loss: 0.5772404670715332 (0.003s), train acc: 0.813, val loss: 0.872, val acc: 0.708\n",
      "\n",
      "-----Epoch 27/50-----\n",
      "Loss: 0.5815119743347168 (0.003s), train acc: 0.792, val loss: 0.882, val acc: 0.708\n",
      "\n",
      "-----Epoch 28/50-----\n",
      "Loss: 0.6222436428070068 (0.003s), train acc: 0.770, val loss: 0.891, val acc: 0.709\n",
      "\n",
      "-----Epoch 29/50-----\n",
      "Loss: 0.5595848560333252 (0.003s), train acc: 0.815, val loss: 0.899, val acc: 0.708\n",
      "\n",
      "-----Epoch 30/50-----\n",
      "Loss: 0.5488611459732056 (0.003s), train acc: 0.793, val loss: 0.906, val acc: 0.709\n",
      "\n",
      "-----Epoch 31/50-----\n",
      "Loss: 0.566430389881134 (0.003s), train acc: 0.806, val loss: 0.914, val acc: 0.712\n",
      "\n",
      "-----Epoch 32/50-----\n",
      "Loss: 0.543716549873352 (0.003s), train acc: 0.821, val loss: 0.923, val acc: 0.710\n",
      "\n",
      "-----Epoch 33/50-----\n",
      "Loss: 0.53822922706604 (0.003s), train acc: 0.809, val loss: 0.932, val acc: 0.713\n",
      "\n",
      "-----Epoch 34/50-----\n",
      "Loss: 0.53575599193573 (0.003s), train acc: 0.797, val loss: 0.941, val acc: 0.715\n",
      "\n",
      "-----Epoch 35/50-----\n",
      "Loss: 0.5186890959739685 (0.003s), train acc: 0.822, val loss: 0.950, val acc: 0.716\n",
      "\n",
      "-----Epoch 36/50-----\n",
      "Loss: 0.46323907375335693 (0.003s), train acc: 0.834, val loss: 0.959, val acc: 0.715\n",
      "\n",
      "-----Epoch 37/50-----\n",
      "Loss: 0.5221248269081116 (0.003s), train acc: 0.806, val loss: 0.969, val acc: 0.715\n",
      "\n",
      "-----Epoch 38/50-----\n",
      "Loss: 0.5300210118293762 (0.003s), train acc: 0.798, val loss: 0.978, val acc: 0.715\n",
      "\n",
      "-----Epoch 39/50-----\n",
      "Loss: 0.5024362206459045 (0.003s), train acc: 0.823, val loss: 0.988, val acc: 0.718\n",
      "\n",
      "-----Epoch 40/50-----\n",
      "Loss: 0.47939059138298035 (0.003s), train acc: 0.829, val loss: 0.998, val acc: 0.713\n",
      "\n",
      "-----Epoch 41/50-----\n",
      "Loss: 0.4928744435310364 (0.003s), train acc: 0.815, val loss: 1.008, val acc: 0.709\n",
      "\n",
      "-----Epoch 42/50-----\n",
      "Loss: 0.4843374192714691 (0.003s), train acc: 0.822, val loss: 1.017, val acc: 0.709\n",
      "\n",
      "-----Epoch 43/50-----\n",
      "Loss: 0.48161131143569946 (0.003s), train acc: 0.823, val loss: 1.026, val acc: 0.709\n",
      "\n",
      "-----Epoch 44/50-----\n",
      "Loss: 0.5160821080207825 (0.003s), train acc: 0.801, val loss: 1.035, val acc: 0.710\n",
      "\n",
      "-----Epoch 45/50-----\n",
      "Loss: 0.4789634346961975 (0.003s), train acc: 0.822, val loss: 1.044, val acc: 0.709\n",
      "\n",
      "-----Epoch 46/50-----\n",
      "Loss: 0.5170614123344421 (0.003s), train acc: 0.816, val loss: 1.053, val acc: 0.709\n",
      "\n",
      "-----Epoch 47/50-----\n",
      "Loss: 0.48076239228248596 (0.003s), train acc: 0.817, val loss: 1.062, val acc: 0.708\n",
      "\n",
      "-----Epoch 48/50-----\n",
      "Loss: 0.47028353810310364 (0.003s), train acc: 0.821, val loss: 1.071, val acc: 0.710\n",
      "\n",
      "-----Epoch 49/50-----\n",
      "Loss: 0.46167227625846863 (0.003s), train acc: 0.839, val loss: 1.081, val acc: 0.710\n",
      "\n",
      "-----Epoch 50/50-----\n",
      "Loss: 0.48991385102272034 (0.003s), train acc: 0.804, val loss: 1.089, val acc: 0.709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenericGATModel(\n",
       "  (gat1): GATConv(3703, 32, heads=2)\n",
       "  (gat2): GATConv(64, 6, heads=1)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.003\n",
    "epochs = 50\n",
    "t_model = GenericGATModel(num_feat=3703, num_classes=6).to(DEVICE)\n",
    "optimizer = optim.Adam(t_model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model_single_graph(t_model, optimizer, t_data, loss_fn, epochs, device=DEVICE)\n",
    "t_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9fc812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.003\n",
      "No learning rate scheduling!\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/50-----\n",
      "Loss: 1.801956057548523 (0.003s), train acc: 0.178, val loss: 1.707, val acc: 0.470\n",
      "\n",
      "-----Epoch 2/50-----\n",
      "Loss: 1.682558298110962 (0.003s), train acc: 0.469, val loss: 1.612, val acc: 0.614\n",
      "\n",
      "-----Epoch 3/50-----\n",
      "Loss: 1.5636154413223267 (0.003s), train acc: 0.607, val loss: 1.507, val acc: 0.657\n",
      "\n",
      "-----Epoch 4/50-----\n",
      "Loss: 1.42742121219635 (0.003s), train acc: 0.668, val loss: 1.397, val acc: 0.694\n",
      "\n",
      "-----Epoch 5/50-----\n",
      "Loss: 1.3284965753555298 (0.003s), train acc: 0.675, val loss: 1.293, val acc: 0.710\n",
      "\n",
      "-----Epoch 6/50-----\n",
      "Loss: 1.1905628442764282 (0.003s), train acc: 0.704, val loss: 1.196, val acc: 0.731\n",
      "\n",
      "-----Epoch 7/50-----\n",
      "Loss: 1.1277176141738892 (0.003s), train acc: 0.685, val loss: 1.111, val acc: 0.737\n",
      "\n",
      "-----Epoch 8/50-----\n",
      "Loss: 1.0200189352035522 (0.003s), train acc: 0.734, val loss: 1.040, val acc: 0.737\n",
      "\n",
      "-----Epoch 9/50-----\n",
      "Loss: 0.9781537055969238 (0.003s), train acc: 0.721, val loss: 0.982, val acc: 0.745\n",
      "\n",
      "-----Epoch 10/50-----\n",
      "Loss: 0.8892552852630615 (0.003s), train acc: 0.761, val loss: 0.935, val acc: 0.740\n",
      "\n",
      "-----Epoch 11/50-----\n",
      "Loss: 0.8634620308876038 (0.003s), train acc: 0.758, val loss: 0.899, val acc: 0.744\n",
      "\n",
      "-----Epoch 12/50-----\n",
      "Loss: 0.8476202487945557 (0.003s), train acc: 0.764, val loss: 0.873, val acc: 0.746\n",
      "\n",
      "-----Epoch 13/50-----\n",
      "Loss: 0.8320804238319397 (0.003s), train acc: 0.758, val loss: 0.855, val acc: 0.742\n",
      "\n",
      "-----Epoch 14/50-----\n",
      "Loss: 0.7824954390525818 (0.003s), train acc: 0.779, val loss: 0.842, val acc: 0.736\n",
      "\n",
      "-----Epoch 15/50-----\n",
      "Loss: 0.7596983909606934 (0.003s), train acc: 0.763, val loss: 0.833, val acc: 0.733\n",
      "\n",
      "-----Epoch 16/50-----\n",
      "Loss: 0.686355710029602 (0.003s), train acc: 0.797, val loss: 0.827, val acc: 0.730\n",
      "\n",
      "-----Epoch 17/50-----\n",
      "Loss: 0.7147409915924072 (0.003s), train acc: 0.788, val loss: 0.823, val acc: 0.738\n",
      "\n",
      "-----Epoch 18/50-----\n",
      "Loss: 0.6931411027908325 (0.003s), train acc: 0.781, val loss: 0.821, val acc: 0.737\n",
      "\n",
      "-----Epoch 19/50-----\n",
      "Loss: 0.7013468146324158 (0.003s), train acc: 0.775, val loss: 0.820, val acc: 0.732\n",
      "\n",
      "-----Epoch 20/50-----\n",
      "Loss: 0.6158124208450317 (0.003s), train acc: 0.805, val loss: 0.820, val acc: 0.731\n",
      "\n",
      "-----Epoch 21/50-----\n",
      "Loss: 0.643548846244812 (0.003s), train acc: 0.792, val loss: 0.820, val acc: 0.731\n",
      "\n",
      "-----Epoch 22/50-----\n",
      "Loss: 0.5968687534332275 (0.003s), train acc: 0.803, val loss: 0.822, val acc: 0.734\n",
      "\n",
      "-----Epoch 23/50-----\n",
      "Loss: 0.649170994758606 (0.003s), train acc: 0.776, val loss: 0.826, val acc: 0.733\n",
      "\n",
      "-----Epoch 24/50-----\n",
      "Loss: 0.6026120185852051 (0.003s), train acc: 0.799, val loss: 0.832, val acc: 0.725\n",
      "\n",
      "-----Epoch 25/50-----\n",
      "Loss: 0.6069155335426331 (0.003s), train acc: 0.791, val loss: 0.840, val acc: 0.726\n",
      "\n",
      "-----Epoch 26/50-----\n",
      "Loss: 0.5878894925117493 (0.003s), train acc: 0.804, val loss: 0.849, val acc: 0.724\n",
      "\n",
      "-----Epoch 27/50-----\n",
      "Loss: 0.5619124174118042 (0.003s), train acc: 0.805, val loss: 0.857, val acc: 0.724\n",
      "\n",
      "-----Epoch 28/50-----\n",
      "Loss: 0.5716946721076965 (0.003s), train acc: 0.808, val loss: 0.866, val acc: 0.725\n",
      "\n",
      "-----Epoch 29/50-----\n",
      "Loss: 0.5613798499107361 (0.003s), train acc: 0.814, val loss: 0.875, val acc: 0.726\n",
      "\n",
      "-----Epoch 30/50-----\n",
      "Loss: 0.5715871453285217 (0.003s), train acc: 0.803, val loss: 0.883, val acc: 0.727\n",
      "\n",
      "-----Epoch 31/50-----\n",
      "Loss: 0.5765340924263 (0.003s), train acc: 0.802, val loss: 0.892, val acc: 0.727\n",
      "\n",
      "-----Epoch 32/50-----\n",
      "Loss: 0.5134755373001099 (0.003s), train acc: 0.810, val loss: 0.900, val acc: 0.726\n",
      "\n",
      "-----Epoch 33/50-----\n",
      "Loss: 0.5521758198738098 (0.003s), train acc: 0.797, val loss: 0.907, val acc: 0.722\n",
      "\n",
      "-----Epoch 34/50-----\n",
      "Loss: 0.5495298504829407 (0.003s), train acc: 0.810, val loss: 0.916, val acc: 0.722\n",
      "\n",
      "-----Epoch 35/50-----\n",
      "Loss: 0.5448267459869385 (0.003s), train acc: 0.805, val loss: 0.925, val acc: 0.721\n",
      "\n",
      "-----Epoch 36/50-----\n",
      "Loss: 0.5186458230018616 (0.003s), train acc: 0.810, val loss: 0.934, val acc: 0.721\n",
      "\n",
      "-----Epoch 37/50-----\n",
      "Loss: 0.5178734064102173 (0.003s), train acc: 0.822, val loss: 0.943, val acc: 0.720\n",
      "\n",
      "-----Epoch 38/50-----\n",
      "Loss: 0.5437580347061157 (0.003s), train acc: 0.799, val loss: 0.952, val acc: 0.721\n",
      "\n",
      "-----Epoch 39/50-----\n",
      "Loss: 0.5594996213912964 (0.003s), train acc: 0.799, val loss: 0.960, val acc: 0.721\n",
      "\n",
      "-----Epoch 40/50-----\n",
      "Loss: 0.48583468794822693 (0.003s), train acc: 0.826, val loss: 0.965, val acc: 0.719\n",
      "\n",
      "-----Epoch 41/50-----\n",
      "Loss: 0.5349532961845398 (0.003s), train acc: 0.793, val loss: 0.969, val acc: 0.721\n",
      "\n",
      "-----Epoch 42/50-----\n",
      "Loss: 0.5324484705924988 (0.003s), train acc: 0.798, val loss: 0.971, val acc: 0.721\n",
      "\n",
      "-----Epoch 43/50-----\n",
      "Loss: 0.5435412526130676 (0.003s), train acc: 0.803, val loss: 0.974, val acc: 0.724\n",
      "\n",
      "-----Epoch 44/50-----\n",
      "Loss: 0.49966901540756226 (0.003s), train acc: 0.812, val loss: 0.976, val acc: 0.727\n",
      "\n",
      "-----Epoch 45/50-----\n",
      "Loss: 0.4875886142253876 (0.003s), train acc: 0.816, val loss: 0.979, val acc: 0.727\n",
      "\n",
      "-----Epoch 46/50-----\n",
      "Loss: 0.46499454975128174 (0.003s), train acc: 0.840, val loss: 0.984, val acc: 0.725\n",
      "\n",
      "-----Epoch 47/50-----\n",
      "Loss: 0.4610865116119385 (0.003s), train acc: 0.829, val loss: 0.989, val acc: 0.724\n",
      "\n",
      "-----Epoch 48/50-----\n",
      "Loss: 0.4642738401889801 (0.003s), train acc: 0.835, val loss: 0.997, val acc: 0.721\n",
      "\n",
      "-----Epoch 49/50-----\n",
      "Loss: 0.5295608043670654 (0.003s), train acc: 0.799, val loss: 1.005, val acc: 0.724\n",
      "\n",
      "-----Epoch 50/50-----\n",
      "Loss: 0.4729023277759552 (0.003s), train acc: 0.816, val loss: 1.013, val acc: 0.725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenericGATModel(\n",
       "  (gat1): GATConv(3703, 32, heads=2)\n",
       "  (gat2): GATConv(64, 6, heads=1)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.003\n",
    "epochs = 50\n",
    "s_model = GenericGATModel(num_feat=3703, num_classes=6).to(DEVICE)\n",
    "optimizer = optim.Adam(s_model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model_single_graph(s_model, optimizer, s_data, loss_fn, epochs, device=DEVICE)\n",
    "s_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67b9a577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "No learning rate scheduling!\n",
      "Training for 500 epochs, with batch size=32\n",
      "Using device: cuda:0\n",
      "WARNING: Will not save model!\n",
      "\n",
      "-----Epoch 1/500-----\n",
      "Batch 10/52, loss: 0.6910130023956299 (0.076s), train acc: 0.531\n",
      "Batch 20/52, loss: 0.68507319688797 (0.010s), train acc: 0.570\n",
      "Batch 30/52, loss: 0.6858041524887085 (0.010s), train acc: 0.561\n",
      "Batch 40/52, loss: 0.6884221792221069 (0.010s), train acc: 0.555\n",
      "Batch 50/52, loss: 0.6780547499656677 (0.010s), train acc: 0.556\n",
      "Batch 52/52, loss: 0.6856201589107513 (0.003s), train acc: 0.556\n",
      "\n",
      "-----Epoch 2/500-----\n",
      "Batch 10/52, loss: 0.6743933022022247 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6737410306930542 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6690486311912537 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6707178831100464 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6740832388401031 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6962690353393555 (0.003s), train acc: 0.599\n",
      "\n",
      "-----Epoch 3/500-----\n",
      "Batch 10/52, loss: 0.669802862405777 (0.009s), train acc: 0.569\n",
      "Batch 20/52, loss: 0.67272047996521 (0.010s), train acc: 0.581\n",
      "Batch 30/52, loss: 0.6545913577079773 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6498487651348114 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6715994358062745 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6428229510784149 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 4/500-----\n",
      "Batch 10/52, loss: 0.6703925251960754 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6579486727714539 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6610949754714965 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6408780932426452 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6601973235607147 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6698022186756134 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 5/500-----\n",
      "Batch 10/52, loss: 0.6594307899475098 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.660272216796875 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6624974250793457 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6488754570484161 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.657314908504486 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6966401040554047 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 6/500-----\n",
      "Batch 10/52, loss: 0.6651530623435974 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6500122606754303 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6793009817600251 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6372681617736816 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6550879061222077 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6755409240722656 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 7/500-----\n",
      "Batch 10/52, loss: 0.6485387146472931 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6488752424716949 (0.010s), train acc: 0.642\n",
      "Batch 30/52, loss: 0.6433162689208984 (0.010s), train acc: 0.633\n",
      "Batch 40/52, loss: 0.6840716540813446 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6707719504833222 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6513046324253082 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 8/500-----\n",
      "Batch 10/52, loss: 0.6402106285095215 (0.009s), train acc: 0.641\n",
      "Batch 20/52, loss: 0.634781563282013 (0.010s), train acc: 0.637\n",
      "Batch 30/52, loss: 0.6696784496307373 (0.010s), train acc: 0.625\n",
      "Batch 40/52, loss: 0.6525648653507232 (0.010s), train acc: 0.618\n",
      "Batch 50/52, loss: 0.690272468328476 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6397269070148468 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 9/500-----\n",
      "Batch 10/52, loss: 0.6759067177772522 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6466848671436309 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6662009656429291 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6435782611370087 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6535344660282135 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6302458643913269 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 10/500-----\n",
      "Batch 10/52, loss: 0.664017504453659 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.6283738076686859 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6554988086223602 (0.010s), train acc: 0.618\n",
      "Batch 40/52, loss: 0.6720119059085846 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6623042941093444 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6670053005218506 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 11/500-----\n",
      "Batch 10/52, loss: 0.6617211401462555 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6535171508789063 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6565895676612854 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6360201597213745 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6627131521701812 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.73137167096138 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 12/500-----\n",
      "Batch 10/52, loss: 0.6654895305633545 (0.009s), train acc: 0.572\n",
      "Batch 20/52, loss: 0.6467770278453827 (0.010s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.6428862512111664 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6661724984645844 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6614211678504944 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6593033373355865 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 13/500-----\n",
      "Batch 10/52, loss: 0.6340399265289307 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6615186214447022 (0.010s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.663578850030899 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6770032584667206 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6385125458240509 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6727702915668488 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 14/500-----\n",
      "Batch 10/52, loss: 0.667366111278534 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6553891658782959 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6565569698810577 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6549383223056793 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6506122350692749 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6478285789489746 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 15/500-----\n",
      "Batch 10/52, loss: 0.6253759503364563 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6680745899677276 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6391512453556061 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6783990323543548 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.67591992020607 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6482530534267426 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 16/500-----\n",
      "Batch 10/52, loss: 0.6357261955738067 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6530270278453827 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6451683223247529 (0.010s), train acc: 0.618\n",
      "Batch 40/52, loss: 0.681036776304245 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6553740799427032 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6980012059211731 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 17/500-----\n",
      "Batch 10/52, loss: 0.6643281936645508 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6547686874866485 (0.010s), train acc: 0.591\n",
      "Batch 30/52, loss: 0.6423586964607239 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6605066180229187 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6594255328178406 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6602280735969543 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 18/500-----\n",
      "Batch 10/52, loss: 0.6408827543258667 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6474595606327057 (0.010s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6798878252506256 (0.010s), train acc: 0.590\n",
      "Batch 40/52, loss: 0.635006719827652 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6785439848899841 (0.010s), train acc: 0.598\n",
      "Batch 52/52, loss: 0.6330470740795135 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 19/500-----\n",
      "Batch 10/52, loss: 0.6434791803359985 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6712973058223725 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6694315016269684 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.657807219028473 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6358865857124328 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6558730006217957 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 20/500-----\n",
      "Batch 10/52, loss: 0.6506546318531037 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6471923172473908 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6609491169452667 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6452851474285126 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6748926103115082 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6632950007915497 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 21/500-----\n",
      "Batch 10/52, loss: 0.6595725119113922 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6677422404289246 (0.010s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6524828612804413 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6650666832923889 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.6350869536399841 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6560774147510529 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 22/500-----\n",
      "Batch 10/52, loss: 0.6819961309432984 (0.009s), train acc: 0.556\n",
      "Batch 20/52, loss: 0.6494682192802429 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6384397506713867 (0.010s), train acc: 0.601\n",
      "Batch 40/52, loss: 0.6548715174198151 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6561671197414398 (0.010s), train acc: 0.600\n",
      "Batch 52/52, loss: 0.6504270136356354 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 23/500-----\n",
      "Batch 10/52, loss: 0.6454519510269165 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6606548070907593 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6465411365032196 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6778305888175964 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6548308312892914 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6389903426170349 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 24/500-----\n",
      "Batch 10/52, loss: 0.6585335552692413 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6580238461494445 (0.009s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6446188688278198 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6678818583488464 (0.010s), train acc: 0.594\n",
      "Batch 50/52, loss: 0.6549590051174163 (0.010s), train acc: 0.600\n",
      "Batch 52/52, loss: 0.6783865988254547 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 25/500-----\n",
      "Batch 10/52, loss: 0.6810908615589142 (0.009s), train acc: 0.572\n",
      "Batch 20/52, loss: 0.6581264078617096 (0.010s), train acc: 0.575\n",
      "Batch 30/52, loss: 0.6360699713230134 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6597648620605469 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6474220454692841 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6715802252292633 (0.003s), train acc: 0.601\n",
      "\n",
      "-----Epoch 26/500-----\n",
      "Batch 10/52, loss: 0.6616428196430206 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6355192720890045 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6626825749874115 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6547403216362 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6589456081390381 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6708643734455109 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 27/500-----\n",
      "Batch 10/52, loss: 0.655826872587204 (0.009s), train acc: 0.553\n",
      "Batch 20/52, loss: 0.6421495676040649 (0.010s), train acc: 0.589\n",
      "Batch 30/52, loss: 0.6755911707878113 (0.010s), train acc: 0.593\n",
      "Batch 40/52, loss: 0.6584607303142548 (0.010s), train acc: 0.587\n",
      "Batch 50/52, loss: 0.6500684440135955 (0.010s), train acc: 0.593\n",
      "Batch 52/52, loss: 0.6397576928138733 (0.003s), train acc: 0.596\n",
      "\n",
      "-----Epoch 28/500-----\n",
      "Batch 10/52, loss: 0.663360208272934 (0.009s), train acc: 0.562\n",
      "Batch 20/52, loss: 0.6538387537002563 (0.010s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6667252480983734 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6682515442371368 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6348548829555511 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6469347774982452 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 29/500-----\n",
      "Batch 10/52, loss: 0.6701916992664337 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.658412367105484 (0.009s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6470667600631714 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6648828864097596 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6335962057113648 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6872066557407379 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 30/500-----\n",
      "Batch 10/52, loss: 0.6367790043354035 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.6732816100120544 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.657679808139801 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6801691770553588 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6351100146770478 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6484148502349854 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 31/500-----\n",
      "Batch 10/52, loss: 0.6623796939849853 (0.009s), train acc: 0.572\n",
      "Batch 20/52, loss: 0.6692412793636322 (0.009s), train acc: 0.586\n",
      "Batch 30/52, loss: 0.6621433675289154 (0.010s), train acc: 0.590\n",
      "Batch 40/52, loss: 0.6430744290351867 (0.010s), train acc: 0.591\n",
      "Batch 50/52, loss: 0.6457199633121491 (0.010s), train acc: 0.598\n",
      "Batch 52/52, loss: 0.6377125680446625 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 32/500-----\n",
      "Batch 10/52, loss: 0.6582980513572693 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6502307474613189 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6475311756134033 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6838163137435913 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6402155458927155 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6595981121063232 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 33/500-----\n",
      "Batch 10/52, loss: 0.6355570137500763 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6745288610458374 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6630352973937989 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6452989697456359 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6572453439235687 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6946959495544434 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 34/500-----\n",
      "Batch 10/52, loss: 0.6673375248908997 (0.009s), train acc: 0.569\n",
      "Batch 20/52, loss: 0.6445258557796478 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6737391769886016 (0.010s), train acc: 0.589\n",
      "Batch 40/52, loss: 0.6547234296798706 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6488028049468995 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6268666088581085 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 35/500-----\n",
      "Batch 10/52, loss: 0.6567723155021667 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6341389358043671 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6602491199970245 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6636951982975006 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.659468287229538 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.689680814743042 (0.003s), train acc: 0.601\n",
      "\n",
      "-----Epoch 36/500-----\n",
      "Batch 10/52, loss: 0.6497926533222198 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6727296650409699 (0.010s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6781798124313354 (0.010s), train acc: 0.590\n",
      "Batch 40/52, loss: 0.6409485399723053 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6387441277503967 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6666190922260284 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 37/500-----\n",
      "Batch 10/52, loss: 0.641932213306427 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6752361357212067 (0.010s), train acc: 0.569\n",
      "Batch 30/52, loss: 0.6371251344680786 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6697509527206421 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6569113850593566 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6405009329319 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 38/500-----\n",
      "Batch 10/52, loss: 0.6492525219917298 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6612017214298248 (0.009s), train acc: 0.591\n",
      "Batch 30/52, loss: 0.6631870448589325 (0.010s), train acc: 0.594\n",
      "Batch 40/52, loss: 0.6556317269802093 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6528449416160583 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6638059020042419 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 39/500-----\n",
      "Batch 10/52, loss: 0.6414387702941895 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6572009682655334 (0.009s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6643749296665191 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6460711538791657 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6611886203289032 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6973754465579987 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 40/500-----\n",
      "Batch 10/52, loss: 0.6505812406539917 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6807665705680848 (0.010s), train acc: 0.588\n",
      "Batch 30/52, loss: 0.6477228343486786 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6447431683540344 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.6633717179298401 (0.010s), train acc: 0.600\n",
      "Batch 52/52, loss: 0.6191084384918213 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 41/500-----\n",
      "Batch 10/52, loss: 0.668829619884491 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6420550405979156 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6668908953666687 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6353502452373505 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6615029156208039 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6751469671726227 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 42/500-----\n",
      "Batch 10/52, loss: 0.6695581793785095 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.6684518039226532 (0.010s), train acc: 0.572\n",
      "Batch 30/52, loss: 0.641278737783432 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6573973953723907 (0.010s), train acc: 0.596\n",
      "Batch 50/52, loss: 0.654102224111557 (0.010s), train acc: 0.596\n",
      "Batch 52/52, loss: 0.6135483980178833 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 43/500-----\n",
      "Batch 10/52, loss: 0.6400344967842102 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6562619745731354 (0.010s), train acc: 0.588\n",
      "Batch 30/52, loss: 0.6574851512908936 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6537512838840485 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6758355975151062 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6459011137485504 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 44/500-----\n",
      "Batch 10/52, loss: 0.6453153491020203 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6598714053630829 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6778652787208557 (0.010s), train acc: 0.594\n",
      "Batch 40/52, loss: 0.6698510408401489 (0.010s), train acc: 0.589\n",
      "Batch 50/52, loss: 0.6364026546478272 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6239342987537384 (0.003s), train acc: 0.601\n",
      "\n",
      "-----Epoch 45/500-----\n",
      "Batch 10/52, loss: 0.67008256316185 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.6497989416122436 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6702103734016418 (0.010s), train acc: 0.584\n",
      "Batch 40/52, loss: 0.652269721031189 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6493588507175445 (0.010s), train acc: 0.591\n",
      "Batch 52/52, loss: 0.5978440046310425 (0.003s), train acc: 0.596\n",
      "\n",
      "-----Epoch 46/500-----\n",
      "Batch 10/52, loss: 0.6408817946910859 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6493533551692963 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6641684651374817 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6697946429252625 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6642739295959472 (0.010s), train acc: 0.600\n",
      "Batch 52/52, loss: 0.609383761882782 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 47/500-----\n",
      "Batch 10/52, loss: 0.6405833244323731 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6392478942871094 (0.009s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6597780466079712 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6858938276767731 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6546759307384491 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6550159454345703 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 48/500-----\n",
      "Batch 10/52, loss: 0.6538320124149323 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6541005790233612 (0.009s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6733028411865234 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6256322801113129 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6746573925018311 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6389884352684021 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 49/500-----\n",
      "Batch 10/52, loss: 0.659966379404068 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6486985087394714 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6712953090667725 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6392693877220154 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6545463442802429 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6714322566986084 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 50/500-----\n",
      "Batch 10/52, loss: 0.6654966056346894 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6436404943466186 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.664736521244049 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6440199732780456 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6639043927192688 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6738192737102509 (0.003s), train acc: 0.601\n",
      "\n",
      "-----Epoch 51/500-----\n",
      "Batch 10/52, loss: 0.6525687873363495 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6562915086746216 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6552709758281707 (0.010s), train acc: 0.595\n",
      "Batch 40/52, loss: 0.6582890033721924 (0.010s), train acc: 0.591\n",
      "Batch 50/52, loss: 0.6534590184688568 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6671676933765411 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 52/500-----\n",
      "Batch 10/52, loss: 0.6533192753791809 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6571157693862915 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6793039083480835 (0.010s), train acc: 0.586\n",
      "Batch 40/52, loss: 0.6523207426071167 (0.010s), train acc: 0.595\n",
      "Batch 50/52, loss: 0.6392337024211884 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6228352785110474 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 53/500-----\n",
      "Batch 10/52, loss: 0.6648304402828217 (0.009s), train acc: 0.566\n",
      "Batch 20/52, loss: 0.6672493398189545 (0.010s), train acc: 0.588\n",
      "Batch 30/52, loss: 0.6600084960460663 (0.010s), train acc: 0.584\n",
      "Batch 40/52, loss: 0.6510494589805603 (0.010s), train acc: 0.593\n",
      "Batch 50/52, loss: 0.6323044717311859 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6658351719379425 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 54/500-----\n",
      "Batch 10/52, loss: 0.6486390948295593 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6467201471328735 (0.010s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6686328113079071 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6467343270778656 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6714653074741364 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6361618041992188 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 55/500-----\n",
      "Batch 10/52, loss: 0.6546449542045594 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6496235251426696 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6482878506183625 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6772364258766175 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6501575589179993 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6439712643623352 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 56/500-----\n",
      "Batch 10/52, loss: 0.641549414396286 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6457783043384552 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6649886250495911 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6653623580932617 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6471747636795044 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.7224907577037811 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 57/500-----\n",
      "Batch 10/52, loss: 0.6677708327770233 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6550024211406708 (0.010s), train acc: 0.573\n",
      "Batch 30/52, loss: 0.6523517727851867 (0.010s), train acc: 0.586\n",
      "Batch 40/52, loss: 0.6526962995529175 (0.010s), train acc: 0.595\n",
      "Batch 50/52, loss: 0.6481440484523773 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6547568738460541 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 58/500-----\n",
      "Batch 10/52, loss: 0.6261587798595428 (0.009s), train acc: 0.656\n",
      "Batch 20/52, loss: 0.6560989439487457 (0.010s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.6573663592338562 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6863668501377106 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6506793022155761 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.657875120639801 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 59/500-----\n",
      "Batch 10/52, loss: 0.6620930969715119 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6598245739936829 (0.009s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6429712474346161 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6653589606285095 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6552904129028321 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6158781945705414 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 60/500-----\n",
      "Batch 10/52, loss: 0.6818502485752106 (0.009s), train acc: 0.572\n",
      "Batch 20/52, loss: 0.62972851395607 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6632258653640747 (0.010s), train acc: 0.601\n",
      "Batch 40/52, loss: 0.6320381999015808 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6697371900081635 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6612766087055206 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 61/500-----\n",
      "Batch 10/52, loss: 0.629577887058258 (0.009s), train acc: 0.650\n",
      "Batch 20/52, loss: 0.6570846557617187 (0.009s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6513279259204865 (0.010s), train acc: 0.618\n",
      "Batch 40/52, loss: 0.6626287281513215 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6676305770874024 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.7017631232738495 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 62/500-----\n",
      "Batch 10/52, loss: 0.6521179139614105 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6826603233814239 (0.009s), train acc: 0.584\n",
      "Batch 30/52, loss: 0.6528903126716614 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.628925359249115 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6574282467365264 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6665028929710388 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 63/500-----\n",
      "Batch 10/52, loss: 0.6343049705028534 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6708381712436676 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6668634176254272 (0.010s), train acc: 0.601\n",
      "Batch 40/52, loss: 0.6516201376914978 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6585161685943604 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6527081429958344 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 64/500-----\n",
      "Batch 10/52, loss: 0.6488982260227203 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6534499526023865 (0.009s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6555272758007049 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.658713448047638 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6676177859306336 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6338976621627808 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 65/500-----\n",
      "Batch 10/52, loss: 0.6624120771884918 (0.009s), train acc: 0.556\n",
      "Batch 20/52, loss: 0.6446105360984802 (0.009s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6564489483833313 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6703894853591919 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6510011672973632 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6126672923564911 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 66/500-----\n",
      "Batch 10/52, loss: 0.6407151758670807 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6310878992080688 (0.010s), train acc: 0.639\n",
      "Batch 30/52, loss: 0.6720900893211365 (0.010s), train acc: 0.630\n",
      "Batch 40/52, loss: 0.6629809498786926 (0.010s), train acc: 0.620\n",
      "Batch 50/52, loss: 0.6675348222255707 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6628810167312622 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 67/500-----\n",
      "Batch 10/52, loss: 0.627377700805664 (0.009s), train acc: 0.659\n",
      "Batch 20/52, loss: 0.6447871804237366 (0.010s), train acc: 0.633\n",
      "Batch 30/52, loss: 0.6744314014911652 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.642216020822525 (0.010s), train acc: 0.620\n",
      "Batch 50/52, loss: 0.684576404094696 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.7119698822498322 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 68/500-----\n",
      "Batch 10/52, loss: 0.6515573501586914 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6500352382659912 (0.009s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6726135909557343 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6501315772533417 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6573654353618622 (0.010s), train acc: 0.600\n",
      "Batch 52/52, loss: 0.6442013680934906 (0.003s), train acc: 0.601\n",
      "\n",
      "-----Epoch 69/500-----\n",
      "Batch 10/52, loss: 0.6818330883979797 (0.009s), train acc: 0.547\n",
      "Batch 20/52, loss: 0.6736818313598633 (0.009s), train acc: 0.561\n",
      "Batch 30/52, loss: 0.6471797227859497 (0.010s), train acc: 0.579\n",
      "Batch 40/52, loss: 0.6262231647968293 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6438266217708588 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6727601885795593 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 70/500-----\n",
      "Batch 10/52, loss: 0.6627118766307831 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6441387236118317 (0.009s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.663457041978836 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6325678884983063 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6753269851207733 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6600437462329865 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 71/500-----\n",
      "Batch 10/52, loss: 0.6441528737545014 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6558940291404725 (0.010s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6470359861850739 (0.010s), train acc: 0.627\n",
      "Batch 40/52, loss: 0.6501896977424622 (0.010s), train acc: 0.622\n",
      "Batch 50/52, loss: 0.6759957432746887 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6882994174957275 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 72/500-----\n",
      "Batch 10/52, loss: 0.6721558272838593 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6438023746013641 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6660575985908508 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6546852767467499 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6342691123485565 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6741638779640198 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 73/500-----\n",
      "Batch 10/52, loss: 0.6584404051303864 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6535853028297425 (0.010s), train acc: 0.597\n",
      "Batch 30/52, loss: 0.6722770512104035 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6327941298484803 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6522017002105713 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6960157155990601 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 74/500-----\n",
      "Batch 10/52, loss: 0.6519861221313477 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6587308764457702 (0.009s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6477750360965728 (0.010s), train acc: 0.595\n",
      "Batch 40/52, loss: 0.672293508052826 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6426389455795288 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6647745370864868 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 75/500-----\n",
      "Batch 10/52, loss: 0.6446494877338409 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.640893566608429 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6591190755367279 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6634009838104248 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6652500927448273 (0.010s), train acc: 0.600\n",
      "Batch 52/52, loss: 0.6877651810646057 (0.003s), train acc: 0.598\n",
      "\n",
      "-----Epoch 76/500-----\n",
      "Batch 10/52, loss: 0.6554134368896485 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6715084314346313 (0.010s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6383975923061371 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6537968575954437 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6512247383594513 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6881413161754608 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 77/500-----\n",
      "Batch 10/52, loss: 0.6689655244350433 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6282039403915405 (0.009s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.657627409696579 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6594778001308441 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6709352672100067 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.605000227689743 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 78/500-----\n",
      "Batch 10/52, loss: 0.6814318597316742 (0.009s), train acc: 0.559\n",
      "Batch 20/52, loss: 0.6523786902427673 (0.009s), train acc: 0.584\n",
      "Batch 30/52, loss: 0.645637595653534 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6536391198635101 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6533622026443482 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6155627369880676 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 79/500-----\n",
      "Batch 10/52, loss: 0.6437649488449096 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6540234088897705 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6650600135326385 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6356117367744446 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.677312296628952 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6440671384334564 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 80/500-----\n",
      "Batch 10/52, loss: 0.6470484912395478 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6612835705280304 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6388570904731751 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6624048829078675 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6649568200111389 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6635774374008179 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 81/500-----\n",
      "Batch 10/52, loss: 0.6539515852928162 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6467152953147888 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6630055487155915 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6609955787658691 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6613415479660034 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6347679793834686 (0.003s), train acc: 0.601\n",
      "\n",
      "-----Epoch 82/500-----\n",
      "Batch 10/52, loss: 0.6432737946510315 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6707603156566619 (0.009s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6564739108085632 (0.010s), train acc: 0.601\n",
      "Batch 40/52, loss: 0.6765656352043152 (0.010s), train acc: 0.591\n",
      "Batch 50/52, loss: 0.6179552912712097 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.7002224922180176 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 83/500-----\n",
      "Batch 10/52, loss: 0.6502849459648132 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6573321521282196 (0.009s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6463403224945068 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6673615932464599 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6568628191947937 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6481349468231201 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 84/500-----\n",
      "Batch 10/52, loss: 0.6403012573719025 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6408630907535553 (0.009s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.6560700714588166 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6688405930995941 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6568236231803894 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.7236993610858917 (0.003s), train acc: 0.599\n",
      "\n",
      "-----Epoch 85/500-----\n",
      "Batch 10/52, loss: 0.6726465821266174 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.638891065120697 (0.009s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6317735612392426 (0.010s), train acc: 0.628\n",
      "Batch 40/52, loss: 0.6592131912708282 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6668203294277191 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6784339547157288 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 86/500-----\n",
      "Batch 10/52, loss: 0.6478905260562897 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6478622376918792 (0.009s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6747825264930725 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6538117289543152 (0.010s), train acc: 0.594\n",
      "Batch 50/52, loss: 0.6400059640407563 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.7136748731136322 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 87/500-----\n",
      "Batch 10/52, loss: 0.655101877450943 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6679045557975769 (0.010s), train acc: 0.578\n",
      "Batch 30/52, loss: 0.6753702759742737 (0.010s), train acc: 0.572\n",
      "Batch 40/52, loss: 0.6379882037639618 (0.010s), train acc: 0.587\n",
      "Batch 50/52, loss: 0.6388778150081634 (0.010s), train acc: 0.600\n",
      "Batch 52/52, loss: 0.662215381860733 (0.003s), train acc: 0.599\n",
      "\n",
      "-----Epoch 88/500-----\n",
      "Batch 10/52, loss: 0.6558289587497711 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6572709441184997 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6532058119773865 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6632694184780121 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6476128101348877 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6383046805858612 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 89/500-----\n",
      "Batch 10/52, loss: 0.6602614343166351 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6589563071727753 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.652255517244339 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6682891845703125 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6574149787425995 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.5859103202819824 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 90/500-----\n",
      "Batch 10/52, loss: 0.6415944159030914 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.6485714793205262 (0.009s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6719117820262909 (0.010s), train acc: 0.594\n",
      "Batch 40/52, loss: 0.6656881332397461 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6538148820400238 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6539835929870605 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 91/500-----\n",
      "Batch 10/52, loss: 0.6619119584560395 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6675995171070099 (0.009s), train acc: 0.591\n",
      "Batch 30/52, loss: 0.6805362641811371 (0.010s), train acc: 0.582\n",
      "Batch 40/52, loss: 0.6308594167232513 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6400759875774383 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6296814680099487 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 92/500-----\n",
      "Batch 10/52, loss: 0.6699884057044982 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.639526778459549 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6364588260650634 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6653977155685424 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6620055913925171 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6708276271820068 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 93/500-----\n",
      "Batch 10/52, loss: 0.6535737574100494 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6519360661506652 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6445141851902008 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.652177232503891 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6816093325614929 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6136810481548309 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 94/500-----\n",
      "Batch 10/52, loss: 0.6565673053264618 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.648222702741623 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6458755195140838 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6521513879299163 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.6644898951053619 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6855216026306152 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 95/500-----\n",
      "Batch 10/52, loss: 0.6695065498352051 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6518489241600036 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.650270402431488 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6579423427581788 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6473783731460572 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6364046335220337 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 96/500-----\n",
      "Batch 10/52, loss: 0.6678683936595917 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6512511968612671 (0.010s), train acc: 0.589\n",
      "Batch 30/52, loss: 0.6737283647060395 (0.010s), train acc: 0.590\n",
      "Batch 40/52, loss: 0.6534006118774414 (0.010s), train acc: 0.595\n",
      "Batch 50/52, loss: 0.6279471278190613 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6838256418704987 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 97/500-----\n",
      "Batch 10/52, loss: 0.6525961101055145 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6538185477256775 (0.009s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6597909152507782 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6553232908248902 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.6460585176944733 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6841995120048523 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 98/500-----\n",
      "Batch 10/52, loss: 0.647563374042511 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6426115334033966 (0.010s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6476741194725036 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6571306526660919 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6757386088371277 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6675278842449188 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 99/500-----\n",
      "Batch 10/52, loss: 0.6492505669593811 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6483058750629425 (0.009s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6512031972408294 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6679242610931396 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6512637197971344 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6790916323661804 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 100/500-----\n",
      "Batch 10/52, loss: 0.6513914585113525 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6530102312564849 (0.009s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6529284238815307 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.655439418554306 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.659441739320755 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6573781073093414 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 101/500-----\n",
      "Batch 10/52, loss: 0.6466449677944184 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6358590304851532 (0.009s), train acc: 0.636\n",
      "Batch 30/52, loss: 0.6751890480518341 (0.010s), train acc: 0.620\n",
      "Batch 40/52, loss: 0.6770725071430206 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6580929577350616 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.5888315737247467 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 102/500-----\n",
      "Batch 10/52, loss: 0.6507043063640594 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6483274817466735 (0.009s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6506506741046906 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6690139055252076 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6469477832317352 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.7144447863101959 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 103/500-----\n",
      "Batch 10/52, loss: 0.6416965663433075 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6541440665721894 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6526967942714691 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6429832994937896 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6768059015274048 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6755160689353943 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 104/500-----\n",
      "Batch 10/52, loss: 0.6734719216823578 (0.009s), train acc: 0.572\n",
      "Batch 20/52, loss: 0.6498227179050445 (0.009s), train acc: 0.586\n",
      "Batch 30/52, loss: 0.6600829124450683 (0.010s), train acc: 0.585\n",
      "Batch 40/52, loss: 0.6649741232395172 (0.010s), train acc: 0.588\n",
      "Batch 50/52, loss: 0.6330384314060211 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6225613653659821 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 105/500-----\n",
      "Batch 10/52, loss: 0.6516669273376465 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.66067875623703 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.643660968542099 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6651349782943725 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.662783944606781 (0.010s), train acc: 0.600\n",
      "Batch 52/52, loss: 0.5923489928245544 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 106/500-----\n",
      "Batch 10/52, loss: 0.6255660712718963 (0.009s), train acc: 0.650\n",
      "Batch 20/52, loss: 0.6498136520385742 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.675281685590744 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6487273156642914 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6663453221321106 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.698826014995575 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 107/500-----\n",
      "Batch 10/52, loss: 0.6577433228492737 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6547887325286865 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6744111835956573 (0.010s), train acc: 0.592\n",
      "Batch 40/52, loss: 0.6397443354129791 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.650972557067871 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6247056126594543 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 108/500-----\n",
      "Batch 10/52, loss: 0.6647381663322449 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6583563089370728 (0.010s), train acc: 0.583\n",
      "Batch 30/52, loss: 0.6494007289409638 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.665537828207016 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6511253237724304 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.5938234031200409 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 109/500-----\n",
      "Batch 10/52, loss: 0.6259295105934143 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.6704111754894256 (0.009s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6468892216682434 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6713872611522674 (0.010s), train acc: 0.595\n",
      "Batch 50/52, loss: 0.6657142221927643 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6366913318634033 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 110/500-----\n",
      "Batch 10/52, loss: 0.6484344363212585 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.638870096206665 (0.010s), train acc: 0.641\n",
      "Batch 30/52, loss: 0.6476297318935395 (0.010s), train acc: 0.625\n",
      "Batch 40/52, loss: 0.6631998836994171 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6756186783313751 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6564519107341766 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 111/500-----\n",
      "Batch 10/52, loss: 0.6661763608455658 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6413481831550598 (0.009s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6670463740825653 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6430002391338349 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6639688432216644 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6325529217720032 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 112/500-----\n",
      "Batch 10/52, loss: 0.6732788026332855 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6531561017036438 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6468565940856934 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6673008561134338 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6330576956272125 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6514535248279572 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 113/500-----\n",
      "Batch 10/52, loss: 0.6297172009944916 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6383455872535706 (0.009s), train acc: 0.637\n",
      "Batch 30/52, loss: 0.6503071784973145 (0.010s), train acc: 0.639\n",
      "Batch 40/52, loss: 0.6819121718406678 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6805404543876648 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6700483560562134 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 114/500-----\n",
      "Batch 10/52, loss: 0.6392674207687378 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6637395322322845 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6353483498096466 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6540275335311889 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6796569883823395 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6508365869522095 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 115/500-----\n",
      "Batch 10/52, loss: 0.6746196627616883 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6375609755516052 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6614165663719177 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6547640860080719 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6464338302612305 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6413947343826294 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 116/500-----\n",
      "Batch 10/52, loss: 0.6518898367881775 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6532209694385529 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6660320401191712 (0.010s), train acc: 0.591\n",
      "Batch 40/52, loss: 0.6474643588066101 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6518883228302002 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6520960628986359 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 117/500-----\n",
      "Batch 10/52, loss: 0.6395571291446686 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6310315251350402 (0.010s), train acc: 0.636\n",
      "Batch 30/52, loss: 0.669844651222229 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6735727965831757 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6579243898391723 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6608589887619019 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 118/500-----\n",
      "Batch 10/52, loss: 0.6588035225868225 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6428848147392273 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.658510047197342 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6552026271820068 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6605447351932525 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6308769285678864 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 119/500-----\n",
      "Batch 10/52, loss: 0.6856901526451111 (0.009s), train acc: 0.550\n",
      "Batch 20/52, loss: 0.6494520425796508 (0.009s), train acc: 0.575\n",
      "Batch 30/52, loss: 0.6406391680240631 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6428201496601105 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6599671125411988 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6311721801757812 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 120/500-----\n",
      "Batch 10/52, loss: 0.6589920043945312 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6599679529666901 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6504392266273499 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6631262004375458 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.646063232421875 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6521826982498169 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 121/500-----\n",
      "Batch 10/52, loss: 0.6639408826828003 (0.009s), train acc: 0.569\n",
      "Batch 20/52, loss: 0.6630815148353577 (0.010s), train acc: 0.586\n",
      "Batch 30/52, loss: 0.6466387450695038 (0.009s), train acc: 0.590\n",
      "Batch 40/52, loss: 0.6400317668914794 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6548670113086701 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6608701944351196 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 122/500-----\n",
      "Batch 10/52, loss: 0.6661225914955139 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6479228854179382 (0.010s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6457520425319672 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6544555127620697 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6647163987159729 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6175394058227539 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 123/500-----\n",
      "Batch 10/52, loss: 0.6585002958774566 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6309891104698181 (0.009s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6512415468692779 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6665033102035522 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.6441204249858856 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.7327179610729218 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 124/500-----\n",
      "Batch 10/52, loss: 0.6649011969566345 (0.009s), train acc: 0.569\n",
      "Batch 20/52, loss: 0.6359580516815185 (0.009s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6431778490543365 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6708162188529968 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6551643073558807 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6590524911880493 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 125/500-----\n",
      "Batch 10/52, loss: 0.6591047406196594 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6413690924644471 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6774129271507263 (0.009s), train acc: 0.589\n",
      "Batch 40/52, loss: 0.6292126297950744 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6578968465328217 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6880139708518982 (0.003s), train acc: 0.599\n",
      "\n",
      "-----Epoch 126/500-----\n",
      "Batch 10/52, loss: 0.6610988914966583 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6540671944618225 (0.009s), train acc: 0.597\n",
      "Batch 30/52, loss: 0.6685101389884949 (0.010s), train acc: 0.588\n",
      "Batch 40/52, loss: 0.6514428317546844 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6517242193222046 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.5808789730072021 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 127/500-----\n",
      "Batch 10/52, loss: 0.6436977922916413 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.653171980381012 (0.009s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6666898846626281 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6626102745532989 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6415386378765107 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.661352664232254 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 128/500-----\n",
      "Batch 10/52, loss: 0.6811023890972138 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.650292557477951 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6553845465183258 (0.010s), train acc: 0.591\n",
      "Batch 40/52, loss: 0.6535530149936676 (0.010s), train acc: 0.592\n",
      "Batch 50/52, loss: 0.6365734577178955 (0.010s), train acc: 0.600\n",
      "Batch 52/52, loss: 0.6183380782604218 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 129/500-----\n",
      "Batch 10/52, loss: 0.6248090207576752 (0.009s), train acc: 0.656\n",
      "Batch 20/52, loss: 0.6439808666706085 (0.010s), train acc: 0.634\n",
      "Batch 30/52, loss: 0.6561341464519501 (0.010s), train acc: 0.618\n",
      "Batch 40/52, loss: 0.6790915548801422 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6553008854389191 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.7029357850551605 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 130/500-----\n",
      "Batch 10/52, loss: 0.6599924862384796 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.6503149747848511 (0.010s), train acc: 0.597\n",
      "Batch 30/52, loss: 0.6446808874607086 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.656330680847168 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6598378658294678 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6471099853515625 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 131/500-----\n",
      "Batch 10/52, loss: 0.6698868870735168 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6608875751495361 (0.010s), train acc: 0.583\n",
      "Batch 30/52, loss: 0.639227420091629 (0.010s), train acc: 0.591\n",
      "Batch 40/52, loss: 0.6601856112480163 (0.010s), train acc: 0.596\n",
      "Batch 50/52, loss: 0.6514489650726318 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6147074401378632 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 132/500-----\n",
      "Batch 10/52, loss: 0.6683271050453186 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6375756204128266 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6503796398639679 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6586575150489807 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6582146644592285 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.64585742354393 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 133/500-----\n",
      "Batch 10/52, loss: 0.6482953011989594 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6406674563884736 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6590381860733032 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6374309957027435 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6749477863311768 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6966987252235413 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 134/500-----\n",
      "Batch 10/52, loss: 0.6083147585391998 (0.009s), train acc: 0.697\n",
      "Batch 20/52, loss: 0.6800430178642273 (0.010s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6403391480445861 (0.010s), train acc: 0.628\n",
      "Batch 40/52, loss: 0.6596947848796845 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6805886030197144 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.647503525018692 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 135/500-----\n",
      "Batch 10/52, loss: 0.6738562762737275 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6442200362682342 (0.010s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6740404963493347 (0.010s), train acc: 0.579\n",
      "Batch 40/52, loss: 0.6496762633323669 (0.010s), train acc: 0.588\n",
      "Batch 50/52, loss: 0.6338119566440582 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6561691761016846 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 136/500-----\n",
      "Batch 10/52, loss: 0.6370062887668609 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6638389825820923 (0.010s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6614323437213898 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6734130144119262 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6508867025375367 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.5929324626922607 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 137/500-----\n",
      "Batch 10/52, loss: 0.6538327276706696 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.670424097776413 (0.009s), train acc: 0.589\n",
      "Batch 30/52, loss: 0.6485955655574799 (0.009s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6435067415237427 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.655071097612381 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6379544138908386 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 138/500-----\n",
      "Batch 10/52, loss: 0.6459900915622712 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6439709484577179 (0.009s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6395477831363678 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6732405960559845 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6577626943588257 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6989721059799194 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 139/500-----\n",
      "Batch 10/52, loss: 0.65064617395401 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6547476053237915 (0.010s), train acc: 0.591\n",
      "Batch 30/52, loss: 0.6234243273735046 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6446500778198242 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6935382783412933 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.667883574962616 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 140/500-----\n",
      "Batch 10/52, loss: 0.6567991137504577 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6505501091480255 (0.010s), train acc: 0.589\n",
      "Batch 30/52, loss: 0.6648502230644227 (0.010s), train acc: 0.593\n",
      "Batch 40/52, loss: 0.6382381916046143 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.657068932056427 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6710733771324158 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 141/500-----\n",
      "Batch 10/52, loss: 0.6310864925384522 (0.009s), train acc: 0.647\n",
      "Batch 20/52, loss: 0.6412095963954926 (0.010s), train acc: 0.636\n",
      "Batch 30/52, loss: 0.6601480662822723 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6643788993358613 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.674854701757431 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6442992687225342 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 142/500-----\n",
      "Batch 10/52, loss: 0.6653194963932038 (0.009s), train acc: 0.559\n",
      "Batch 20/52, loss: 0.6655357003211975 (0.010s), train acc: 0.578\n",
      "Batch 30/52, loss: 0.627685934305191 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6536805033683777 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6564971208572388 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6398172378540039 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 143/500-----\n",
      "Batch 10/52, loss: 0.6447296440601349 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6184100806713104 (0.010s), train acc: 0.655\n",
      "Batch 30/52, loss: 0.6855046272277832 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6528260290622712 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6678296387195587 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6531577110290527 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 144/500-----\n",
      "Batch 10/52, loss: 0.6544741630554199 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6508688032627106 (0.009s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6447626233100892 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6542516231536866 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6661467969417572 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6549700200557709 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 145/500-----\n",
      "Batch 10/52, loss: 0.6510336518287658 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6438761472702026 (0.009s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6863732874393463 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6310611724853515 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6630249500274659 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6125174462795258 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 146/500-----\n",
      "Batch 10/52, loss: 0.6473534584045411 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6636942982673645 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6616687476634979 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6569297432899475 (0.010s), train acc: 0.593\n",
      "Batch 50/52, loss: 0.6333686649799347 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6800833344459534 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 147/500-----\n",
      "Batch 10/52, loss: 0.6317493379116058 (0.009s), train acc: 0.647\n",
      "Batch 20/52, loss: 0.6538548648357392 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6243659794330597 (0.010s), train acc: 0.633\n",
      "Batch 40/52, loss: 0.6829754412174225 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6647178113460541 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.7019959688186646 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 148/500-----\n",
      "Batch 10/52, loss: 0.6654495358467102 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6333980739116669 (0.009s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6532202184200286 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6422075629234314 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6712988197803498 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6830955147743225 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 149/500-----\n",
      "Batch 10/52, loss: 0.652778822183609 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6617196559906006 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6589301586151123 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6476841032505035 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6430409610271454 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6728513240814209 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 150/500-----\n",
      "Batch 10/52, loss: 0.6620600700378418 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6414587020874023 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.664570426940918 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6445016980171203 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6595498025417328 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6269140243530273 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 151/500-----\n",
      "Batch 10/52, loss: 0.6711508512496949 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6478768408298492 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6489906013011932 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.652421373128891 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6529037833213807 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.647242546081543 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 152/500-----\n",
      "Batch 10/52, loss: 0.6456425547599792 (0.009s), train acc: 0.641\n",
      "Batch 20/52, loss: 0.641380923986435 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6390259861946106 (0.010s), train acc: 0.627\n",
      "Batch 40/52, loss: 0.6778348684310913 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6746312558650971 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6303286254405975 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 153/500-----\n",
      "Batch 10/52, loss: 0.6566224932670593 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6641822397708893 (0.010s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6489440083503724 (0.010s), train acc: 0.592\n",
      "Batch 40/52, loss: 0.6531933844089508 (0.010s), train acc: 0.592\n",
      "Batch 50/52, loss: 0.6495132446289062 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6490889191627502 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 154/500-----\n",
      "Batch 10/52, loss: 0.6592205226421356 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.6561252474784851 (0.010s), train acc: 0.588\n",
      "Batch 30/52, loss: 0.6522127389907837 (0.010s), train acc: 0.595\n",
      "Batch 40/52, loss: 0.6459492981433869 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6538520693778992 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6528406739234924 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 155/500-----\n",
      "Batch 10/52, loss: 0.6502826273441314 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6467977166175842 (0.009s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6757736146450043 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6619533777236939 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6326299428939819 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6514977216720581 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 156/500-----\n",
      "Batch 10/52, loss: 0.6905465185642242 (0.009s), train acc: 0.556\n",
      "Batch 20/52, loss: 0.6525570094585419 (0.009s), train acc: 0.580\n",
      "Batch 30/52, loss: 0.6498796164989471 (0.010s), train acc: 0.593\n",
      "Batch 40/52, loss: 0.6493484258651734 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6363975763320923 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.610470175743103 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 157/500-----\n",
      "Batch 10/52, loss: 0.6373765766620636 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6482516169548035 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6444900631904602 (0.010s), train acc: 0.628\n",
      "Batch 40/52, loss: 0.6625267386436462 (0.010s), train acc: 0.621\n",
      "Batch 50/52, loss: 0.6750200927257538 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.668268471956253 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 158/500-----\n",
      "Batch 10/52, loss: 0.6311130225658417 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6650213003158569 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6644643187522888 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6432951807975769 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6603652834892273 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6572434306144714 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 159/500-----\n",
      "Batch 10/52, loss: 0.6348691880702972 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.675661039352417 (0.009s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6594677448272706 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6581394076347351 (0.010s), train acc: 0.593\n",
      "Batch 50/52, loss: 0.6425242364406586 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.648176908493042 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 160/500-----\n",
      "Batch 10/52, loss: 0.6538318693637848 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6413264870643616 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6619634985923767 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6619047105312348 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6486938059329986 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6479752957820892 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 161/500-----\n",
      "Batch 10/52, loss: 0.665762186050415 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.6517139256000519 (0.010s), train acc: 0.580\n",
      "Batch 30/52, loss: 0.676713740825653 (0.010s), train acc: 0.580\n",
      "Batch 40/52, loss: 0.6338200032711029 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6403762221336364 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6305970251560211 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 162/500-----\n",
      "Batch 10/52, loss: 0.6732544124126434 (0.009s), train acc: 0.566\n",
      "Batch 20/52, loss: 0.6388408899307251 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.64363734126091 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6436676502227783 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6571398437023163 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6900405883789062 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 163/500-----\n",
      "Batch 10/52, loss: 0.6569084644317627 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6962745606899261 (0.009s), train acc: 0.564\n",
      "Batch 30/52, loss: 0.6353817284107208 (0.010s), train acc: 0.589\n",
      "Batch 40/52, loss: 0.6337777495384216 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6487292289733887 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6459913849830627 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 164/500-----\n",
      "Batch 10/52, loss: 0.6527152061462402 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6580855906009674 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.64125736951828 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6389731943607331 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6725622296333313 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6689528524875641 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 165/500-----\n",
      "Batch 10/52, loss: 0.663081020116806 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6547644853591919 (0.009s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6702075660228729 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6629610359668732 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.633544534444809 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6081888675689697 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 166/500-----\n",
      "Batch 10/52, loss: 0.6466615974903107 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6686693787574768 (0.009s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6619031369686127 (0.009s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6503178358078003 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6446345090866089 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6445843875408173 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 167/500-----\n",
      "Batch 10/52, loss: 0.6396612524986267 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6660938143730164 (0.009s), train acc: 0.597\n",
      "Batch 30/52, loss: 0.6482337296009064 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6635177969932556 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6534938454627991 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6203182935714722 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 168/500-----\n",
      "Batch 10/52, loss: 0.6358409464359284 (0.009s), train acc: 0.650\n",
      "Batch 20/52, loss: 0.6484274983406066 (0.010s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.6693867802619934 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6475847244262696 (0.010s), train acc: 0.621\n",
      "Batch 50/52, loss: 0.6518015921115875 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.7449667155742645 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 169/500-----\n",
      "Batch 10/52, loss: 0.6503249287605286 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6521285831928253 (0.010s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6760213911533356 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6473756194114685 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6461682915687561 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6298673152923584 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 170/500-----\n",
      "Batch 10/52, loss: 0.6574983954429626 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6521293163299561 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6493636906147003 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6500114560127258 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6566438496112823 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6864886283874512 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 171/500-----\n",
      "Batch 10/52, loss: 0.6627068102359772 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.6506116271018982 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6625840842723847 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6423733949661254 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6475459396839142 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6591924130916595 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 172/500-----\n",
      "Batch 10/52, loss: 0.6597049772739411 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6543894112110138 (0.010s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6466910481452942 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6428199112415314 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6620878040790558 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6500348150730133 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 173/500-----\n",
      "Batch 10/52, loss: 0.6542435586452484 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6527030646800995 (0.009s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6445290207862854 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6493033707141876 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6485628008842468 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.7252606451511383 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 174/500-----\n",
      "Batch 10/52, loss: 0.6705611169338226 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.64927978515625 (0.009s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6367783129215241 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6536666035652161 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6660429835319519 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6029759645462036 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 175/500-----\n",
      "Batch 10/52, loss: 0.6609750628471375 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6472935974597931 (0.009s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.652666699886322 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6515304148197174 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6574243485927582 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6435661315917969 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 176/500-----\n",
      "Batch 10/52, loss: 0.6399870753288269 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6613493740558625 (0.009s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6643607258796692 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6481690108776093 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.6498864889144897 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6661507785320282 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 177/500-----\n",
      "Batch 10/52, loss: 0.6563767552375793 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6650246739387512 (0.009s), train acc: 0.591\n",
      "Batch 30/52, loss: 0.6647843778133392 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6473661601543427 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6403750479221344 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6203871071338654 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 178/500-----\n",
      "Batch 10/52, loss: 0.6455063581466675 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6560382664203643 (0.009s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6533028423786164 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6831419050693512 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6230282247066498 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6759619414806366 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 179/500-----\n",
      "Batch 10/52, loss: 0.6516102731227875 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6781192421913147 (0.009s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6543093383312225 (0.010s), train acc: 0.595\n",
      "Batch 40/52, loss: 0.6423138320446015 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6468566536903382 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6294224858283997 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 180/500-----\n",
      "Batch 10/52, loss: 0.6425052523612976 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.6497899830341339 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6629277050495148 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6474549353122712 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6562978029251099 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6731395423412323 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 181/500-----\n",
      "Batch 10/52, loss: 0.6517585098743439 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6489723503589631 (0.009s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6549377799034118 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.6756990134716034 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6451324820518494 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6029516756534576 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 182/500-----\n",
      "Batch 10/52, loss: 0.6455404579639434 (0.009s), train acc: 0.647\n",
      "Batch 20/52, loss: 0.6449128568172455 (0.009s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6530776381492615 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6674896001815795 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6558399617671966 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6495514512062073 (0.003s), train acc: 0.601\n",
      "\n",
      "-----Epoch 183/500-----\n",
      "Batch 10/52, loss: 0.6447088420391083 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.646653562784195 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6450011551380157 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6611778795719147 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6714928984642029 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6267248392105103 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 184/500-----\n",
      "Batch 10/52, loss: 0.6752491950988769 (0.009s), train acc: 0.556\n",
      "Batch 20/52, loss: 0.6455731451511383 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6323961675167084 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6558188557624817 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6588275492191314 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6694396436214447 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 185/500-----\n",
      "Batch 10/52, loss: 0.6489076673984527 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.657116824388504 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.646423727273941 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6522121548652648 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6568365275859833 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6731550693511963 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 186/500-----\n",
      "Batch 10/52, loss: 0.6437209010124206 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6439055263996124 (0.009s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.685453349351883 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6311736404895782 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6478019714355469 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.7229770720005035 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 187/500-----\n",
      "Batch 10/52, loss: 0.6247879147529602 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6311317324638367 (0.010s), train acc: 0.636\n",
      "Batch 30/52, loss: 0.6660221636295318 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6692246556282043 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6840519905090332 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6213224232196808 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 188/500-----\n",
      "Batch 10/52, loss: 0.6428215622901916 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6426436960697174 (0.010s), train acc: 0.639\n",
      "Batch 30/52, loss: 0.657356059551239 (0.009s), train acc: 0.625\n",
      "Batch 40/52, loss: 0.6726169407367706 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6476376593112946 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6631859838962555 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 189/500-----\n",
      "Batch 10/52, loss: 0.640953779220581 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6595940470695496 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6427068650722504 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6537446975708008 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.665430998802185 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6814467906951904 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 190/500-----\n",
      "Batch 10/52, loss: 0.6346616923809052 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6338426887989044 (0.009s), train acc: 0.641\n",
      "Batch 30/52, loss: 0.6503593921661377 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6773973941802979 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.6699130117893219 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6844654381275177 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 191/500-----\n",
      "Batch 10/52, loss: 0.6485844135284424 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6566999971866607 (0.009s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.651434850692749 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6515533208847046 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6665412485599518 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6239016056060791 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 192/500-----\n",
      "Batch 10/52, loss: 0.6572936832904815 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6567731440067291 (0.009s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6656085252761841 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6512385249137879 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6283218681812286 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6785916388034821 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 193/500-----\n",
      "Batch 10/52, loss: 0.6668004989624023 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6544637739658355 (0.010s), train acc: 0.586\n",
      "Batch 30/52, loss: 0.6390346407890319 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6596744954586029 (0.010s), train acc: 0.592\n",
      "Batch 50/52, loss: 0.642042487859726 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6794137954711914 (0.003s), train acc: 0.601\n",
      "\n",
      "-----Epoch 194/500-----\n",
      "Batch 10/52, loss: 0.691706919670105 (0.009s), train acc: 0.562\n",
      "Batch 20/52, loss: 0.6367317378520966 (0.010s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6336813807487488 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6526917278766632 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6453732788562775 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6708718240261078 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 195/500-----\n",
      "Batch 10/52, loss: 0.6357063055038452 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.652726149559021 (0.009s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6342024147510529 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.6618175208568573 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6658994019031524 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.7181961834430695 (0.003s), train acc: 0.599\n",
      "\n",
      "-----Epoch 196/500-----\n",
      "Batch 10/52, loss: 0.6489336669445038 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6346282958984375 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6610565423965454 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6651375710964202 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6534269630908967 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6481507420539856 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 197/500-----\n",
      "Batch 10/52, loss: 0.6341899514198304 (0.009s), train acc: 0.650\n",
      "Batch 20/52, loss: 0.6444970369338989 (0.010s), train acc: 0.639\n",
      "Batch 30/52, loss: 0.6753198862075805 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6470231473445892 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6594250917434692 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6768978238105774 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 198/500-----\n",
      "Batch 10/52, loss: 0.6436721563339234 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6691615045070648 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6412256896495819 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6316747963428497 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6797416985034943 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6398658156394958 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 199/500-----\n",
      "Batch 10/52, loss: 0.6356803238391876 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6707459986209869 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6548911511898041 (0.010s), train acc: 0.594\n",
      "Batch 40/52, loss: 0.6454817116260528 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6486372768878936 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6852307319641113 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 200/500-----\n",
      "Batch 10/52, loss: 0.6618426382541657 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6497886002063751 (0.009s), train acc: 0.583\n",
      "Batch 30/52, loss: 0.6583234369754791 (0.010s), train acc: 0.595\n",
      "Batch 40/52, loss: 0.662520456314087 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6361084163188935 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6239373087882996 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 201/500-----\n",
      "Batch 10/52, loss: 0.6545101404190063 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6366577208042145 (0.009s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6568873703479767 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6469160258769989 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.673202246427536 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6435306370258331 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 202/500-----\n",
      "Batch 10/52, loss: 0.6378393530845642 (0.009s), train acc: 0.647\n",
      "Batch 20/52, loss: 0.6597140729427338 (0.009s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6596240222454071 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6555858612060547 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6558917760848999 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6341715753078461 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 203/500-----\n",
      "Batch 10/52, loss: 0.65715172290802 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6423124670982361 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6412504076957702 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6787006556987762 (0.010s), train acc: 0.595\n",
      "Batch 50/52, loss: 0.6494444191455842 (0.010s), train acc: 0.596\n",
      "Batch 52/52, loss: 0.6356344819068909 (0.003s), train acc: 0.598\n",
      "\n",
      "-----Epoch 204/500-----\n",
      "Batch 10/52, loss: 0.6677029550075531 (0.009s), train acc: 0.562\n",
      "Batch 20/52, loss: 0.6411246061325073 (0.009s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6514585673809051 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.660619592666626 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6463098108768464 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.638709545135498 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 205/500-----\n",
      "Batch 10/52, loss: 0.6541234374046325 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6540423333644867 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6656038880348205 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.651047021150589 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6366330862045289 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6528940796852112 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 206/500-----\n",
      "Batch 10/52, loss: 0.6393031299114227 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6551305234432221 (0.009s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6510799288749695 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6594737827777862 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6631758749485016 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6516786515712738 (0.003s), train acc: 0.601\n",
      "\n",
      "-----Epoch 207/500-----\n",
      "Batch 10/52, loss: 0.672920286655426 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6441477119922638 (0.009s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6472421586513519 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6278393924236297 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6709032893180847 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.661276638507843 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 208/500-----\n",
      "Batch 10/52, loss: 0.6576377570629119 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6596160888671875 (0.009s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6427595496177674 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6632621824741364 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6460852861404419 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6195048689842224 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 209/500-----\n",
      "Batch 10/52, loss: 0.6384596228599548 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6479218125343322 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6648562669754028 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.649966710805893 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6664885520935059 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6204308867454529 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 210/500-----\n",
      "Batch 10/52, loss: 0.6504913330078125 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6599694132804871 (0.009s), train acc: 0.588\n",
      "Batch 30/52, loss: 0.6523712575435638 (0.010s), train acc: 0.588\n",
      "Batch 40/52, loss: 0.6410456895828247 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6509837567806244 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.7006434500217438 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 211/500-----\n",
      "Batch 10/52, loss: 0.6722707390785218 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6269640028476715 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6635765433311462 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6612422525882721 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6535342156887054 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6020643711090088 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 212/500-----\n",
      "Batch 10/52, loss: 0.6655661106109619 (0.009s), train acc: 0.569\n",
      "Batch 20/52, loss: 0.6599312365055084 (0.010s), train acc: 0.584\n",
      "Batch 30/52, loss: 0.6426676750183106 (0.010s), train acc: 0.592\n",
      "Batch 40/52, loss: 0.6475738525390625 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6419159591197967 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.690782904624939 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 213/500-----\n",
      "Batch 10/52, loss: 0.6769621908664704 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6472582876682281 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6259924948215485 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6599949121475219 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6605582952499389 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6268423199653625 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 214/500-----\n",
      "Batch 10/52, loss: 0.6649785041809082 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.6563567042350769 (0.009s), train acc: 0.584\n",
      "Batch 30/52, loss: 0.640116548538208 (0.010s), train acc: 0.601\n",
      "Batch 40/52, loss: 0.639034366607666 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.6550640106201172 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6794874370098114 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 215/500-----\n",
      "Batch 10/52, loss: 0.6476170361042023 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6288037717342376 (0.010s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6902776479721069 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6696746647357941 (0.010s), train acc: 0.591\n",
      "Batch 50/52, loss: 0.6272644162178039 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6483246088027954 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 216/500-----\n",
      "Batch 10/52, loss: 0.6414553582668304 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6463282525539398 (0.010s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6586605072021484 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6724850237369537 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6493245005607605 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6356296539306641 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 217/500-----\n",
      "Batch 10/52, loss: 0.663460123538971 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6769384920597077 (0.009s), train acc: 0.580\n",
      "Batch 30/52, loss: 0.6435397684574127 (0.010s), train acc: 0.595\n",
      "Batch 40/52, loss: 0.6203513979911804 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6522743344306946 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.679239422082901 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 218/500-----\n",
      "Batch 10/52, loss: 0.6489754498004914 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6705814599990845 (0.010s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6146378576755523 (0.010s), train acc: 0.629\n",
      "Batch 40/52, loss: 0.6742657482624054 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6615559220314026 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.646021693944931 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 219/500-----\n",
      "Batch 10/52, loss: 0.6568195760250092 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.656937050819397 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6720134973526001 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6482010662555695 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6337589025497437 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6486741900444031 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 220/500-----\n",
      "Batch 10/52, loss: 0.6572328090667725 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6628195524215699 (0.009s), train acc: 0.583\n",
      "Batch 30/52, loss: 0.6357853710651398 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.652815705537796 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6519001662731171 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6489415466785431 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 221/500-----\n",
      "Batch 10/52, loss: 0.6475883960723877 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6689206480979919 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.648554140329361 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6634323358535766 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6387710571289062 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6205410659313202 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 222/500-----\n",
      "Batch 10/52, loss: 0.6597969114780426 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6538048624992371 (0.009s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6468727052211761 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6591249346733093 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.638117104768753 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6721798479557037 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 223/500-----\n",
      "Batch 10/52, loss: 0.6532340109348297 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6385757088661194 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6424448490142822 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6845991492271424 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6463927030563354 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6624368131160736 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 224/500-----\n",
      "Batch 10/52, loss: 0.6301955938339233 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6626671433448792 (0.009s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6421497762203217 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.684128749370575 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6500071465969086 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6379537880420685 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 225/500-----\n",
      "Batch 10/52, loss: 0.6264449834823609 (0.009s), train acc: 0.662\n",
      "Batch 20/52, loss: 0.6633289217948913 (0.009s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6812590718269348 (0.009s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6516604363918305 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.6381531238555909 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6568614840507507 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 226/500-----\n",
      "Batch 10/52, loss: 0.6515446066856384 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6553447723388672 (0.010s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.6676782906055451 (0.010s), train acc: 0.582\n",
      "Batch 40/52, loss: 0.6525660693645478 (0.010s), train acc: 0.595\n",
      "Batch 50/52, loss: 0.6227204203605652 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.7124058306217194 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 227/500-----\n",
      "Batch 10/52, loss: 0.6618210434913635 (0.009s), train acc: 0.562\n",
      "Batch 20/52, loss: 0.6400247991085053 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6703717470169067 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6657575964927673 (0.010s), train acc: 0.596\n",
      "Batch 50/52, loss: 0.6366490960121155 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6275087296962738 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 228/500-----\n",
      "Batch 10/52, loss: 0.6662832796573639 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.6670870125293732 (0.010s), train acc: 0.584\n",
      "Batch 30/52, loss: 0.6304330825805664 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6635414659976959 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6279360115528106 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6869194507598877 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 229/500-----\n",
      "Batch 10/52, loss: 0.6343082785606384 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6341881632804871 (0.010s), train acc: 0.637\n",
      "Batch 30/52, loss: 0.6795960605144501 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.6568696737289429 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6408310651779174 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.7116566002368927 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 230/500-----\n",
      "Batch 10/52, loss: 0.6332890212535858 (0.009s), train acc: 0.656\n",
      "Batch 20/52, loss: 0.6815597772598266 (0.009s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.6630798518657685 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6477652370929718 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6520164012908936 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6117490530014038 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 231/500-----\n",
      "Batch 10/52, loss: 0.6230299234390259 (0.009s), train acc: 0.659\n",
      "Batch 20/52, loss: 0.6527373492717743 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6506221175193787 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6654993712902069 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.6778214991092681 (0.010s), train acc: 0.597\n",
      "Batch 52/52, loss: 0.6299736797809601 (0.003s), train acc: 0.600\n",
      "\n",
      "-----Epoch 232/500-----\n",
      "Batch 10/52, loss: 0.6456798791885376 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6467537641525268 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6743796050548554 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6577534317970276 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.6417398810386657 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6181765496730804 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 233/500-----\n",
      "Batch 10/52, loss: 0.6551643908023834 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6469552457332611 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6510479509830475 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6777558326721191 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6304258584976197 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6556066274642944 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 234/500-----\n",
      "Batch 10/52, loss: 0.6478005945682526 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6628118693828583 (0.009s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.668716549873352 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6316881597042083 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6580967366695404 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.603598028421402 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 235/500-----\n",
      "Batch 10/52, loss: 0.6414634406566619 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6704062223434448 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6477589130401611 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6537194669246673 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.645618611574173 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6760459840297699 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 236/500-----\n",
      "Batch 10/52, loss: 0.6333345592021942 (0.009s), train acc: 0.641\n",
      "Batch 20/52, loss: 0.6842094600200653 (0.010s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.644220232963562 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6386238992214203 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6666632950305938 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6308525204658508 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 237/500-----\n",
      "Batch 10/52, loss: 0.6508026957511902 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6322958588600158 (0.009s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6531208455562592 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6714479386806488 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6561474442481995 (0.010s), train acc: 0.599\n",
      "Batch 52/52, loss: 0.6341176629066467 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 238/500-----\n",
      "Batch 10/52, loss: 0.6667422711849212 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6338850080966949 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6731916606426239 (0.010s), train acc: 0.601\n",
      "Batch 40/52, loss: 0.6426984310150147 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.643045836687088 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.649091899394989 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 239/500-----\n",
      "Batch 10/52, loss: 0.6651432514190674 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6326908767223358 (0.010s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.6645167291164398 (0.010s), train acc: 0.595\n",
      "Batch 40/52, loss: 0.646971833705902 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6508873522281646 (0.010s), train acc: 0.605\n",
      "Batch 52/52, loss: 0.6478298902511597 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 240/500-----\n",
      "Batch 10/52, loss: 0.6728029012680053 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6472995400428772 (0.010s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6318504989147187 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.6560365259647369 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6383748769760131 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.7225868403911591 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 241/500-----\n",
      "Batch 10/52, loss: 0.6401651203632355 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.6520137548446655 (0.009s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6390162527561187 (0.010s), train acc: 0.627\n",
      "Batch 40/52, loss: 0.659874165058136 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6723937928676605 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6271785497665405 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 242/500-----\n",
      "Batch 10/52, loss: 0.6525746166706086 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6325116217136383 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.640613216161728 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6714886248111724 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6758849143981933 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.591376930475235 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 243/500-----\n",
      "Batch 10/52, loss: 0.6551416397094727 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6689927458763123 (0.009s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.6285749197006225 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6654577732086182 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6429257273674012 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6475904583930969 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 244/500-----\n",
      "Batch 10/52, loss: 0.6583940386772156 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.653486704826355 (0.009s), train acc: 0.581\n",
      "Batch 30/52, loss: 0.6646608650684357 (0.010s), train acc: 0.582\n",
      "Batch 40/52, loss: 0.6577810227870942 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6285668015480042 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6483303308486938 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 245/500-----\n",
      "Batch 10/52, loss: 0.6452532470226288 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6409701108932495 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6757081568241119 (0.010s), train acc: 0.591\n",
      "Batch 40/52, loss: 0.6386506259441376 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.664920973777771 (0.010s), train acc: 0.602\n",
      "Batch 52/52, loss: 0.6409494876861572 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 246/500-----\n",
      "Batch 10/52, loss: 0.6799822092056275 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6485349118709565 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6496729552745819 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6437659502029419 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6480290174484253 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6049564480781555 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 247/500-----\n",
      "Batch 10/52, loss: 0.6367432355880738 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6553879737854004 (0.009s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6620051622390747 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6584540724754333 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6421452283859252 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6885731816291809 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 248/500-----\n",
      "Batch 10/52, loss: 0.6386781811714173 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6373057305812836 (0.009s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.6781649231910706 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.660077714920044 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.636475557088852 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6936313211917877 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 249/500-----\n",
      "Batch 10/52, loss: 0.6387046277523041 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6467647910118103 (0.009s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6428905487060547 (0.010s), train acc: 0.628\n",
      "Batch 40/52, loss: 0.6404855251312256 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.687879878282547 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6696541607379913 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 250/500-----\n",
      "Batch 10/52, loss: 0.6445562899112701 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6613766074180603 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6469479620456695 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6522445023059845 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6487709581851959 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6799719929695129 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 251/500-----\n",
      "Batch 10/52, loss: 0.6505158901214599 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6328619658946991 (0.009s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6641463756561279 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6505460917949677 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6541033029556275 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6747678518295288 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 252/500-----\n",
      "Batch 10/52, loss: 0.6407951891422272 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6660324454307556 (0.010s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.655628091096878 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6621398031711578 (0.010s), train acc: 0.596\n",
      "Batch 50/52, loss: 0.6449784636497498 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6283241510391235 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 253/500-----\n",
      "Batch 10/52, loss: 0.6637842714786529 (0.009s), train acc: 0.569\n",
      "Batch 20/52, loss: 0.6360367357730865 (0.009s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6453614354133606 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6655652582645416 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6558918714523315 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6436489820480347 (0.003s), train acc: 0.603\n",
      "\n",
      "-----Epoch 254/500-----\n",
      "Batch 10/52, loss: 0.6552810728549957 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6382088780403137 (0.009s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.659487271308899 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6497296333312989 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6552950382232666 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6677194833755493 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 255/500-----\n",
      "Batch 10/52, loss: 0.6252916753292084 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.6562456429004669 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6589406609535218 (0.010s), train acc: 0.618\n",
      "Batch 40/52, loss: 0.6516902089118958 (0.010s), train acc: 0.620\n",
      "Batch 50/52, loss: 0.6668413281440735 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6545448303222656 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 256/500-----\n",
      "Batch 10/52, loss: 0.636855274438858 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6588805794715882 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.669615375995636 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6486342132091523 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6465922951698303 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6399177014827728 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 257/500-----\n",
      "Batch 10/52, loss: 0.6389227271080017 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6606514394283295 (0.010s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6466688573360443 (0.010s), train acc: 0.618\n",
      "Batch 40/52, loss: 0.6488847732543945 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6670426666736603 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6535804271697998 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 258/500-----\n",
      "Batch 10/52, loss: 0.6627495110034942 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6512655019760132 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6542725443840027 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6402916133403778 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6531617879867554 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6523712575435638 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 259/500-----\n",
      "Batch 10/52, loss: 0.6631855249404908 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6474559426307678 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6516673028469085 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6288606643676757 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6768758535385132 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6142292320728302 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 260/500-----\n",
      "Batch 10/52, loss: 0.6305670857429504 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.6593835175037384 (0.010s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6348827362060547 (0.010s), train acc: 0.625\n",
      "Batch 40/52, loss: 0.6761139869689942 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.660082095861435 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6457255482673645 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 261/500-----\n",
      "Batch 10/52, loss: 0.626767349243164 (0.009s), train acc: 0.659\n",
      "Batch 20/52, loss: 0.6699230790138244 (0.009s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6513047456741333 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6525544166564942 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6513217926025391 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6921185851097107 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 262/500-----\n",
      "Batch 10/52, loss: 0.6379114151000976 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6657437920570374 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6583822786808013 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6469611942768096 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6518081486225128 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.630719780921936 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 263/500-----\n",
      "Batch 10/52, loss: 0.6612457990646362 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6367383778095246 (0.009s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6650199830532074 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6437946736812592 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6540374159812927 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6472179889678955 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 264/500-----\n",
      "Batch 10/52, loss: 0.655940979719162 (0.009s), train acc: 0.572\n",
      "Batch 20/52, loss: 0.6413392543792724 (0.010s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6667495429515838 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6550762891769409 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6308939635753632 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6882539689540863 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 265/500-----\n",
      "Batch 10/52, loss: 0.64824737906456 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6517649233341217 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6507703304290772 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6634030103683471 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6293507218360901 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.734710156917572 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 266/500-----\n",
      "Batch 10/52, loss: 0.6491491973400116 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6416239321231842 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6528741478919983 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6537591278553009 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6512228429317475 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.7026166617870331 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 267/500-----\n",
      "Batch 10/52, loss: 0.6379641950130462 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6301749646663666 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6529269754886627 (0.010s), train acc: 0.627\n",
      "Batch 40/52, loss: 0.6669198453426362 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6731623768806457 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6416177153587341 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 268/500-----\n",
      "Batch 10/52, loss: 0.6421158730983734 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6293132424354553 (0.009s), train acc: 0.642\n",
      "Batch 30/52, loss: 0.6628688514232636 (0.009s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.678422349691391 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6417077004909515 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.6851982176303864 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 269/500-----\n",
      "Batch 10/52, loss: 0.6438198626041413 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6525818824768066 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6421071112155914 (0.010s), train acc: 0.618\n",
      "Batch 40/52, loss: 0.6507539093494416 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6579377233982087 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.7064881026744843 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 270/500-----\n",
      "Batch 10/52, loss: 0.6609201312065125 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.6530421435832977 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6469023942947387 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6522375345230103 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6548621475696563 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.5968887209892273 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 271/500-----\n",
      "Batch 10/52, loss: 0.6499241173267365 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6423367977142334 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6673421263694763 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6353484809398651 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6559756338596344 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6863497495651245 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 272/500-----\n",
      "Batch 10/52, loss: 0.6314972341060638 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6781821072101593 (0.009s), train acc: 0.584\n",
      "Batch 30/52, loss: 0.6551340818405151 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6374435186386108 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6626845836639405 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6262588500976562 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 273/500-----\n",
      "Batch 10/52, loss: 0.647434413433075 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6839691758155823 (0.010s), train acc: 0.591\n",
      "Batch 30/52, loss: 0.6524992763996125 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6424220860004425 (0.010s), train acc: 0.599\n",
      "Batch 50/52, loss: 0.6490760207176208 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.5821921229362488 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 274/500-----\n",
      "Batch 10/52, loss: 0.6395582497119904 (0.009s), train acc: 0.650\n",
      "Batch 20/52, loss: 0.6640826821327209 (0.009s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.655917888879776 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6518995225429535 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6529358208179474 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6590955257415771 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 275/500-----\n",
      "Batch 10/52, loss: 0.6510586142539978 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6916891872882843 (0.010s), train acc: 0.580\n",
      "Batch 30/52, loss: 0.6275276362895965 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6695378124713898 (0.010s), train acc: 0.592\n",
      "Batch 50/52, loss: 0.6209560573101044 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6358031928539276 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 276/500-----\n",
      "Batch 10/52, loss: 0.6626984298229217 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6439495742321014 (0.010s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6409596860408783 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.640029352903366 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6704268872737884 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6686975359916687 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 277/500-----\n",
      "Batch 10/52, loss: 0.6611683607101441 (0.009s), train acc: 0.566\n",
      "Batch 20/52, loss: 0.6108195543289184 (0.009s), train acc: 0.637\n",
      "Batch 30/52, loss: 0.6683892190456391 (0.010s), train acc: 0.625\n",
      "Batch 40/52, loss: 0.6537551581859589 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6616856813430786 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6690700352191925 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 278/500-----\n",
      "Batch 10/52, loss: 0.6499909520149231 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6505359292030335 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6607506334781647 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.643564248085022 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6525881111621856 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6721455156803131 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 279/500-----\n",
      "Batch 10/52, loss: 0.6425611495971679 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6423739612102508 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.660565173625946 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6580554664134979 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6504465460777282 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6735436320304871 (0.003s), train acc: 0.604\n",
      "\n",
      "-----Epoch 280/500-----\n",
      "Batch 10/52, loss: 0.6505959093570709 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6524746239185333 (0.009s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6540724873542786 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.637957614660263 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6536092877388 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.7121785581111908 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 281/500-----\n",
      "Batch 10/52, loss: 0.6817896068096161 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6503969371318817 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6369930863380432 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6482374668121338 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6446500062942505 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6327893137931824 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 282/500-----\n",
      "Batch 10/52, loss: 0.6208606898784638 (0.009s), train acc: 0.659\n",
      "Batch 20/52, loss: 0.6646206796169281 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.634870707988739 (0.010s), train acc: 0.631\n",
      "Batch 40/52, loss: 0.6861477017402648 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6527361750602723 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6535607576370239 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 283/500-----\n",
      "Batch 10/52, loss: 0.6597688555717468 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6578315079212189 (0.009s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.662621694803238 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6413759350776672 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6344390690326691 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6646452248096466 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 284/500-----\n",
      "Batch 10/52, loss: 0.6927553653717041 (0.009s), train acc: 0.562\n",
      "Batch 20/52, loss: 0.648327785730362 (0.009s), train acc: 0.580\n",
      "Batch 30/52, loss: 0.6465546786785126 (0.010s), train acc: 0.589\n",
      "Batch 40/52, loss: 0.6310898423194885 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6346801847219468 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6676792204380035 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 285/500-----\n",
      "Batch 10/52, loss: 0.6597049474716187 (0.009s), train acc: 0.562\n",
      "Batch 20/52, loss: 0.6336625099182129 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6486099362373352 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6533404409885406 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6581452548503876 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6679189205169678 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 286/500-----\n",
      "Batch 10/52, loss: 0.6436485171318054 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6505093336105346 (0.009s), train acc: 0.589\n",
      "Batch 30/52, loss: 0.6506360113620758 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6546179056167603 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.653532612323761 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6779755353927612 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 287/500-----\n",
      "Batch 10/52, loss: 0.6442094087600708 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6391489505767822 (0.009s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.650067126750946 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6573084950447082 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6665966212749481 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6556186378002167 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 288/500-----\n",
      "Batch 10/52, loss: 0.6595429003238678 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6428786754608155 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6478269875049592 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6682006776332855 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.636155492067337 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6532701253890991 (0.003s), train acc: 0.616\n",
      "\n",
      "-----Epoch 289/500-----\n",
      "Batch 10/52, loss: 0.6578719615936279 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6555889070034027 (0.010s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6697409868240356 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.634181696176529 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6366401433944702 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6678600907325745 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 290/500-----\n",
      "Batch 10/52, loss: 0.6326339840888977 (0.009s), train acc: 0.669\n",
      "Batch 20/52, loss: 0.6536380350589752 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6538282692432403 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6510415077209473 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.666252589225769 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6576088070869446 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 291/500-----\n",
      "Batch 10/52, loss: 0.6559568583965302 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6470265746116638 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6510887622833252 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6526619970798493 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6607490599155426 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6205920577049255 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 292/500-----\n",
      "Batch 10/52, loss: 0.6346511244773865 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6418321609497071 (0.009s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6659970641136169 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6460775971412659 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6627963364124299 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.680985152721405 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 293/500-----\n",
      "Batch 10/52, loss: 0.6375468373298645 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6591081380844116 (0.009s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6576411783695221 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6437171876430512 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6472829282283783 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.7086768746376038 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 294/500-----\n",
      "Batch 10/52, loss: 0.6586884021759033 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6749745607376099 (0.010s), train acc: 0.581\n",
      "Batch 30/52, loss: 0.6279487907886505 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6406981587409973 (0.010s), train acc: 0.618\n",
      "Batch 50/52, loss: 0.6633384943008422 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6258396804332733 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 295/500-----\n",
      "Batch 10/52, loss: 0.6341818273067474 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6453620553016662 (0.009s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6572969853878021 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6854764580726623 (0.010s), train acc: 0.600\n",
      "Batch 50/52, loss: 0.6343418776988983 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6532943844795227 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 296/500-----\n",
      "Batch 10/52, loss: 0.6484395802021027 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6535901427268982 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.660258686542511 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6532547533512115 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6373795926570892 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6708903610706329 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 297/500-----\n",
      "Batch 10/52, loss: 0.6675414144992828 (0.009s), train acc: 0.569\n",
      "Batch 20/52, loss: 0.6334333896636963 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6683707654476165 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6437650442123413 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6536805510520936 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6045258641242981 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 298/500-----\n",
      "Batch 10/52, loss: 0.6381794273853302 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6631618797779083 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6415028512477875 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6594196856021881 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.66445871591568 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6018864810466766 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 299/500-----\n",
      "Batch 10/52, loss: 0.6941105961799622 (0.009s), train acc: 0.556\n",
      "Batch 20/52, loss: 0.6394115030765534 (0.009s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6521226584911346 (0.010s), train acc: 0.601\n",
      "Batch 40/52, loss: 0.6204457879066467 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6515646159648896 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6468794643878937 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 300/500-----\n",
      "Batch 10/52, loss: 0.6672742545604706 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.6471939265727997 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6484252095222474 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6317703664302826 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6519363999366761 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.7137865722179413 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 301/500-----\n",
      "Batch 10/52, loss: 0.6473562896251679 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6534100413322449 (0.009s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6644834697246551 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.648392653465271 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.650484585762024 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6356796026229858 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 302/500-----\n",
      "Batch 10/52, loss: 0.6523826241493225 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6730223953723907 (0.009s), train acc: 0.588\n",
      "Batch 30/52, loss: 0.6431293368339539 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6470565021038055 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.6444197177886963 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6420270502567291 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 303/500-----\n",
      "Batch 10/52, loss: 0.6198134958744049 (0.009s), train acc: 0.659\n",
      "Batch 20/52, loss: 0.6583442449569702 (0.009s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6664937019348145 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6609932839870453 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6526184856891633 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6604078114032745 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 304/500-----\n",
      "Batch 10/52, loss: 0.6204154670238495 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6856866240501404 (0.009s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6367620766162873 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6565435051918029 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6468746125698089 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.7005247473716736 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 305/500-----\n",
      "Batch 10/52, loss: 0.6568584561347961 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6499721527099609 (0.009s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6352809369564056 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6464082181453705 (0.010s), train acc: 0.620\n",
      "Batch 50/52, loss: 0.666334617137909 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6924614608287811 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 306/500-----\n",
      "Batch 10/52, loss: 0.644809889793396 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6298089861869812 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6765768885612488 (0.009s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6514325082302094 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6578093051910401 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6377816200256348 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 307/500-----\n",
      "Batch 10/52, loss: 0.6614640474319458 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6552182197570801 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6375228464603424 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6584554970264435 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6449166595935821 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6558678150177002 (0.003s), train acc: 0.616\n",
      "\n",
      "-----Epoch 308/500-----\n",
      "Batch 10/52, loss: 0.6603593170642853 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6450123250484466 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6336223304271698 (0.009s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6636396646499634 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6546440720558167 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6411273181438446 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 309/500-----\n",
      "Batch 10/52, loss: 0.6293045699596405 (0.009s), train acc: 0.666\n",
      "Batch 20/52, loss: 0.6727848768234252 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6423346996307373 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6370013296604157 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6705508649349212 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6755638122558594 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 310/500-----\n",
      "Batch 10/52, loss: 0.6505279898643493 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6720300674438476 (0.009s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6306766748428345 (0.009s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.646057665348053 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.6638529181480408 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6371453106403351 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 311/500-----\n",
      "Batch 10/52, loss: 0.6448647975921631 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6495225965976715 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6644633173942566 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6642650067806244 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6380843222141266 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6260952949523926 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 312/500-----\n",
      "Batch 10/52, loss: 0.6456179261207581 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6640706539154053 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6336212635040284 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6671633541584014 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6461518287658692 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6459949612617493 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 313/500-----\n",
      "Batch 10/52, loss: 0.6449132919311523 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.666839873790741 (0.009s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6412742972373963 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6491005718708038 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6585791885852814 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6278854012489319 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 314/500-----\n",
      "Batch 10/52, loss: 0.6608431696891784 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.627304482460022 (0.010s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6529235184192658 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6466451287269592 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.658322674036026 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6977233290672302 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 315/500-----\n",
      "Batch 10/52, loss: 0.6556710362434387 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6588351905345917 (0.009s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6496997594833374 (0.010s), train acc: 0.591\n",
      "Batch 40/52, loss: 0.6532068371772766 (0.010s), train acc: 0.597\n",
      "Batch 50/52, loss: 0.6493112146854401 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.5896716713905334 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 316/500-----\n",
      "Batch 10/52, loss: 0.6508512496948242 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6576971411705017 (0.010s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6549347579479218 (0.010s), train acc: 0.590\n",
      "Batch 40/52, loss: 0.6609443724155426 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6446049392223359 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.572252094745636 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 317/500-----\n",
      "Batch 10/52, loss: 0.6505992770195007 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6464362204074859 (0.010s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6365886747837066 (0.010s), train acc: 0.620\n",
      "Batch 40/52, loss: 0.6353621780872345 (0.010s), train acc: 0.620\n",
      "Batch 50/52, loss: 0.6748818814754486 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.7273290753364563 (0.003s), train acc: 0.602\n",
      "\n",
      "-----Epoch 318/500-----\n",
      "Batch 10/52, loss: 0.6501008927822113 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6703039050102234 (0.009s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.6400840401649475 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6436167657375336 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6534370720386505 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6515867114067078 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 319/500-----\n",
      "Batch 10/52, loss: 0.6485853910446167 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.6217797279357911 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6934697389602661 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6534330010414123 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6508418083190918 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.591810017824173 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 320/500-----\n",
      "Batch 10/52, loss: 0.6472458124160767 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6401578068733216 (0.010s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6344668805599213 (0.010s), train acc: 0.628\n",
      "Batch 40/52, loss: 0.6784569621086121 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6574084162712097 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6625097990036011 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 321/500-----\n",
      "Batch 10/52, loss: 0.6670415043830872 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6440831363201142 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6521764755249023 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6336369574069977 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6584491550922393 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6536532938480377 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 322/500-----\n",
      "Batch 10/52, loss: 0.6398811221122742 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6455070495605468 (0.009s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6684155225753784 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6632852256298065 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6451800346374512 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6066458225250244 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 323/500-----\n",
      "Batch 10/52, loss: 0.6561905205249786 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6436278343200683 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6469433009624481 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6358867108821868 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.658565354347229 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.7278000712394714 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 324/500-----\n",
      "Batch 10/52, loss: 0.6262695372104645 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.685234671831131 (0.009s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6381085991859436 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6580840706825256 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6504567742347718 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.657676100730896 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 325/500-----\n",
      "Batch 10/52, loss: 0.6556831181049347 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6538049280643463 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6428208410739898 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.639898830652237 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6627386510372162 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6598816514015198 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 326/500-----\n",
      "Batch 10/52, loss: 0.6587346434593201 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6600537955760956 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6420853912830353 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6414861738681793 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6568640947341919 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6204108595848083 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 327/500-----\n",
      "Batch 10/52, loss: 0.663229089975357 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6547655522823334 (0.009s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6586820662021637 (0.010s), train acc: 0.592\n",
      "Batch 40/52, loss: 0.6381917834281922 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6566200733184815 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.5754789710044861 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 328/500-----\n",
      "Batch 10/52, loss: 0.6666159927845001 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6576759338378906 (0.009s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6290444076061249 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6604340255260468 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6500700116157532 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.6511058211326599 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 329/500-----\n",
      "Batch 10/52, loss: 0.6504087388515473 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6466395914554596 (0.009s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.654932188987732 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.642642480134964 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6628502786159516 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6707803308963776 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 330/500-----\n",
      "Batch 10/52, loss: 0.6533317804336548 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6669779062271118 (0.010s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6640636444091796 (0.010s), train acc: 0.591\n",
      "Batch 40/52, loss: 0.6409733474254609 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6371401011943817 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.614067554473877 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 331/500-----\n",
      "Batch 10/52, loss: 0.6610438168048859 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6718894124031067 (0.009s), train acc: 0.578\n",
      "Batch 30/52, loss: 0.6610337376594544 (0.010s), train acc: 0.584\n",
      "Batch 40/52, loss: 0.6420116782188415 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6359793365001678 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.591545432806015 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 332/500-----\n",
      "Batch 10/52, loss: 0.6422630369663238 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.643989360332489 (0.009s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.6517136633396149 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6362349510192871 (0.010s), train acc: 0.618\n",
      "Batch 50/52, loss: 0.682648503780365 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.628614068031311 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 333/500-----\n",
      "Batch 10/52, loss: 0.6453471958637238 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.661855286359787 (0.009s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6147714555263519 (0.010s), train acc: 0.624\n",
      "Batch 40/52, loss: 0.658676129579544 (0.010s), train acc: 0.622\n",
      "Batch 50/52, loss: 0.6721918523311615 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.6626231968402863 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 334/500-----\n",
      "Batch 10/52, loss: 0.6366848230361939 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6528367459774017 (0.009s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.640656566619873 (0.010s), train acc: 0.626\n",
      "Batch 40/52, loss: 0.6671206235885621 (0.010s), train acc: 0.622\n",
      "Batch 50/52, loss: 0.6575256586074829 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6458437442779541 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 335/500-----\n",
      "Batch 10/52, loss: 0.6594859004020691 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6439326107501984 (0.009s), train acc: 0.588\n",
      "Batch 30/52, loss: 0.6445545613765716 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6479359567165375 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6615147054195404 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6277779936790466 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 336/500-----\n",
      "Batch 10/52, loss: 0.6627657115459442 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.6606481432914734 (0.010s), train acc: 0.589\n",
      "Batch 30/52, loss: 0.6592964589595794 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6468393087387085 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6310984909534454 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6240658760070801 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 337/500-----\n",
      "Batch 10/52, loss: 0.6485164701938629 (0.009s), train acc: 0.641\n",
      "Batch 20/52, loss: 0.6320011794567109 (0.010s), train acc: 0.633\n",
      "Batch 30/52, loss: 0.6582044959068298 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6445177674293519 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6820912063121796 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6034206449985504 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 338/500-----\n",
      "Batch 10/52, loss: 0.6641663193702698 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6549951493740082 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6519948184490204 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6508003413677216 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6303255081176757 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6570093929767609 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 339/500-----\n",
      "Batch 10/52, loss: 0.6464172959327698 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6429149329662323 (0.010s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6802033543586731 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6671796798706054 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6255109667778015 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6163811683654785 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 340/500-----\n",
      "Batch 10/52, loss: 0.6544867515563965 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.635179877281189 (0.010s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6369226396083831 (0.010s), train acc: 0.630\n",
      "Batch 40/52, loss: 0.6694018185138703 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.656447958946228 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6497350037097931 (0.003s), train acc: 0.606\n",
      "\n",
      "-----Epoch 341/500-----\n",
      "Batch 10/52, loss: 0.6571888029575348 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6646298587322235 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6541215181350708 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6325998902320862 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6446992695331574 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.665332019329071 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 342/500-----\n",
      "Batch 10/52, loss: 0.6682129085063935 (0.009s), train acc: 0.562\n",
      "Batch 20/52, loss: 0.6250351309776306 (0.009s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6478673756122589 (0.009s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6220238029956817 (0.010s), train acc: 0.629\n",
      "Batch 50/52, loss: 0.6890977084636688 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.659006655216217 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 343/500-----\n",
      "Batch 10/52, loss: 0.6328691780567169 (0.009s), train acc: 0.653\n",
      "Batch 20/52, loss: 0.6480170428752899 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6519447684288024 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.656071662902832 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6613306701183319 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.673588216304779 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 344/500-----\n",
      "Batch 10/52, loss: 0.6280884265899658 (0.009s), train acc: 0.675\n",
      "Batch 20/52, loss: 0.6327526211738587 (0.010s), train acc: 0.672\n",
      "Batch 30/52, loss: 0.6699381172657013 (0.010s), train acc: 0.641\n",
      "Batch 40/52, loss: 0.6702734470367432 (0.010s), train acc: 0.627\n",
      "Batch 50/52, loss: 0.6626280844211578 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6252488195896149 (0.003s), train acc: 0.619\n",
      "\n",
      "-----Epoch 345/500-----\n",
      "Batch 10/52, loss: 0.6483140349388122 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6817731499671936 (0.009s), train acc: 0.583\n",
      "Batch 30/52, loss: 0.6365935862064361 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6364132523536682 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6610932886600495 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6142374277114868 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 346/500-----\n",
      "Batch 10/52, loss: 0.6655099809169769 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6581192314624786 (0.009s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.649465698003769 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6528758406639099 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6309768915176391 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.64899942278862 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 347/500-----\n",
      "Batch 10/52, loss: 0.6393188774585724 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6545543372631073 (0.010s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.6274531841278076 (0.010s), train acc: 0.630\n",
      "Batch 40/52, loss: 0.6875656366348266 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6540110588073731 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.5963526368141174 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 348/500-----\n",
      "Batch 10/52, loss: 0.6585991621017456 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.6646847128868103 (0.010s), train acc: 0.584\n",
      "Batch 30/52, loss: 0.6242160975933075 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.650120484828949 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6591140627861023 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6338140368461609 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 349/500-----\n",
      "Batch 10/52, loss: 0.6620151281356812 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6499956905841827 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6391758739948272 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6644062936306 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6404584884643555 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6421748101711273 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 350/500-----\n",
      "Batch 10/52, loss: 0.6731370985507965 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.671581357717514 (0.010s), train acc: 0.573\n",
      "Batch 30/52, loss: 0.649011904001236 (0.010s), train acc: 0.586\n",
      "Batch 40/52, loss: 0.636373883485794 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.6291261076927185 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6458677649497986 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 351/500-----\n",
      "Batch 10/52, loss: 0.6369338452816009 (0.009s), train acc: 0.641\n",
      "Batch 20/52, loss: 0.6439269483089447 (0.009s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6593288302421569 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6655542969703674 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6532752633094787 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6128152310848236 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 352/500-----\n",
      "Batch 10/52, loss: 0.6417384624481202 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6406285166740417 (0.010s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6394168913364411 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6815937101840973 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6431019067764282 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.67973193526268 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 353/500-----\n",
      "Batch 10/52, loss: 0.6506137371063232 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6621732532978057 (0.009s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.645275866985321 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6543324649333954 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6354354679584503 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6768242716789246 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 354/500-----\n",
      "Batch 10/52, loss: 0.6577844619750977 (0.009s), train acc: 0.559\n",
      "Batch 20/52, loss: 0.6561909258365631 (0.009s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.63204425573349 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6619441568851471 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6340849041938782 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.7163344323635101 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 355/500-----\n",
      "Batch 10/52, loss: 0.6622021436691284 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6609777569770813 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6477677822113037 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6345678448677063 (0.010s), train acc: 0.618\n",
      "Batch 50/52, loss: 0.6489335715770721 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6497298181056976 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 356/500-----\n",
      "Batch 10/52, loss: 0.6605376124382019 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6446825683116912 (0.010s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.6487432360649109 (0.010s), train acc: 0.618\n",
      "Batch 40/52, loss: 0.645505279302597 (0.010s), train acc: 0.620\n",
      "Batch 50/52, loss: 0.6558799147605896 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6528298258781433 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 357/500-----\n",
      "Batch 10/52, loss: 0.653898811340332 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6623193502426148 (0.009s), train acc: 0.597\n",
      "Batch 30/52, loss: 0.6506086111068725 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6657941222190857 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6279374301433563 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.6143413782119751 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 358/500-----\n",
      "Batch 10/52, loss: 0.6487671434879303 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6390338897705078 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6344482660293579 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6592337608337402 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.672053599357605 (0.010s), train acc: 0.607\n",
      "Batch 52/52, loss: 0.6513660550117493 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 359/500-----\n",
      "Batch 10/52, loss: 0.6662693977355957 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6440451800823211 (0.009s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6444515883922577 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.646964567899704 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6486964344978332 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6651051342487335 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 360/500-----\n",
      "Batch 10/52, loss: 0.6807969808578491 (0.009s), train acc: 0.569\n",
      "Batch 20/52, loss: 0.6126980006694793 (0.009s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.6510040879249572 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6344939172267914 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.6767759919166565 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.652509868144989 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 361/500-----\n",
      "Batch 10/52, loss: 0.65604327917099 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6680804789066315 (0.009s), train acc: 0.591\n",
      "Batch 30/52, loss: 0.6238782286643982 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6459409058094024 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6639720320701599 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6234345138072968 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 362/500-----\n",
      "Batch 10/52, loss: 0.634401261806488 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.6757517993450165 (0.009s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6855353951454163 (0.010s), train acc: 0.584\n",
      "Batch 40/52, loss: 0.6400615036487579 (0.010s), train acc: 0.598\n",
      "Batch 50/52, loss: 0.6240250408649445 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.6735706627368927 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 363/500-----\n",
      "Batch 10/52, loss: 0.6469676196575165 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6451982975006103 (0.009s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6784619390964508 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.625678551197052 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6539321124553681 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6712439656257629 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 364/500-----\n",
      "Batch 10/52, loss: 0.6324235737323761 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6713756680488586 (0.009s), train acc: 0.586\n",
      "Batch 30/52, loss: 0.6736531615257263 (0.010s), train acc: 0.594\n",
      "Batch 40/52, loss: 0.633652639389038 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6377521097660065 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6865343451499939 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 365/500-----\n",
      "Batch 10/52, loss: 0.6484188199043274 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6351156532764435 (0.009s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6562501132488251 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6500429630279541 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6592973351478577 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6685819029808044 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 366/500-----\n",
      "Batch 10/52, loss: 0.6417260468006134 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6476466536521912 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6515679955482483 (0.010s), train acc: 0.620\n",
      "Batch 40/52, loss: 0.6708561480045319 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6502144932746887 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6101618111133575 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 367/500-----\n",
      "Batch 10/52, loss: 0.6622989237308502 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6606462955474853 (0.009s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6564317643642426 (0.010s), train acc: 0.594\n",
      "Batch 40/52, loss: 0.6346894919872283 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6370910048484802 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6635433435440063 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 368/500-----\n",
      "Batch 10/52, loss: 0.6497926652431488 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6595426619052887 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6568401515483856 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.622639262676239 (0.010s), train acc: 0.618\n",
      "Batch 50/52, loss: 0.6542005658149719 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6956948637962341 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 369/500-----\n",
      "Batch 10/52, loss: 0.6483474373817444 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6481707990169525 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6183930993080139 (0.010s), train acc: 0.620\n",
      "Batch 40/52, loss: 0.6718864262104034 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6655700325965881 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6466284096240997 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 370/500-----\n",
      "Batch 10/52, loss: 0.6335982978343964 (0.009s), train acc: 0.650\n",
      "Batch 20/52, loss: 0.6777596652507782 (0.009s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6390347063541413 (0.010s), train acc: 0.624\n",
      "Batch 40/52, loss: 0.6479377150535583 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6515478909015655 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.656143456697464 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 371/500-----\n",
      "Batch 10/52, loss: 0.6553296446800232 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6406641364097595 (0.009s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.6489030957221985 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6559394776821137 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.652909231185913 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6479116380214691 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 372/500-----\n",
      "Batch 10/52, loss: 0.6337077200412751 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6375784814357758 (0.009s), train acc: 0.636\n",
      "Batch 30/52, loss: 0.6570865571498871 (0.010s), train acc: 0.628\n",
      "Batch 40/52, loss: 0.6955846965312957 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6304295063018799 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.6542869806289673 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 373/500-----\n",
      "Batch 10/52, loss: 0.6533597409725189 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6496048688888549 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6271703898906708 (0.010s), train acc: 0.624\n",
      "Batch 40/52, loss: 0.6738175868988037 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6513084232807159 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6608657538890839 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 374/500-----\n",
      "Batch 10/52, loss: 0.6597341120243072 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.646915340423584 (0.009s), train acc: 0.623\n",
      "Batch 30/52, loss: 0.6518279731273651 (0.009s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6446695804595948 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.6558735966682434 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6165635585784912 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 375/500-----\n",
      "Batch 10/52, loss: 0.6521420001983642 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6759217500686645 (0.009s), train acc: 0.572\n",
      "Batch 30/52, loss: 0.6395992875099182 (0.010s), train acc: 0.597\n",
      "Batch 40/52, loss: 0.6348041951656341 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6551025331020355 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.618461012840271 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 376/500-----\n",
      "Batch 10/52, loss: 0.6480856478214264 (0.009s), train acc: 0.637\n",
      "Batch 20/52, loss: 0.6628458857536316 (0.009s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6557808876037597 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6693787813186646 (0.010s), train acc: 0.601\n",
      "Batch 50/52, loss: 0.6292864918708801 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6087435483932495 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 377/500-----\n",
      "Batch 10/52, loss: 0.6440878689289093 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6509109795093536 (0.009s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6380661725997925 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.66437908411026 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.645295649766922 (0.010s), train acc: 0.619\n",
      "Batch 52/52, loss: 0.7148916125297546 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 378/500-----\n",
      "Batch 10/52, loss: 0.6304502129554749 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6601432859897614 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6460569679737092 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6449822843074798 (0.010s), train acc: 0.620\n",
      "Batch 50/52, loss: 0.6563289999961853 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.7298116087913513 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 379/500-----\n",
      "Batch 10/52, loss: 0.637529581785202 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6541821777820587 (0.009s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6702791213989258 (0.010s), train acc: 0.595\n",
      "Batch 40/52, loss: 0.6329972922801972 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.659070736169815 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6441621780395508 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 380/500-----\n",
      "Batch 10/52, loss: 0.6608444571495056 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6375873565673829 (0.009s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6517337739467621 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6562530517578125 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6459108352661133 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6584852635860443 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 381/500-----\n",
      "Batch 10/52, loss: 0.6399788618087768 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6495247304439544 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.642029732465744 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6199511229991913 (0.010s), train acc: 0.627\n",
      "Batch 50/52, loss: 0.6952222466468811 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6763308048248291 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 382/500-----\n",
      "Batch 10/52, loss: 0.6399816274642944 (0.009s), train acc: 0.650\n",
      "Batch 20/52, loss: 0.6578508019447327 (0.009s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6577280819416046 (0.010s), train acc: 0.620\n",
      "Batch 40/52, loss: 0.6472671091556549 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6463817000389099 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6588880717754364 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 383/500-----\n",
      "Batch 10/52, loss: 0.6704354524612427 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6207309424877167 (0.009s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6760675609111786 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6529151737689972 (0.010s), train acc: 0.603\n",
      "Batch 50/52, loss: 0.64179847240448 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6008079946041107 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 384/500-----\n",
      "Batch 10/52, loss: 0.655213075876236 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6626694977283478 (0.010s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6468985497951507 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.648433268070221 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6494050920009613 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.5879783630371094 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 385/500-----\n",
      "Batch 10/52, loss: 0.6617624759674072 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6587576925754547 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6250002682209015 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6278561294078827 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6875459372997283 (0.010s), train acc: 0.601\n",
      "Batch 52/52, loss: 0.6249635517597198 (0.003s), train acc: 0.605\n",
      "\n",
      "-----Epoch 386/500-----\n",
      "Batch 10/52, loss: 0.6284896671772003 (0.009s), train acc: 0.641\n",
      "Batch 20/52, loss: 0.6439076006412506 (0.010s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.6403557658195496 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.670119172334671 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.641717129945755 (0.010s), train acc: 0.620\n",
      "Batch 52/52, loss: 0.7749727666378021 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 387/500-----\n",
      "Batch 10/52, loss: 0.643244618177414 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6384412109851837 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6707691252231598 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6167755603790284 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.6731270790100098 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6951717436313629 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 388/500-----\n",
      "Batch 10/52, loss: 0.6316714942455292 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6596798598766327 (0.009s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6521447241306305 (0.010s), train acc: 0.618\n",
      "Batch 40/52, loss: 0.6456152856349945 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6536040246486664 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6999905407428741 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 389/500-----\n",
      "Batch 10/52, loss: 0.6449245750904083 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6496469020843506 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6447548151016236 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6708729445934296 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6371164441108703 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6879124939441681 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 390/500-----\n",
      "Batch 10/52, loss: 0.6278141260147094 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6513546645641327 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.671746015548706 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6398260295391083 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6704680621623993 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6063500046730042 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 391/500-----\n",
      "Batch 10/52, loss: 0.6730068743228912 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6442851662635803 (0.009s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.6489856481552124 (0.010s), train acc: 0.626\n",
      "Batch 40/52, loss: 0.6502615869045257 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6540260434150695 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.5557715594768524 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 392/500-----\n",
      "Batch 10/52, loss: 0.6600240111351013 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6740682899951935 (0.009s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.64072545170784 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6530795991420746 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6274512469768524 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6335329413414001 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 393/500-----\n",
      "Batch 10/52, loss: 0.6501560270786285 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6535406649112702 (0.010s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6457697451114655 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6591865003108979 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6515119254589081 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.5960520803928375 (0.003s), train acc: 0.610\n",
      "\n",
      "-----Epoch 394/500-----\n",
      "Batch 10/52, loss: 0.6743590295314789 (0.009s), train acc: 0.562\n",
      "Batch 20/52, loss: 0.6245945572853089 (0.009s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.652259361743927 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6513930857181549 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6493636071681976 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6906383633613586 (0.003s), train acc: 0.609\n",
      "\n",
      "-----Epoch 395/500-----\n",
      "Batch 10/52, loss: 0.6406432092189789 (0.009s), train acc: 0.641\n",
      "Batch 20/52, loss: 0.6416528046131134 (0.010s), train acc: 0.637\n",
      "Batch 30/52, loss: 0.6509497821331024 (0.010s), train acc: 0.626\n",
      "Batch 40/52, loss: 0.6765749275684356 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.6438391149044037 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6361394822597504 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 396/500-----\n",
      "Batch 10/52, loss: 0.6567979991436005 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6510399162769318 (0.009s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6470973253250122 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.652318674325943 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.6431978642940521 (0.010s), train acc: 0.619\n",
      "Batch 52/52, loss: 0.6530351042747498 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 397/500-----\n",
      "Batch 10/52, loss: 0.6648453712463379 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.631154578924179 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6532909572124481 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6701101779937744 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6374932587146759 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.6219941973686218 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 398/500-----\n",
      "Batch 10/52, loss: 0.6435651659965516 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6386084198951721 (0.009s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6505927801132202 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6528132140636445 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6673479199409484 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.6346015334129333 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 399/500-----\n",
      "Batch 10/52, loss: 0.614218020439148 (0.009s), train acc: 0.666\n",
      "Batch 20/52, loss: 0.6739712953567505 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6658724486827851 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.65291348695755 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6457295119762421 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6456238925457001 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 400/500-----\n",
      "Batch 10/52, loss: 0.6650242269039154 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.6697664976119995 (0.009s), train acc: 0.589\n",
      "Batch 30/52, loss: 0.6262099325656891 (0.010s), train acc: 0.602\n",
      "Batch 40/52, loss: 0.6481357395648957 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6486257970333099 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6217898428440094 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 401/500-----\n",
      "Batch 10/52, loss: 0.6571892499923706 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6459310114383697 (0.010s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6478681921958923 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.6395272850990296 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6675199329853058 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6076782047748566 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 402/500-----\n",
      "Batch 10/52, loss: 0.6506280243396759 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.639627605676651 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6469355821609497 (0.010s), train acc: 0.620\n",
      "Batch 40/52, loss: 0.6585277438163757 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6617552042007446 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6128200888633728 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 403/500-----\n",
      "Batch 10/52, loss: 0.6654535472393036 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6493458449840546 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6519731938838959 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6438647687435151 (0.010s), train acc: 0.612\n",
      "Batch 50/52, loss: 0.6437404692173004 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6453603208065033 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 404/500-----\n",
      "Batch 10/52, loss: 0.6470325469970704 (0.009s), train acc: 0.656\n",
      "Batch 20/52, loss: 0.6478230059146881 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6746437072753906 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6281107246875763 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6487208247184754 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6689912676811218 (0.003s), train acc: 0.616\n",
      "\n",
      "-----Epoch 405/500-----\n",
      "Batch 10/52, loss: 0.6461911916732788 (0.009s), train acc: 0.647\n",
      "Batch 20/52, loss: 0.665849369764328 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6391698122024536 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6267801523208618 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6628991544246674 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.7001674473285675 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 406/500-----\n",
      "Batch 10/52, loss: 0.6474859654903412 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6502805590629578 (0.010s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.666402131319046 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6358107805252076 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.649885356426239 (0.010s), train acc: 0.621\n",
      "Batch 52/52, loss: 0.6583641469478607 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 407/500-----\n",
      "Batch 10/52, loss: 0.6445965230464935 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.673603743314743 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6337846696376801 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6338562726974487 (0.010s), train acc: 0.627\n",
      "Batch 50/52, loss: 0.6549823701381683 (0.010s), train acc: 0.622\n",
      "Batch 52/52, loss: 0.7065506875514984 (0.003s), train acc: 0.619\n",
      "\n",
      "-----Epoch 408/500-----\n",
      "Batch 10/52, loss: 0.6438653647899628 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6376818418502808 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6411604523658753 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6874332249164581 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6387806117534638 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6516910791397095 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 409/500-----\n",
      "Batch 10/52, loss: 0.6410828232765198 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6444765150547027 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6575427114963531 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.6483672618865967 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6621607601642608 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6418450772762299 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 410/500-----\n",
      "Batch 10/52, loss: 0.6365270793437958 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6439449071884156 (0.010s), train acc: 0.633\n",
      "Batch 30/52, loss: 0.6478348910808563 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6460127174854279 (0.010s), train acc: 0.627\n",
      "Batch 50/52, loss: 0.670287448167801 (0.010s), train acc: 0.619\n",
      "Batch 52/52, loss: 0.6810663640499115 (0.003s), train acc: 0.616\n",
      "\n",
      "-----Epoch 411/500-----\n",
      "Batch 10/52, loss: 0.6520055770874024 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6384840786457062 (0.010s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6702480256557465 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6570742726325989 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6381190598011017 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.6198127865791321 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 412/500-----\n",
      "Batch 10/52, loss: 0.6387163877487183 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.6644551634788514 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6757835566997528 (0.010s), train acc: 0.591\n",
      "Batch 40/52, loss: 0.6544744193553924 (0.010s), train acc: 0.595\n",
      "Batch 50/52, loss: 0.621025437116623 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6627200245857239 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 413/500-----\n",
      "Batch 10/52, loss: 0.6330958366394043 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6594165802001953 (0.010s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6618308663368225 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6295508921146393 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6646422147750854 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6561965346336365 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 414/500-----\n",
      "Batch 10/52, loss: 0.6579901456832886 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6479389309883118 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6472809970378876 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6647264301776886 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6480632245540618 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.5719221234321594 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 415/500-----\n",
      "Batch 10/52, loss: 0.6660160005092621 (0.009s), train acc: 0.562\n",
      "Batch 20/52, loss: 0.6669827044010163 (0.010s), train acc: 0.581\n",
      "Batch 30/52, loss: 0.6302225649356842 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6525346219539643 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6417756736278534 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6228192746639252 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 416/500-----\n",
      "Batch 10/52, loss: 0.6553879678249359 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6364763379096985 (0.010s), train acc: 0.633\n",
      "Batch 30/52, loss: 0.6591930747032165 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6643113553524017 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6210290431976319 (0.010s), train acc: 0.619\n",
      "Batch 52/52, loss: 0.7134343981742859 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 417/500-----\n",
      "Batch 10/52, loss: 0.6326746225357056 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6418199419975281 (0.010s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.6416000008583069 (0.010s), train acc: 0.631\n",
      "Batch 40/52, loss: 0.6597054541110993 (0.010s), train acc: 0.627\n",
      "Batch 50/52, loss: 0.659435760974884 (0.010s), train acc: 0.620\n",
      "Batch 52/52, loss: 0.7253803610801697 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 418/500-----\n",
      "Batch 10/52, loss: 0.6715677917003632 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6272873044013977 (0.010s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.6632109344005584 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6512233138084411 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6376088857650757 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6510615646839142 (0.003s), train acc: 0.616\n",
      "\n",
      "-----Epoch 419/500-----\n",
      "Batch 10/52, loss: 0.6559897720813751 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6284301102161407 (0.010s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.641279548406601 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6467965483665467 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.685718160867691 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6068528890609741 (0.003s), train acc: 0.616\n",
      "\n",
      "-----Epoch 420/500-----\n",
      "Batch 10/52, loss: 0.6467938005924225 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6718632340431213 (0.010s), train acc: 0.594\n",
      "Batch 30/52, loss: 0.6543561577796936 (0.010s), train acc: 0.604\n",
      "Batch 40/52, loss: 0.6334407448768615 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6588990867137909 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.5863973200321198 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 421/500-----\n",
      "Batch 10/52, loss: 0.6576169729232788 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6386569559574127 (0.010s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.6548785030841827 (0.010s), train acc: 0.605\n",
      "Batch 40/52, loss: 0.6463671624660492 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6473677515983581 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6775717437267303 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 422/500-----\n",
      "Batch 10/52, loss: 0.6595911622047425 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6393448948860169 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6331393480300903 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6515322923660278 (0.010s), train acc: 0.621\n",
      "Batch 50/52, loss: 0.6542648255825043 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.7040368318557739 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 423/500-----\n",
      "Batch 10/52, loss: 0.6577115535736084 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6595244348049164 (0.010s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.6452721178531646 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6449074506759643 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6447877287864685 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6453942358493805 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 424/500-----\n",
      "Batch 10/52, loss: 0.6660299837589264 (0.009s), train acc: 0.591\n",
      "Batch 20/52, loss: 0.6440003633499145 (0.010s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6307614624500275 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6669642984867096 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6457102596759796 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6193882822990417 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 425/500-----\n",
      "Batch 10/52, loss: 0.6784609258174896 (0.009s), train acc: 0.550\n",
      "Batch 20/52, loss: 0.6127976894378662 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6596782028675079 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6537247478961945 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6440458655357361 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6700415313243866 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 426/500-----\n",
      "Batch 10/52, loss: 0.6482248187065125 (0.009s), train acc: 0.622\n",
      "Batch 20/52, loss: 0.654885321855545 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6571453511714935 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6358153462409973 (0.010s), train acc: 0.621\n",
      "Batch 50/52, loss: 0.6579407095909119 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.6258075535297394 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 427/500-----\n",
      "Batch 10/52, loss: 0.6485665321350098 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6505621314048767 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6337256133556366 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.67282634973526 (0.010s), train acc: 0.620\n",
      "Batch 50/52, loss: 0.6334791541099548 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.7131238579750061 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 428/500-----\n",
      "Batch 10/52, loss: 0.6479274511337281 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6563413560390472 (0.010s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.6514034509658814 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6431966602802277 (0.010s), train acc: 0.618\n",
      "Batch 50/52, loss: 0.6642686247825622 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6385464668273926 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 429/500-----\n",
      "Batch 10/52, loss: 0.6498501241207123 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6378260672092437 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6582901298999786 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.653014624118805 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6563423335552215 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6111503541469574 (0.003s), train acc: 0.607\n",
      "\n",
      "-----Epoch 430/500-----\n",
      "Batch 10/52, loss: 0.6396316170692444 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.683962345123291 (0.010s), train acc: 0.592\n",
      "Batch 30/52, loss: 0.6415354073047638 (0.010s), train acc: 0.607\n",
      "Batch 40/52, loss: 0.6512868583202363 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6420701622962952 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6062606573104858 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 431/500-----\n",
      "Batch 10/52, loss: 0.6623775422573089 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6497594714164734 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.642689311504364 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6524640619754791 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6477898955345154 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.621107280254364 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 432/500-----\n",
      "Batch 10/52, loss: 0.6665920734405517 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.6383057355880737 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6200015902519226 (0.010s), train acc: 0.626\n",
      "Batch 40/52, loss: 0.6818794548511505 (0.010s), train acc: 0.604\n",
      "Batch 50/52, loss: 0.6603644251823425 (0.010s), train acc: 0.603\n",
      "Batch 52/52, loss: 0.5954590141773224 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 433/500-----\n",
      "Batch 10/52, loss: 0.6529629766941071 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6453332602977753 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6573064982891083 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6694224596023559 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6386659204959869 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6018352508544922 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 434/500-----\n",
      "Batch 10/52, loss: 0.6369279563426972 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6524718880653382 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6472533345222473 (0.010s), train acc: 0.625\n",
      "Batch 40/52, loss: 0.6680408358573914 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6400298118591309 (0.010s), train acc: 0.620\n",
      "Batch 52/52, loss: 0.6790830492973328 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 435/500-----\n",
      "Batch 10/52, loss: 0.6530978322029114 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.655725646018982 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6304582536220551 (0.010s), train acc: 0.609\n",
      "Batch 40/52, loss: 0.6440093874931335 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6722426176071167 (0.010s), train acc: 0.604\n",
      "Batch 52/52, loss: 0.6043029129505157 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 436/500-----\n",
      "Batch 10/52, loss: 0.6686745166778565 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6468072474002838 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6410141229629517 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.655570262670517 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6363898575305938 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6579372584819794 (0.003s), train acc: 0.619\n",
      "\n",
      "-----Epoch 437/500-----\n",
      "Batch 10/52, loss: 0.6452131569385529 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6399057507514954 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6809725224971771 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6478432714939117 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6259676516056061 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.7163026332855225 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 438/500-----\n",
      "Batch 10/52, loss: 0.667796528339386 (0.009s), train acc: 0.575\n",
      "Batch 20/52, loss: 0.6389273762702942 (0.010s), train acc: 0.619\n",
      "Batch 30/52, loss: 0.6374844133853912 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6531571507453918 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6592737674713135 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6292392611503601 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 439/500-----\n",
      "Batch 10/52, loss: 0.6564935266971588 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6616747915744782 (0.010s), train acc: 0.602\n",
      "Batch 30/52, loss: 0.6296709448099136 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6497013211250305 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6570891380310059 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6247402131557465 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 440/500-----\n",
      "Batch 10/52, loss: 0.6645710706710816 (0.009s), train acc: 0.578\n",
      "Batch 20/52, loss: 0.6493363380432129 (0.010s), train acc: 0.591\n",
      "Batch 30/52, loss: 0.6431084990501403 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6524939060211181 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6437538862228394 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6208392977714539 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 441/500-----\n",
      "Batch 10/52, loss: 0.6512372076511384 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6479288101196289 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6638129770755767 (0.010s), train acc: 0.606\n",
      "Batch 40/52, loss: 0.6473987460136413 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6380652844905853 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6554998159408569 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 442/500-----\n",
      "Batch 10/52, loss: 0.6713134288787842 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.641897052526474 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6426917016506195 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6355990469455719 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6512273728847504 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.674964964389801 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 443/500-----\n",
      "Batch 10/52, loss: 0.6475272297859191 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6609471261501312 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6473664104938507 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6361158013343811 (0.010s), train acc: 0.618\n",
      "Batch 50/52, loss: 0.6645161926746368 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.6293164789676666 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 444/500-----\n",
      "Batch 10/52, loss: 0.6378269910812377 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6447492241859436 (0.010s), train acc: 0.633\n",
      "Batch 30/52, loss: 0.652039247751236 (0.010s), train acc: 0.628\n",
      "Batch 40/52, loss: 0.6647121787071228 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6526035726070404 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6345645487308502 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 445/500-----\n",
      "Batch 10/52, loss: 0.65762500166893 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6610002458095551 (0.010s), train acc: 0.588\n",
      "Batch 30/52, loss: 0.632995992898941 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6500608742237091 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6448433995246887 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.6725971698760986 (0.003s), train acc: 0.616\n",
      "\n",
      "-----Epoch 446/500-----\n",
      "Batch 10/52, loss: 0.6378130316734314 (0.009s), train acc: 0.647\n",
      "Batch 20/52, loss: 0.6550812900066376 (0.010s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.6477390766143799 (0.010s), train acc: 0.620\n",
      "Batch 40/52, loss: 0.6601270496845245 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6409390270709991 (0.010s), train acc: 0.621\n",
      "Batch 52/52, loss: 0.6792866587638855 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 447/500-----\n",
      "Batch 10/52, loss: 0.6155538380146026 (0.009s), train acc: 0.656\n",
      "Batch 20/52, loss: 0.6467575192451477 (0.010s), train acc: 0.642\n",
      "Batch 30/52, loss: 0.7001200437545776 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6233169734477997 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6597235321998596 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6841737627983093 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 448/500-----\n",
      "Batch 10/52, loss: 0.6327243030071259 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6520010590553283 (0.010s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6678346455097198 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.6505990505218506 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.654237812757492 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6082780659198761 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 449/500-----\n",
      "Batch 10/52, loss: 0.6531764924526214 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6451210796833038 (0.009s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6510295569896698 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6573107898235321 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6531535863876343 (0.010s), train acc: 0.610\n",
      "Batch 52/52, loss: 0.5859236419200897 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 450/500-----\n",
      "Batch 10/52, loss: 0.6423350751399994 (0.009s), train acc: 0.619\n",
      "Batch 20/52, loss: 0.6581397354602814 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6361580312252044 (0.010s), train acc: 0.619\n",
      "Batch 40/52, loss: 0.6415172755718231 (0.010s), train acc: 0.625\n",
      "Batch 50/52, loss: 0.6789463579654693 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.6024489402770996 (0.003s), train acc: 0.616\n",
      "\n",
      "-----Epoch 451/500-----\n",
      "Batch 10/52, loss: 0.6561166226863862 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6593632817268371 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6382772505283356 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6474638342857361 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6465225040912628 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.6507943272590637 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 452/500-----\n",
      "Batch 10/52, loss: 0.6456452250480652 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6488507091999054 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6681606650352478 (0.010s), train acc: 0.616\n",
      "Batch 40/52, loss: 0.6474591493606567 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.636772608757019 (0.010s), train acc: 0.619\n",
      "Batch 52/52, loss: 0.6510726809501648 (0.003s), train acc: 0.620\n",
      "\n",
      "-----Epoch 453/500-----\n",
      "Batch 10/52, loss: 0.6444066524505615 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6662496387958526 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6603835701942444 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6305216491222382 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6453665375709534 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6489412188529968 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 454/500-----\n",
      "Batch 10/52, loss: 0.6270349025726318 (0.009s), train acc: 0.678\n",
      "Batch 20/52, loss: 0.669814544916153 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6537398576736451 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6419570684432984 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6573233902454376 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6441155970096588 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 455/500-----\n",
      "Batch 10/52, loss: 0.6511336743831635 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6658745527267456 (0.010s), train acc: 0.606\n",
      "Batch 30/52, loss: 0.6517744183540344 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6423376977443696 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.647035276889801 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.616840273141861 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 456/500-----\n",
      "Batch 10/52, loss: 0.633037143945694 (0.009s), train acc: 0.644\n",
      "Batch 20/52, loss: 0.6591968655586242 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6691348433494568 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.656079888343811 (0.010s), train acc: 0.611\n",
      "Batch 50/52, loss: 0.6264793276786804 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.6864734292030334 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 457/500-----\n",
      "Batch 10/52, loss: 0.6550178766250611 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6332048892974853 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6527279376983642 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6511736154556275 (0.010s), train acc: 0.615\n",
      "Batch 50/52, loss: 0.6575424432754516 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6525940597057343 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 458/500-----\n",
      "Batch 10/52, loss: 0.6318743348121643 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6709903299808502 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6388147294521331 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6582324624061584 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6461944341659546 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6775293350219727 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 459/500-----\n",
      "Batch 10/52, loss: 0.6571575939655304 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6490529417991638 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6559297442436218 (0.010s), train acc: 0.594\n",
      "Batch 40/52, loss: 0.6373395979404449 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.6413915514945984 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6932316422462463 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 460/500-----\n",
      "Batch 10/52, loss: 0.6279932737350464 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.663299947977066 (0.010s), train acc: 0.611\n",
      "Batch 30/52, loss: 0.6540519893169403 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6587926089763642 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6349122405052186 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.6896551251411438 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 461/500-----\n",
      "Batch 10/52, loss: 0.6672918200492859 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6355341732501983 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6245918333530426 (0.010s), train acc: 0.625\n",
      "Batch 40/52, loss: 0.661686760187149 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6634882748126983 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.630668044090271 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 462/500-----\n",
      "Batch 10/52, loss: 0.6580119132995605 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6500576615333558 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6541489481925964 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6314292073249816 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.654343044757843 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6596593260765076 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 463/500-----\n",
      "Batch 10/52, loss: 0.6480159938335419 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6541992664337158 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6490214109420777 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6377746820449829 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.6520524561405182 (0.010s), train acc: 0.621\n",
      "Batch 52/52, loss: 0.6821224391460419 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 464/500-----\n",
      "Batch 10/52, loss: 0.6635886311531067 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.628999400138855 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.6574358642101288 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6427695870399475 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6532168507575988 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.6483847498893738 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 465/500-----\n",
      "Batch 10/52, loss: 0.6161031603813172 (0.009s), train acc: 0.681\n",
      "Batch 20/52, loss: 0.6622084856033326 (0.010s), train acc: 0.633\n",
      "Batch 30/52, loss: 0.6390340268611908 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6764333486557007 (0.010s), train acc: 0.608\n",
      "Batch 50/52, loss: 0.6541044354438782 (0.010s), train acc: 0.608\n",
      "Batch 52/52, loss: 0.6735284924507141 (0.003s), train acc: 0.608\n",
      "\n",
      "-----Epoch 466/500-----\n",
      "Batch 10/52, loss: 0.659235554933548 (0.009s), train acc: 0.581\n",
      "Batch 20/52, loss: 0.6525361120700837 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.646891039609909 (0.010s), train acc: 0.598\n",
      "Batch 40/52, loss: 0.6557034552097321 (0.010s), train acc: 0.602\n",
      "Batch 50/52, loss: 0.6405734598636628 (0.010s), train acc: 0.612\n",
      "Batch 52/52, loss: 0.631089985370636 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 467/500-----\n",
      "Batch 10/52, loss: 0.6364482343196869 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6882439196109772 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6370383441448212 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6441822528839112 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6386700332164764 (0.010s), train acc: 0.622\n",
      "Batch 52/52, loss: 0.6637791693210602 (0.003s), train acc: 0.621\n",
      "\n",
      "-----Epoch 468/500-----\n",
      "Batch 10/52, loss: 0.6654908239841462 (0.009s), train acc: 0.603\n",
      "Batch 20/52, loss: 0.6426156222820282 (0.010s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6349567234516144 (0.010s), train acc: 0.626\n",
      "Batch 40/52, loss: 0.6559630990028381 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6546414375305176 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6134196519851685 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 469/500-----\n",
      "Batch 10/52, loss: 0.6664785444736481 (0.009s), train acc: 0.584\n",
      "Batch 20/52, loss: 0.66759153008461 (0.010s), train acc: 0.588\n",
      "Batch 30/52, loss: 0.6391427636146545 (0.010s), train acc: 0.599\n",
      "Batch 40/52, loss: 0.6327268421649933 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6405423283576965 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.651553601026535 (0.003s), train acc: 0.613\n",
      "\n",
      "-----Epoch 470/500-----\n",
      "Batch 10/52, loss: 0.6450471878051758 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6615352869033814 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6538459479808807 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6596326410770417 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6214141130447388 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.6718518435955048 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 471/500-----\n",
      "Batch 10/52, loss: 0.6316073536872864 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6835435092449188 (0.010s), train acc: 0.598\n",
      "Batch 30/52, loss: 0.6498169541358948 (0.010s), train acc: 0.596\n",
      "Batch 40/52, loss: 0.6445802509784698 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6416729509830474 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6408000588417053 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 472/500-----\n",
      "Batch 10/52, loss: 0.6511038184165955 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6541667699813842 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.6347150266170501 (0.010s), train acc: 0.624\n",
      "Batch 40/52, loss: 0.6509748995304108 (0.010s), train acc: 0.622\n",
      "Batch 50/52, loss: 0.6545702755451203 (0.010s), train acc: 0.619\n",
      "Batch 52/52, loss: 0.6419980525970459 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 473/500-----\n",
      "Batch 10/52, loss: 0.6303770303726196 (0.009s), train acc: 0.650\n",
      "Batch 20/52, loss: 0.6676263689994812 (0.010s), train acc: 0.630\n",
      "Batch 30/52, loss: 0.657963615655899 (0.010s), train acc: 0.623\n",
      "Batch 40/52, loss: 0.6454630374908448 (0.010s), train acc: 0.625\n",
      "Batch 50/52, loss: 0.6479530036449432 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6324608325958252 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 474/500-----\n",
      "Batch 10/52, loss: 0.6568638622760773 (0.009s), train acc: 0.597\n",
      "Batch 20/52, loss: 0.6412155091762543 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6519862711429596 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6406842827796936 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6598906219005585 (0.010s), train acc: 0.614\n",
      "Batch 52/52, loss: 0.6287631392478943 (0.003s), train acc: 0.616\n",
      "\n",
      "-----Epoch 475/500-----\n",
      "Batch 10/52, loss: 0.6448193311691284 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6490212440490722 (0.010s), train acc: 0.616\n",
      "Batch 30/52, loss: 0.6284205913543701 (0.010s), train acc: 0.629\n",
      "Batch 40/52, loss: 0.6622324407100677 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.6691556394100189 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6334014236927032 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 476/500-----\n",
      "Batch 10/52, loss: 0.6248828113079071 (0.009s), train acc: 0.641\n",
      "Batch 20/52, loss: 0.6671920359134674 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6432068824768067 (0.010s), train acc: 0.620\n",
      "Batch 40/52, loss: 0.6328336000442505 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.6771878838539124 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.653168261051178 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 477/500-----\n",
      "Batch 10/52, loss: 0.6538763642311096 (0.009s), train acc: 0.616\n",
      "Batch 20/52, loss: 0.6351220786571503 (0.010s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6677932143211365 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6448476314544678 (0.010s), train acc: 0.609\n",
      "Batch 50/52, loss: 0.6442500650882721 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.6515624225139618 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 478/500-----\n",
      "Batch 10/52, loss: 0.6603090107440949 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6590395629405975 (0.010s), train acc: 0.586\n",
      "Batch 30/52, loss: 0.6624351918697358 (0.010s), train acc: 0.594\n",
      "Batch 40/52, loss: 0.6155364871025085 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.6713748514652252 (0.010s), train acc: 0.606\n",
      "Batch 52/52, loss: 0.5736035406589508 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 479/500-----\n",
      "Batch 10/52, loss: 0.656464946269989 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6490201890468598 (0.010s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.6461193323135376 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6370951235294342 (0.010s), train acc: 0.618\n",
      "Batch 50/52, loss: 0.6599642634391785 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6384493410587311 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 480/500-----\n",
      "Batch 10/52, loss: 0.6411677062511444 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6533783972263336 (0.010s), train acc: 0.633\n",
      "Batch 30/52, loss: 0.6533114016056061 (0.010s), train acc: 0.624\n",
      "Batch 40/52, loss: 0.6525900423526764 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6444685161113739 (0.010s), train acc: 0.621\n",
      "Batch 52/52, loss: 0.6603029072284698 (0.003s), train acc: 0.620\n",
      "\n",
      "-----Epoch 481/500-----\n",
      "Batch 10/52, loss: 0.62724289894104 (0.009s), train acc: 0.656\n",
      "Batch 20/52, loss: 0.6725482225418091 (0.010s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.683159065246582 (0.010s), train acc: 0.600\n",
      "Batch 40/52, loss: 0.6301381409168243 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6353633880615235 (0.010s), train acc: 0.621\n",
      "Batch 52/52, loss: 0.660076379776001 (0.003s), train acc: 0.620\n",
      "\n",
      "-----Epoch 482/500-----\n",
      "Batch 10/52, loss: 0.633920282125473 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6413283765316009 (0.010s), train acc: 0.631\n",
      "Batch 30/52, loss: 0.6712401926517486 (0.010s), train acc: 0.617\n",
      "Batch 40/52, loss: 0.6609832406044006 (0.010s), train acc: 0.610\n",
      "Batch 50/52, loss: 0.635608434677124 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.665442943572998 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 483/500-----\n",
      "Batch 10/52, loss: 0.6573463261127472 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.64787158370018 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6557432413101196 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6406373023986817 (0.010s), train acc: 0.613\n",
      "Batch 50/52, loss: 0.6494778692722321 (0.010s), train acc: 0.616\n",
      "Batch 52/52, loss: 0.6243313252925873 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 484/500-----\n",
      "Batch 10/52, loss: 0.6445581197738648 (0.009s), train acc: 0.600\n",
      "Batch 20/52, loss: 0.6581912279129029 (0.010s), train acc: 0.614\n",
      "Batch 30/52, loss: 0.640600323677063 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.663007402420044 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6452378690242767 (0.010s), train acc: 0.619\n",
      "Batch 52/52, loss: 0.6228258311748505 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 485/500-----\n",
      "Batch 10/52, loss: 0.6536777019500732 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6304680407047272 (0.010s), train acc: 0.647\n",
      "Batch 30/52, loss: 0.6455055177211761 (0.010s), train acc: 0.632\n",
      "Batch 40/52, loss: 0.6503127455711365 (0.010s), train acc: 0.626\n",
      "Batch 50/52, loss: 0.6479776680469513 (0.010s), train acc: 0.617\n",
      "Batch 52/52, loss: 0.7420311272144318 (0.003s), train acc: 0.614\n",
      "\n",
      "-----Epoch 486/500-----\n",
      "Batch 10/52, loss: 0.6573374152183533 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6481807827949524 (0.010s), train acc: 0.622\n",
      "Batch 30/52, loss: 0.6440457999706268 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6447103559970856 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6582087993621826 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6510065793991089 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 487/500-----\n",
      "Batch 10/52, loss: 0.6498587250709533 (0.009s), train acc: 0.613\n",
      "Batch 20/52, loss: 0.6337025701999665 (0.010s), train acc: 0.627\n",
      "Batch 30/52, loss: 0.6597316682338714 (0.010s), train acc: 0.622\n",
      "Batch 40/52, loss: 0.6602775454521179 (0.010s), train acc: 0.620\n",
      "Batch 50/52, loss: 0.6449208915233612 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6616882681846619 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 488/500-----\n",
      "Batch 10/52, loss: 0.6419129192829132 (0.009s), train acc: 0.634\n",
      "Batch 20/52, loss: 0.6550531089305878 (0.010s), train acc: 0.628\n",
      "Batch 30/52, loss: 0.6245513737201691 (0.010s), train acc: 0.640\n",
      "Batch 40/52, loss: 0.6639713525772095 (0.010s), train acc: 0.623\n",
      "Batch 50/52, loss: 0.6563650250434876 (0.010s), train acc: 0.619\n",
      "Batch 52/52, loss: 0.6731977760791779 (0.003s), train acc: 0.618\n",
      "\n",
      "-----Epoch 489/500-----\n",
      "Batch 10/52, loss: 0.6521032273769378 (0.009s), train acc: 0.631\n",
      "Batch 20/52, loss: 0.6491352558135987 (0.010s), train acc: 0.617\n",
      "Batch 30/52, loss: 0.6653058350086212 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6432168662548066 (0.010s), train acc: 0.606\n",
      "Batch 50/52, loss: 0.6429237186908722 (0.010s), train acc: 0.609\n",
      "Batch 52/52, loss: 0.6341884434223175 (0.003s), train acc: 0.612\n",
      "\n",
      "-----Epoch 490/500-----\n",
      "Batch 10/52, loss: 0.6673829734325409 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6265808522701264 (0.010s), train acc: 0.620\n",
      "Batch 30/52, loss: 0.6334290087223053 (0.010s), train acc: 0.615\n",
      "Batch 40/52, loss: 0.6483524382114411 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6612008452415467 (0.010s), train acc: 0.620\n",
      "Batch 52/52, loss: 0.7019163072109222 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 491/500-----\n",
      "Batch 10/52, loss: 0.6320956826210022 (0.009s), train acc: 0.650\n",
      "Batch 20/52, loss: 0.6536322176456452 (0.010s), train acc: 0.637\n",
      "Batch 30/52, loss: 0.6534412026405334 (0.010s), train acc: 0.624\n",
      "Batch 40/52, loss: 0.658257269859314 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.6478197336196899 (0.010s), train acc: 0.618\n",
      "Batch 52/52, loss: 0.6424728333950043 (0.003s), train acc: 0.617\n",
      "\n",
      "-----Epoch 492/500-----\n",
      "Batch 10/52, loss: 0.6435178697109223 (0.009s), train acc: 0.625\n",
      "Batch 20/52, loss: 0.6644055783748627 (0.010s), train acc: 0.600\n",
      "Batch 30/52, loss: 0.6518174409866333 (0.010s), train acc: 0.603\n",
      "Batch 40/52, loss: 0.6464566648006439 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.642286217212677 (0.010s), train acc: 0.619\n",
      "Batch 52/52, loss: 0.6568571925163269 (0.003s), train acc: 0.619\n",
      "\n",
      "-----Epoch 493/500-----\n",
      "Batch 10/52, loss: 0.6732948541641235 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.6507066309452056 (0.010s), train acc: 0.595\n",
      "Batch 30/52, loss: 0.6279276013374329 (0.010s), train acc: 0.614\n",
      "Batch 40/52, loss: 0.6524711132049561 (0.010s), train acc: 0.607\n",
      "Batch 50/52, loss: 0.6505611896514892 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6120078265666962 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 494/500-----\n",
      "Batch 10/52, loss: 0.6610390305519104 (0.009s), train acc: 0.588\n",
      "Batch 20/52, loss: 0.63311567902565 (0.010s), train acc: 0.609\n",
      "Batch 30/52, loss: 0.6390224158763885 (0.010s), train acc: 0.610\n",
      "Batch 40/52, loss: 0.6577595055103302 (0.010s), train acc: 0.616\n",
      "Batch 50/52, loss: 0.6581012785434723 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6224814653396606 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 495/500-----\n",
      "Batch 10/52, loss: 0.6640389621257782 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6338211297988892 (0.010s), train acc: 0.613\n",
      "Batch 30/52, loss: 0.629205971956253 (0.010s), train acc: 0.632\n",
      "Batch 40/52, loss: 0.666640168428421 (0.010s), train acc: 0.619\n",
      "Batch 50/52, loss: 0.647279018163681 (0.010s), train acc: 0.623\n",
      "Batch 52/52, loss: 0.6973805725574493 (0.003s), train acc: 0.621\n",
      "\n",
      "-----Epoch 496/500-----\n",
      "Batch 10/52, loss: 0.6635765612125397 (0.009s), train acc: 0.606\n",
      "Batch 20/52, loss: 0.6587032377719879 (0.010s), train acc: 0.597\n",
      "Batch 30/52, loss: 0.6158267498016358 (0.010s), train acc: 0.621\n",
      "Batch 40/52, loss: 0.6478201985359192 (0.010s), train acc: 0.621\n",
      "Batch 50/52, loss: 0.6511691868305206 (0.010s), train acc: 0.624\n",
      "Batch 52/52, loss: 0.699284553527832 (0.003s), train acc: 0.620\n",
      "\n",
      "-----Epoch 497/500-----\n",
      "Batch 10/52, loss: 0.653161746263504 (0.009s), train acc: 0.609\n",
      "Batch 20/52, loss: 0.6618309617042542 (0.010s), train acc: 0.603\n",
      "Batch 30/52, loss: 0.6330985069274903 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6353035390377044 (0.010s), train acc: 0.622\n",
      "Batch 50/52, loss: 0.6691108822822571 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.6412016153335571 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 498/500-----\n",
      "Batch 10/52, loss: 0.6448959827423095 (0.009s), train acc: 0.628\n",
      "Batch 20/52, loss: 0.6497017621994019 (0.010s), train acc: 0.625\n",
      "Batch 30/52, loss: 0.6540835916996002 (0.010s), train acc: 0.613\n",
      "Batch 40/52, loss: 0.6429623603820801 (0.010s), train acc: 0.617\n",
      "Batch 50/52, loss: 0.6622591435909271 (0.010s), train acc: 0.613\n",
      "Batch 52/52, loss: 0.6168969571590424 (0.003s), train acc: 0.615\n",
      "\n",
      "-----Epoch 499/500-----\n",
      "Batch 10/52, loss: 0.6484610199928283 (0.009s), train acc: 0.594\n",
      "Batch 20/52, loss: 0.6716564357280731 (0.010s), train acc: 0.605\n",
      "Batch 30/52, loss: 0.6389604330062866 (0.010s), train acc: 0.608\n",
      "Batch 40/52, loss: 0.6479686617851257 (0.010s), train acc: 0.605\n",
      "Batch 50/52, loss: 0.6369910717010498 (0.010s), train acc: 0.611\n",
      "Batch 52/52, loss: 0.679962694644928 (0.003s), train acc: 0.611\n",
      "\n",
      "-----Epoch 500/500-----\n",
      "Batch 10/52, loss: 0.6614671945571899 (0.009s), train acc: 0.572\n",
      "Batch 20/52, loss: 0.6520935773849488 (0.010s), train acc: 0.608\n",
      "Batch 30/52, loss: 0.6524087190628052 (0.010s), train acc: 0.611\n",
      "Batch 40/52, loss: 0.6335477828979492 (0.010s), train acc: 0.614\n",
      "Batch 50/52, loss: 0.645566874742508 (0.010s), train acc: 0.615\n",
      "Batch 52/52, loss: 0.6664061546325684 (0.003s), train acc: 0.615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenericAttackModel(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_x, attack_y = create_attack_dataset(s_model, s_data)\n",
    "att_dataset = GenericDataset(attack_x, attack_y)\n",
    "lr = 0.001\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "att_model = GenericAttackModel(num_feat=6).to(DEVICE)\n",
    "optimizer = optim.Adam(att_model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(att_model, optimizer, att_dataset, loss_fn, epochs, batch_size, device=DEVICE)\n",
    "att_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7821bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.608538785327721"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_model.eval()\n",
    "t_x, t_y = create_attack_dataset(t_model, t_data)\n",
    "\n",
    "pred = att_model(t_x.to(DEVICE)).detach().cpu()\n",
    "get_accuracy(pred, t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "111132bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4675480769230769"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.where(np.argmax(t_y, axis=1) == 0)[0]\n",
    "get_accuracy(pred[idx], t_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "056c7e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7496991576413959"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.where(np.argmax(t_y, axis=1) == 1)[0]\n",
    "get_accuracy(pred[idx], t_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edf812f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b78c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(list(range(6)), t_x[t_y.argmin(dim=1).to(bool)].mean(dim=0), alpha=0.4)\n",
    "plt.bar(list(range(6)), t_x[t_y.argmax(dim=1).to(bool)].mean(dim=0), alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34128b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(list(range(6)), attack_x[attack_y.argmin(dim=1).to(bool)].mean(dim=0), alpha=0.4)\n",
    "plt.bar(list(range(6)), attack_x[attack_y.argmax(dim=1).to(bool)].mean(dim=0), alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f047f490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6288, -1.5556,  0.2882,  5.3236, -1.2754,  0.3285]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_zero_hop(t_model, s_data.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e961b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0381, -2.7032, -1.2279,  4.5965, -1.2803, -1.0144]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_zero_hop(s_model, s_data.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e750ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6288, -1.5556,  0.2882,  5.3236, -1.2754,  0.3285]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_zero_hop(t_model, s_data.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73885141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327, 6], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9aca6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.7845e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0502, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2569, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.7717e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.8514e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.8504e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.4466e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.6592e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.6951e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.5780e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.9902e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.3584e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.1257e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.4908e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.4417e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.5056e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0130, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.5978e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1047e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.9085e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.9085e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.6682e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.2864e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0130, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.8359e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.0544e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.4705e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.5614e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5301e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.5811e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4444, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.4544e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.1410e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.6480e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9633e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9066e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.7931e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.7319e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6569, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.4404e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2306e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9521e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.6446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.3884e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.4044e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.7555e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.2382e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.5958e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9418e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.4619e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.5398e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.5845e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.2786e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.2181e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1569, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.1738e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5916e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1413, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9297e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.7093e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.4818e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.1372e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.9371e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.7057e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.8333e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1976e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.3720e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.7320e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.4555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.3573e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.3696e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9716e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1447e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8658e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.0389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.7163e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.8754e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2437e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.3962e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0230e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.2494e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.8077e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.6352e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.9273e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9.0461e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.6783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.4214e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.6163e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1788, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.9469e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.0923e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8857e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2914e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.5122e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.8333e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.2119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.9795e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1009e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1872e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.6460e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.3081e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.7566e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.5860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.7568e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.6685e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.0779e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.1478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.9002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.2285e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.3036e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.0544e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.6556e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.4634e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(5.4286e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.3212e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.6584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.8776e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.1256e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.0976e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.2912e-05, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.2664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.3993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.1529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.7414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.8773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.5636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for x in s_data.x:\n",
    "    t_pred = query_zero_hop(t_model, x).softmax(dim=1)[0].log()\n",
    "    s_pred = query_zero_hop(s_model, x).softmax(dim=1)[0].log()\n",
    "#     print(s_pred.softmax(dim=1)[0])\n",
    "#     print(s_pred)\n",
    "    print(F.kl_div(t_pred, s_pred, log_target=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae19b2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2024,  0.2694],\n",
       "        [-0.2251,  0.2920],\n",
       "        [ 1.3049, -1.1046],\n",
       "        ...,\n",
       "        [-0.2102,  0.2770],\n",
       "        [-0.1613,  0.2264],\n",
       "        [ 0.3046, -0.1908]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_model(t_x.to(DEVICE)).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52d5a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3306,  6.5233,  0.3790, -5.2127, -0.0700, -1.2676]]) 0\n",
      "tensor([[-4.2230, -2.8926, -2.3202, -0.6136, -1.0923,  9.0295]]) 1\n",
      "tensor([[-2.4567,  1.2455, -0.1968, -0.6522, -0.1249,  1.4294]]) 0\n",
      "tensor([[-0.1942,  6.4990,  1.1117, -1.6230, -1.8155, -2.8304]]) 1\n",
      "tensor([[-0.1924,  1.3156,  5.0560, -0.7706, -1.6254, -0.9846]]) 0\n",
      "tensor([[ 0.0217, -4.1570, -5.0231,  3.4691,  3.9772, -0.7018]]) 0\n",
      "tensor([[ 3.1671,  2.4182,  5.5301, -2.0429, -3.6690, -4.2726]]) 0\n",
      "tensor([[-0.0350,  8.1872,  1.3754, -1.5269, -2.2231, -3.7270]]) 0\n",
      "tensor([[-1.5291,  1.2116,  4.3174,  1.7053, -1.9073,  0.0722]]) 0\n",
      "tensor([[-1.4291, -0.4917,  7.3205,  2.4674, -1.2470, -2.4339]]) 1\n",
      "tensor([[-2.8103,  1.2664, -0.1583,  0.6598,  2.2892, -0.2350]]) 1\n",
      "tensor([[ 3.9910,  2.6695, -1.2225, -0.9036, -1.0643, -5.1664]]) 1\n",
      "tensor([[-2.5793, -2.8477, -2.4662, -0.0145, -0.1930,  7.0797]]) 0\n",
      "tensor([[-0.2909,  1.8908,  3.4079,  2.4875, -1.9633, -2.7896]]) 0\n",
      "tensor([[-1.2980, -0.4327, -1.5348, -0.6909,  0.2168,  2.6888]]) 0\n",
      "tensor([[-3.8837, -1.2318, -2.6317,  1.5830,  1.0158,  3.3601]]) 1\n",
      "tensor([[-0.6373,  0.6990,  9.7052, -0.8129, -1.8816, -1.3060]]) 1\n",
      "tensor([[-2.2604, -0.6313, -1.2923, -0.2364,  3.9360, -1.7078]]) 1\n",
      "tensor([[-0.6273,  0.8134,  5.8346, -1.2180, -2.5335, -0.2338]]) 0\n",
      "tensor([[ 0.3303,  4.9785, -0.2054, -3.1441, -2.1805, -0.5417]]) 0\n",
      "tensor([[-2.6696, -0.5271,  4.2547, -0.0501, -0.7906,  1.8542]]) 1\n",
      "tensor([[-0.8852,  0.2347,  2.1396,  4.1488, -1.6985, -1.7907]]) 1\n",
      "tensor([[-1.0994, -2.1530,  1.1726,  7.0415, -2.1071, -0.8076]]) 1\n",
      "tensor([[ 2.9017,  1.3137, -2.9425, -1.7335, -0.0043, -3.2534]]) 0\n",
      "tensor([[-1.6952,  6.8993,  0.6159, -0.5266, -1.7602, -1.6556]]) 1\n",
      "tensor([[-0.7846, -1.0633, -2.1648,  0.7324, -0.5326,  3.3775]]) 0\n",
      "tensor([[-0.0950,  7.3928, -0.1430, -3.3569,  0.3334, -5.2221]]) 1\n",
      "tensor([[-1.6911,  0.2099,  1.1900,  0.4809, -2.6657,  3.1879]]) 0\n",
      "tensor([[-0.8612, -0.4620,  3.1344,  3.7515, -1.5778, -1.5486]]) 0\n",
      "tensor([[-0.7403, -1.6941,  0.4563,  3.6927, -0.6292, -0.0518]]) 1\n",
      "tensor([[-2.3156, -2.6346, -0.6798,  1.6171, -0.6185,  5.5708]]) 1\n",
      "tensor([[-2.1989, -3.5558,  3.1081,  2.8446, -0.5022,  2.0143]]) 0\n",
      "tensor([[ 0.0107,  6.7639, -1.0207,  1.6292, -0.8672, -4.3077]]) 1\n",
      "tensor([[-4.2137, -1.5683, -1.2522, -1.3215,  8.1208, -1.1899]]) 1\n",
      "tensor([[ 0.1806,  1.4291,  0.5945, -0.5728, -0.1048, -1.2477]]) 0\n",
      "tensor([[ 0.4431,  1.2582, -3.4616,  0.1918,  3.0783, -5.1205]]) 0\n",
      "tensor([[-2.0245, -0.4142,  5.7696,  0.9162, -1.7589,  1.4617]]) 0\n",
      "tensor([[ 0.0508,  5.8034, -0.4082, -0.7856, -0.4031, -3.1344]]) 0\n",
      "tensor([[ 1.7559,  0.5349,  8.2260, -0.7561, -1.4046, -5.2917]]) 0\n",
      "tensor([[-1.7670,  1.2731, -2.0104, -2.0164,  4.0044, -2.5377]]) 0\n",
      "tensor([[-2.6732, -1.9034,  0.7456, -0.0426, -0.9110,  5.1618]]) 1\n",
      "tensor([[-3.5739, -2.0246, -1.6350, -1.7582, -1.0656,  7.9631]]) 0\n",
      "tensor([[-1.8899, -0.8813, -1.5519,  2.8484,  1.5213, -0.6226]]) 0\n",
      "tensor([[ 0.2736, -1.9029, -4.7009, -1.5146,  6.6882, -4.6538]]) 1\n",
      "tensor([[-1.7415, -0.9536,  5.2016, -0.4327, -0.3527,  0.8290]]) 0\n",
      "tensor([[-0.0545,  5.8631, -3.2905,  0.4355,  1.2810, -4.9242]]) 0\n",
      "tensor([[-0.0703,  6.6580, -1.8894, -1.9751, -0.1214, -2.0752]]) 0\n",
      "tensor([[ 2.6030, -0.8542, -2.0650,  0.0277, -1.6611, -0.7707]]) 1\n",
      "tensor([[ 1.1761,  4.3281, -1.6795, -3.0946, -1.4787, -3.0701]]) 0\n",
      "tensor([[-1.2649, -1.0263,  3.9617,  2.8345, -0.9543, -0.5106]]) 0\n",
      "tensor([[-0.3316,  9.4481,  0.8359, -3.2342, -1.6247, -3.8813]]) 0\n",
      "tensor([[-0.1297,  5.9283, -0.7054,  0.0287, -2.5731, -1.3810]]) 1\n",
      "tensor([[-3.2215,  0.4874, -3.9282, -1.6233,  9.2452, -5.5993]]) 0\n",
      "tensor([[-0.1200, -2.7693, -0.8849,  0.7268,  3.0312, -1.6547]]) 0\n",
      "tensor([[ 0.4374,  8.6717,  0.7442, -2.3616, -2.1306, -4.2613]]) 0\n",
      "tensor([[-0.8224,  0.9018, -2.8558, -0.6756,  5.5143, -4.4343]]) 1\n",
      "tensor([[-0.8039,  0.8702, -2.2817,  0.6762,  0.1223, -0.0878]]) 0\n",
      "tensor([[-2.3421, -0.5931, -0.6420,  6.8331, -0.3247, -1.3253]]) 1\n",
      "tensor([[ 0.5691,  0.4415, -2.4079, -0.3207,  0.7959, -1.9594]]) 0\n",
      "tensor([[-2.9590, -0.6453,  1.3317, -0.1889, -0.1236,  3.0482]]) 1\n",
      "tensor([[ 1.8339,  6.1688, -2.1288, -3.7514,  1.6347, -7.4593]]) 1\n",
      "tensor([[-0.9527, -2.1100, -0.1836,  6.2867,  1.3477, -1.5493]]) 1\n",
      "tensor([[ 1.4666,  7.6095, -1.0825, -2.5985, -0.6177, -5.4268]]) 1\n",
      "tensor([[ 2.8849,  3.1219,  0.0282, -2.5051, -1.5229, -4.1418]]) 0\n",
      "tensor([[-2.2430,  0.3824, -0.0330, -2.5138,  0.1614,  3.7515]]) 0\n",
      "tensor([[ 4.9397, -0.1395, -4.2637, -2.6855,  0.7004, -6.4958]]) 0\n",
      "tensor([[ 3.9740,  1.3748, -0.9832, -1.1041, -1.9749, -3.7306]]) 1\n",
      "tensor([[-1.4400, -2.3766,  0.0540,  7.3106, -1.3038, -0.1333]]) 1\n",
      "tensor([[ 0.0917,  5.5436,  0.3853, -0.5658, -1.8357, -4.2508]]) 0\n",
      "tensor([[-1.6139,  1.6495,  0.0100, -1.1625,  0.9127,  0.9179]]) 0\n",
      "tensor([[ 1.6749, -0.3440, -0.3437, -0.2138, -0.5022, -2.6476]]) 1\n",
      "tensor([[-1.6717, -2.4175, -2.9173,  0.9056,  4.5708, -0.7631]]) 0\n",
      "tensor([[-0.5942,  0.2816,  1.7112,  3.0799, -0.8786, -1.5599]]) 0\n",
      "tensor([[ 0.0851,  2.2810, -1.3850, -2.0399,  1.5363, -1.6501]]) 0\n",
      "tensor([[-1.0812, -0.7364,  2.5946,  4.5045, -0.9538, -1.1186]]) 0\n",
      "tensor([[ 0.3488,  1.3495,  3.9483, -0.2312, -0.3277, -3.6333]]) 0\n",
      "tensor([[-0.9444,  0.9404,  8.0284, -0.6039, -1.5251, -1.2539]]) 1\n",
      "tensor([[-0.8488, -0.1735,  4.4064, -0.4784, -2.0307,  0.9764]]) 1\n",
      "tensor([[-2.5238, -2.9308, -3.4556,  1.7518,  9.3961, -3.1157]]) 1\n",
      "tensor([[-0.0927,  0.2401, -2.1294, -0.5588,  0.0814, -0.4521]]) 0\n",
      "tensor([[-0.1671,  0.6241,  3.2148, -0.5618, -1.4749,  0.1435]]) 0\n",
      "tensor([[ 0.7437,  0.4877,  2.9577, -0.3341, -1.7065, -0.8582]]) 1\n",
      "tensor([[-1.6263, -0.8329, -2.5365,  5.3299,  0.1963, -1.6492]]) 0\n",
      "tensor([[-0.6800, -0.8389,  7.4215,  2.7648, -2.4901, -2.2456]]) 1\n",
      "tensor([[-1.7552, -1.9484, -1.3245,  3.9321,  2.0673,  0.2680]]) 0\n",
      "tensor([[-0.2074, -0.3533,  0.6698,  2.7484, -1.8430, -0.1280]]) 0\n",
      "tensor([[-0.2223, -0.9222,  1.0888,  5.1788, -2.5098, -1.0995]]) 1\n",
      "tensor([[-0.6621, -1.7804, -2.1337,  1.9293, -1.7300,  3.2278]]) 0\n",
      "tensor([[-0.6229, -3.0119, -1.8078,  7.3639, -0.8053, -0.4924]]) 0\n",
      "tensor([[-0.4808,  1.4017,  6.1977, -0.5271, -2.2307, -1.6894]]) 1\n",
      "tensor([[-1.3301, -2.7094,  0.3073,  9.5323, -1.4175, -1.5323]]) 1\n",
      "tensor([[-2.3938, -0.5878, -0.1645,  4.0281, -0.4328,  0.1827]]) 0\n",
      "tensor([[-1.0536,  0.7787,  0.0222,  0.2510,  2.1828, -2.2776]]) 0\n",
      "tensor([[-1.2399,  0.9300,  1.1874, -0.4662, -2.2337,  2.7393]]) 0\n",
      "tensor([[-0.3064,  2.6597, -2.9677, -3.2023,  4.2490, -3.2704]]) 1\n",
      "tensor([[-1.3307, -3.5708, -1.5516,  8.5820, -1.2855,  0.6356]]) 1\n",
      "tensor([[-0.2931, -1.2047, -1.6101, -0.4228,  4.8316, -2.2342]]) 1\n",
      "tensor([[-0.8428,  4.7898, -2.2293,  3.4460, -1.6015, -3.1157]]) 0\n",
      "tensor([[-2.2578, -1.2951, -1.3358,  3.2928, -1.5297,  3.3004]]) 0\n",
      "tensor([[-0.1637, -0.6171, -1.9356, -1.3573,  5.9379, -4.9853]]) 1\n",
      "tensor([[ 0.5189, -0.1540,  0.2589,  4.4675, -2.5427, -1.7857]]) 1\n",
      "tensor([[ 1.6167,  2.0078,  0.8928,  1.7542, -0.6874, -4.8998]]) 0\n",
      "tensor([[-1.4520, -3.2889, -1.8573,  6.3685, -0.0246, -0.2503]]) 1\n",
      "tensor([[-2.5320, -1.9077, -0.4569,  0.9529, -0.0326,  4.6481]]) 1\n",
      "tensor([[-1.3114, -1.7647, -2.7259,  4.5837,  0.0646,  0.1034]]) 1\n",
      "tensor([[-1.4900, -2.5489, -1.4595,  6.8173, -0.8983,  0.1202]]) 1\n",
      "tensor([[-0.8856, -0.9952,  0.0503,  1.4188,  0.2900, -0.3612]]) 0\n",
      "tensor([[-0.6164,  6.7078, -1.8010, -3.5919,  1.2823, -2.5578]]) 1\n",
      "tensor([[-4.5948, -1.9032, -0.2976, -0.2205, -1.7617,  8.4874]]) 1\n",
      "tensor([[-0.0182,  7.3252,  2.0510, -1.3763, -3.0184, -2.7646]]) 1\n",
      "tensor([[-1.7938, -3.7570, -1.7911, 10.6334, -0.3148, -1.0428]]) 1\n",
      "tensor([[ 0.3662, 10.2512, -1.8729, -3.4767,  0.3800, -4.8902]]) 1\n",
      "tensor([[-0.4683, -0.6607, -0.2086,  3.8861, -0.4099, -1.2477]]) 1\n",
      "tensor([[ 0.5052,  2.2481,  6.8207, -1.4230, -2.7160, -2.9184]]) 0\n",
      "tensor([[ 0.1638,  3.4619,  0.0295, -1.0135, -1.4789, -0.6950]]) 0\n",
      "tensor([[-1.2419, -1.3951,  2.5642,  0.3389, -0.1347,  1.2444]]) 0\n",
      "tensor([[-2.3787, -2.3471,  1.0880,  1.8274, -1.0984,  3.5267]]) 0\n",
      "tensor([[-1.7665,  0.2521, -4.1532,  0.1727,  6.0561, -3.2875]]) 1\n",
      "tensor([[-2.9219, -1.2329, -1.5210,  0.4105,  5.6761, -1.2586]]) 1\n",
      "tensor([[-1.2374, -1.8523,  5.5328,  1.4319, -2.3090,  1.3414]]) 1\n",
      "tensor([[-1.2805, -3.6086, -1.3786,  0.6637, -0.0662,  4.6413]]) 1\n",
      "tensor([[-1.9645, -2.1978,  2.0315,  7.3345, -1.1458, -1.3382]]) 1\n",
      "tensor([[-0.0246,  0.6600,  4.4856,  1.3376, -0.2481, -3.5847]]) 0\n",
      "tensor([[-0.1682,  7.5939,  0.6710,  0.3128, -2.8134, -4.4848]]) 1\n",
      "tensor([[-0.2097,  1.8942,  5.1423,  1.3826, -3.3968, -1.4830]]) 1\n",
      "tensor([[-0.8458,  1.5948, -1.3080,  4.4234, -0.3391, -2.5171]]) 0\n",
      "tensor([[-2.1895,  0.2100,  1.9140,  5.4991, -1.0457, -1.4196]]) 1\n",
      "tensor([[-0.2834,  5.0018, -1.4228, -2.6903, -0.6761, -0.9817]]) 1\n",
      "tensor([[-2.6714, -0.3824, -2.8030, -0.3697, -0.8855,  5.4720]]) 1\n",
      "tensor([[-2.5312, -1.4034, -0.4236,  0.1146,  0.0549,  4.7842]]) 0\n",
      "tensor([[-3.0134, -1.3485, -0.8867, -0.1905, -1.0220,  5.7461]]) 0\n",
      "tensor([[-5.4292, -0.0415, -2.0082,  0.0650, -2.4426,  9.0002]]) 1\n",
      "tensor([[-0.0991,  0.3009,  2.9549,  1.3286, -1.0989, -1.3752]]) 0\n",
      "tensor([[ 1.0712, -1.1194,  1.8892,  0.2425, -1.4081, -0.1982]]) 0\n",
      "tensor([[-2.6545,  2.9767,  3.6307, -1.2066, -2.3359,  2.0386]]) 0\n",
      "tensor([[ 0.1663, -1.2729, -1.4245,  5.3593, -0.4537, -1.7954]]) 1\n",
      "tensor([[ 0.5527, -0.5063, -1.1398, -0.7329, -5.2330,  4.2016]]) 0\n",
      "tensor([[-0.4082, -0.2976,  7.7866, -0.5529, -1.5856, -1.9437]]) 1\n",
      "tensor([[-2.8730, -3.1860,  3.1938,  1.5373, -0.1264,  3.1056]]) 0\n",
      "tensor([[-0.7015, -2.6828, -0.4745,  7.9474,  0.1589, -1.8788]]) 1\n",
      "tensor([[-0.4711, -1.5621, -0.1627,  5.5930, -1.2201, -0.9173]]) 0\n",
      "tensor([[-2.3172, -3.4168, -0.4784,  4.8925, -1.2795,  2.5939]]) 1\n",
      "tensor([[-0.1858,  5.8504,  0.3429, -1.3706, -0.3383, -4.0056]]) 1\n",
      "tensor([[-4.3958,  0.6542, -5.6606,  0.5806,  5.0509, -0.6972]]) 0\n",
      "tensor([[-0.7461, 10.4458, -0.8334, -1.7326, -1.5710, -5.3526]]) 0\n",
      "tensor([[-0.7846,  0.0046, -1.7849,  2.6000,  0.1275, -0.6311]]) 0\n",
      "tensor([[-1.1140,  0.4640,  0.8615,  3.6897, -2.4800, -0.0290]]) 1\n",
      "tensor([[-3.6170, -0.9498, -0.9254,  1.0249, -0.0198,  5.0291]]) 1\n",
      "tensor([[ 0.4794,  9.9505, -0.9765, -1.8385, -1.0368, -4.5829]]) 1\n",
      "tensor([[-1.7297, -2.8788, -1.4502,  6.7100,  1.0386, -0.6134]]) 0\n",
      "tensor([[-1.1127, -0.2148,  0.5985,  0.4758,  3.0910, -3.3114]]) 1\n",
      "tensor([[-0.4606,  1.9336,  4.9783,  0.3202, -3.6064, -0.8962]]) 0\n",
      "tensor([[-1.8484,  8.7984, -2.0037, -4.0834, -0.1937, -1.1969]]) 1\n",
      "tensor([[ 0.7933,  4.4057, -2.6679, -1.2283,  1.2462, -4.6127]]) 0\n",
      "tensor([[-0.3409, -0.9182,  4.9323,  2.3539, -1.4029, -1.6722]]) 0\n",
      "tensor([[-1.4349, -1.3573, -0.0336,  6.2712, -1.2126, -0.6749]]) 1\n",
      "tensor([[-0.3201, -3.0746, -1.5703,  7.3694, -1.4703, -0.5106]]) 1\n",
      "tensor([[-0.7323, -0.0924,  2.7720,  5.6705, -1.6045, -2.7370]]) 0\n",
      "tensor([[-1.5298, -1.0386,  7.4288,  3.9530, -2.0420, -1.7124]]) 0\n",
      "tensor([[-0.9720,  4.8438, -3.5196, -2.9012,  1.9666, -2.4442]]) 1\n",
      "tensor([[-3.2949, -2.1858, -0.0160,  0.5241,  4.2468,  0.3878]]) 0\n",
      "tensor([[-0.0486,  6.9460, -0.6582, -1.4387,  0.2093, -4.1808]]) 0\n",
      "tensor([[ 0.9069,  5.9808, -2.2940, -2.7744, -0.6791, -2.5783]]) 1\n",
      "tensor([[-0.7688,  1.0700,  5.0970, -0.9993, -1.2975, -0.7773]]) 1\n",
      "tensor([[-0.1017, -1.0800, -0.7512,  3.6364, -0.2926, -0.9900]]) 0\n",
      "tensor([[ 0.3383,  3.9206, -2.0117, -0.8967, -0.4432, -3.1638]]) 0\n",
      "tensor([[-3.1279, -2.7546, -0.5306,  0.1292, -0.2666,  7.4853]]) 1\n",
      "tensor([[ 1.0707,  0.3187, -3.2538, -0.4573,  2.3960, -2.4776]]) 0\n",
      "tensor([[-0.9038, -2.3042, -0.5905, -0.6510, -0.1111,  3.6509]]) 0\n",
      "tensor([[ 2.4686,  1.3461, -1.8801,  2.2643, -3.4214, -1.8514]]) 1\n",
      "tensor([[ 2.7489, -1.5740, -2.4123,  5.7206, -1.5500, -3.3177]]) 0\n",
      "tensor([[-0.7764, -2.7805, -0.9457,  6.4623, -0.4377, -0.4053]]) 1\n",
      "tensor([[ 1.9377, -0.0075, -2.6846, -2.2526, -0.9109,  1.4448]]) 1\n",
      "tensor([[-1.7464, -1.1994, -3.9763, -0.1378,  6.7976, -1.8277]]) 0\n",
      "tensor([[-2.0513, -2.3362, -1.3229,  2.2037,  0.5350,  2.1643]]) 1\n",
      "tensor([[-0.1582,  3.5158,  5.2952, -1.9257, -1.4102, -2.3644]]) 1\n",
      "tensor([[-1.8402, -0.8584, -0.6894,  0.7167,  0.5388,  1.6198]]) 1\n",
      "tensor([[ 0.1561,  0.7316,  3.2149, -0.2443, -1.4749, -1.5935]]) 0\n",
      "tensor([[ 0.0547,  0.0243,  2.9582,  2.5697, -2.1362, -1.6928]]) 0\n",
      "tensor([[-2.2331, -0.1788, -2.5412, -0.3367,  0.4900,  3.7537]]) 1\n",
      "tensor([[-2.4341, -0.3046,  1.7572,  0.0982, -0.5626,  3.1923]]) 0\n",
      "tensor([[ 0.2029, -0.5677, -1.1348,  3.7663, -0.5579, -1.9212]]) 0\n",
      "tensor([[-1.2770,  0.9017,  5.6979,  1.7098, -2.5092, -0.1263]]) 1\n",
      "tensor([[-1.0487,  8.2831, -2.5300, -1.9874,  0.1168, -2.6672]]) 1\n",
      "tensor([[-0.9158, -0.7654,  8.1220, -1.4696, -1.7418,  0.3917]]) 1\n",
      "tensor([[ 1.2509,  0.2379,  3.5983,  0.0240, -1.9105, -1.3301]]) 1\n",
      "tensor([[-2.5288,  5.2137,  1.2207, -1.8478, -0.7755, -0.1854]]) 0\n",
      "tensor([[ 1.3210,  8.4094, -0.2219, -3.5823, -1.9147, -4.4830]]) 0\n",
      "tensor([[-0.5938, -2.4163, -3.5470,  1.7614,  2.5698, -0.5537]]) 0\n",
      "tensor([[-4.2275, -0.0833, -0.9465, -1.3390, -1.2571,  7.7437]]) 1\n",
      "tensor([[ 0.3133,  4.7540, -1.2462, -1.1460,  0.5492, -3.6073]]) 0\n",
      "tensor([[-2.1637, -2.1346, -0.4363,  0.5446,  3.8002, -0.6224]]) 1\n",
      "tensor([[ 3.8896,  3.4978, -2.7342, -2.8156, -2.6699, -3.1859]]) 0\n",
      "tensor([[ 0.2262, -1.1231, -1.3523,  5.9682, -0.3568, -2.6435]]) 0\n",
      "tensor([[-1.9544, -0.7651, -1.4255, -0.6536,  0.9346,  1.6818]]) 0\n",
      "tensor([[ 5.2706, -0.8274, -1.9589, -0.8439, -2.0189, -4.3464]]) 1\n",
      "tensor([[ 0.3420, -4.4703, -1.9394, 10.0287, -1.6165, -1.6187]]) 1\n",
      "tensor([[-2.8735,  1.9560, -4.7754, -2.2189,  4.0506,  0.6359]]) 0\n",
      "tensor([[-0.6964, -0.7897,  5.3481,  1.8837, -1.8571, -0.6646]]) 1\n",
      "tensor([[ 0.1548,  1.9480,  2.4089,  1.9379, -2.3716, -1.8573]]) 0\n",
      "tensor([[ 0.2764,  6.4515, -2.1478, -0.8832,  0.0477, -4.2802]]) 0\n",
      "tensor([[-0.6804,  5.0621, -0.6182,  1.0319, -1.3957, -2.3298]]) 1\n",
      "tensor([[-0.2561,  0.9535, -3.4339, -1.3647,  4.3592, -4.4768]]) 0\n",
      "tensor([[-0.9869, -1.5609,  4.4282,  1.1621, -0.5468, -0.1394]]) 0\n",
      "tensor([[-1.2124, -1.0186, -0.4023,  4.2476,  1.0418, -1.0445]]) 1\n",
      "tensor([[-0.6583,  4.0835, -0.6350, -0.8636,  0.1494, -1.8073]]) 0\n",
      "tensor([[-3.1755, -1.4287, -1.2259, -0.3008,  4.5927,  0.5076]]) 1\n",
      "tensor([[ 0.7106,  1.2331,  7.5114,  1.3562, -3.2821, -3.0751]]) 0\n",
      "tensor([[-1.7335, -1.6361,  0.9797,  0.7174,  1.2340,  0.6374]]) 0\n",
      "tensor([[-3.5903, -0.9628,  1.5027, -1.1300,  4.1775,  0.6970]]) 1\n",
      "tensor([[ 1.4646,  0.5455,  0.1614,  0.2995, -0.6043, -3.0244]]) 0\n",
      "tensor([[ 4.2442,  2.1779, -2.4311, -0.3953, -2.4675, -4.2971]]) 1\n",
      "tensor([[ 3.1601, -3.0675, -3.1207,  6.4723, -2.3022, -3.1047]]) 0\n",
      "tensor([[-1.7229, -1.2529, -1.6927,  0.0124, -0.6612,  5.0220]]) 0\n",
      "tensor([[-2.1336,  1.0845,  1.9352,  0.3040, -0.1818, -0.3367]]) 0\n",
      "tensor([[-1.2035,  4.9157,  5.1627, -3.1344, -0.8887, -2.7389]]) 1\n",
      "tensor([[-0.9541,  4.8079,  0.3605,  1.0956, -1.9182, -1.8991]]) 0\n",
      "tensor([[-1.9903,  9.1446, -0.2077, -3.0031,  0.3774, -2.9554]]) 1\n",
      "tensor([[-0.7367, -0.5325,  5.4116,  1.8244, -2.2501, -0.2724]]) 0\n",
      "tensor([[-0.5210, -0.4037, -4.6661, -1.5733,  5.5694, -3.1898]]) 1\n",
      "tensor([[-1.7007,  0.3839, -3.7935, -2.4553,  5.3273, -1.4586]]) 1\n",
      "tensor([[-0.0702,  1.0686,  0.5188,  0.5466, -3.0834,  1.6615]]) 0\n",
      "tensor([[-1.6863,  1.3170,  5.0141,  0.7477, -0.5482, -0.9404]]) 1\n",
      "tensor([[-0.4142,  1.1939,  6.3498,  2.1109, -2.4982, -1.9790]]) 1\n",
      "tensor([[-3.6761, -0.4074, -1.0441,  1.2179, -0.8766,  5.4375]]) 1\n",
      "tensor([[-0.8967,  3.3753,  5.9047,  0.8302, -3.3517, -1.3119]]) 0\n",
      "tensor([[-1.5117,  0.0453,  8.3553,  1.5110, -2.9124, -0.1441]]) 0\n",
      "tensor([[-1.7453, -2.6511,  0.9767,  7.5634, -1.4270, -0.8181]]) 1\n",
      "tensor([[-0.1465, -0.0441,  8.7275, -0.0445, -2.5354, -1.5922]]) 0\n",
      "tensor([[-0.5314, -1.7924, -0.2322,  7.8035, -1.9183, -1.2503]]) 0\n",
      "tensor([[ 0.5194, -2.0150, -2.8343, -2.4948,  4.1099, -2.2344]]) 0\n",
      "tensor([[-1.7555,  0.4740, -1.0779,  0.6958,  0.0332,  0.9527]]) 0\n",
      "tensor([[ 3.2412, -0.6910, -2.6012, -0.9036, -1.0122, -3.3801]]) 1\n",
      "tensor([[-2.3385, -1.1738, -0.6817,  1.1117, -0.9512,  4.7996]]) 1\n",
      "tensor([[-1.3397, -1.1647, -2.4594, -1.0218,  0.2131,  3.7095]]) 0\n",
      "tensor([[-2.0022,  1.1696, -0.3048, -0.2977,  0.0234,  1.5939]]) 0\n",
      "tensor([[-1.4787, -1.3192,  0.0705,  2.2648,  2.7582, -2.4210]]) 0\n",
      "tensor([[-3.4946, -1.3572,  3.8548,  0.3885, -2.0182,  5.0304]]) 1\n",
      "tensor([[-3.3444, -2.5347, -1.3889,  0.8149, -1.0736,  6.1187]]) 1\n",
      "tensor([[-1.4581, -3.9253, -0.2851,  8.1575,  0.6476, -1.9006]]) 0\n",
      "tensor([[-1.7274,  0.0161, -4.2033,  0.2569,  7.0993, -3.4790]]) 0\n",
      "tensor([[ 2.2323,  7.4844, -1.0279, -2.4668, -2.5973, -5.1345]]) 1\n",
      "tensor([[-0.6418,  3.5700, -2.4819, -0.6229,  0.9443, -2.7031]]) 0\n",
      "tensor([[-0.9983,  4.7009, -3.8814, -2.3325,  1.5251, -2.3931]]) 0\n",
      "tensor([[-1.3629, -1.6468,  0.7822,  4.1638,  0.6756, -1.0031]]) 0\n",
      "tensor([[-3.0071,  0.3288,  6.8388,  0.4142, -0.4190,  0.4767]]) 1\n",
      "tensor([[-1.8194, -2.0477, -0.8515,  2.4609, -0.2023,  2.4683]]) 1\n",
      "tensor([[-1.8753, -0.9664,  0.2460,  0.6810, -1.6362,  3.7550]]) 1\n",
      "tensor([[-1.1138,  2.8371,  4.8067,  0.7876, -2.0477, -1.4971]]) 0\n",
      "tensor([[-2.6125, -3.1604, -2.7832,  2.1692,  0.2573,  4.7501]]) 0\n",
      "tensor([[-1.0147,  3.2070,  1.0806, -0.4934, -1.2927,  0.0306]]) 0\n",
      "tensor([[-1.7546, -2.0945,  1.2662, -0.5934, -1.7819,  5.5417]]) 1\n",
      "tensor([[ 0.0140,  5.9688,  0.0472, -0.1832, -1.6563, -3.5917]]) 1\n",
      "tensor([[ 1.2838,  4.8458, -2.4256, -1.8066, -0.9449, -3.1195]]) 1\n",
      "tensor([[-1.3192, -1.5823, -1.5630, -0.8108, -1.0688,  3.8277]]) 0\n",
      "tensor([[-2.5048, -2.8332,  1.4126,  7.1732, -0.5265, -0.4534]]) 1\n",
      "tensor([[ 0.4046,  7.6768, -2.3998, -4.6631, -0.7202, -2.5178]]) 1\n",
      "tensor([[-1.8433,  3.9554, -3.9981, -2.1077,  4.8691, -5.0143]]) 1\n",
      "tensor([[-3.2524, -0.8565, -2.8768,  0.8284,  5.5543, -1.1298]]) 1\n",
      "tensor([[-0.8049,  5.8676,  2.6550, -0.2348, -2.3855, -2.0222]]) 1\n",
      "tensor([[-1.6279,  2.9191, -0.8126,  1.0032,  0.3509, -1.5600]]) 0\n",
      "tensor([[-0.4764,  5.2612, -2.9692, -2.4552,  0.0263, -1.6863]]) 1\n",
      "tensor([[ 1.3208, -4.8490, -4.3882,  9.0826, -1.0400, -1.1382]]) 1\n",
      "tensor([[-0.7042,  3.8373, -0.3761, -1.2381, -0.2733, -0.6077]]) 0\n",
      "tensor([[-2.9694, -1.0516, -1.8624, -1.0337,  1.3790,  3.4928]]) 0\n",
      "tensor([[-0.9935, -0.0975,  5.9468, -0.6180, -0.7066, -0.0985]]) 1\n",
      "tensor([[ 0.3244,  1.4844,  4.5402, -0.1728, -0.9494, -2.8401]]) 1\n",
      "tensor([[-3.0258, -1.7815, -1.7259, -0.7627,  3.6970,  2.0825]]) 0\n",
      "tensor([[-1.1136, -0.2611,  5.4471,  0.4469, -1.3479,  0.0842]]) 1\n",
      "tensor([[ 1.7290,  2.7482, -1.3324,  1.6988, -1.3272, -4.1206]]) 0\n",
      "tensor([[-2.0214,  5.4890, -0.5434, -2.3600, -0.6508, -0.2455]]) 1\n",
      "tensor([[ 0.4415,  7.0894, -2.0902, -1.1458,  0.7744, -5.1091]]) 0\n",
      "tensor([[-3.5379, -1.5866, -0.3097,  0.4630, -1.1210,  5.5102]]) 0\n",
      "tensor([[-2.7058, -0.6433, -3.6071,  0.1693,  4.9558, -0.7466]]) 1\n",
      "tensor([[ 2.4522, -0.3737, -3.5635, -1.2617,  4.7897, -6.9694]]) 1\n",
      "tensor([[-0.0122, -0.9718, -1.3072,  0.1004,  1.7583, -2.1890]]) 0\n",
      "tensor([[-0.3337, -0.0472,  7.9652, -0.2064, -2.4160, -1.5615]]) 1\n",
      "tensor([[ 4.1119, -0.0187, -1.3422,  0.9350, -1.2330, -4.7139]]) 0\n",
      "tensor([[-2.4072, -0.9251, -1.6762, -0.3976,  1.2627,  2.3906]]) 0\n",
      "tensor([[-2.8398, -0.0626, -4.0372, -1.3859,  4.8571, -0.0875]]) 1\n",
      "tensor([[-2.9731,  0.6205,  1.9443, -1.5199, -1.5222,  4.0459]]) 1\n",
      "tensor([[-0.8089, -2.7836,  3.5784,  4.0725, -1.3051, -0.2255]]) 0\n",
      "tensor([[-0.5705,  8.4667, -1.7460, -2.0817, -0.0748, -3.4018]]) 1\n",
      "tensor([[ 4.2647, -2.9867, -1.4250,  6.5502, -3.1800, -2.3861]]) 0\n",
      "tensor([[-0.7581,  0.2450, -5.2770, -0.2860,  5.2047, -2.2858]]) 1\n",
      "tensor([[-1.8388, -2.0199, -1.5345,  8.2111, -0.6014, -0.4083]]) 1\n",
      "tensor([[-2.7548, -0.0369,  5.8102, -0.6148,  1.0511, -0.4724]]) 1\n",
      "tensor([[-0.5365, -0.3037,  4.4282, -0.0980, -0.8265, -0.7666]]) 0\n",
      "tensor([[-0.9190, -2.0154,  0.5184, -1.6599,  5.4955, -2.7719]]) 1\n",
      "tensor([[-0.7338,  1.6362,  1.7941,  2.3281, -1.7883, -1.2214]]) 0\n",
      "tensor([[-1.4505, -3.0072,  0.0803,  7.9471, -0.8598, -1.1550]]) 0\n",
      "tensor([[-1.4314, -2.1054,  5.3418,  2.5869,  1.1791, -0.9097]]) 0\n",
      "tensor([[-1.5471, -1.8319, -0.6423, -1.0375,  5.4628, -1.7566]]) 1\n",
      "tensor([[ 0.3224,  5.6404, -1.0535, -0.3377, -1.2607, -3.6660]]) 1\n",
      "tensor([[-1.4601, -1.3465, -5.4381,  0.0358,  9.0213, -5.1576]]) 0\n",
      "tensor([[-3.1455, -0.4920, -1.9998, -0.5963,  4.1836,  0.4428]]) 0\n",
      "tensor([[-1.4235,  2.6919,  2.9837, -0.8469, -1.9475, -0.1325]]) 0\n",
      "tensor([[ 0.0112, -3.1761, -5.2233,  0.3505,  5.9276, -2.6173]]) 1\n",
      "tensor([[-0.1180,  8.2403,  1.0736, -4.3162, -1.1833, -3.5708]]) 0\n",
      "tensor([[ 0.8343,  7.2233, -0.5180, -3.0312, -1.1351, -3.8191]]) 0\n",
      "tensor([[-1.0836, -0.2678,  5.7736, -0.9895, -0.8474, -0.1662]]) 0\n",
      "tensor([[-0.1447, -1.4183,  0.8883,  5.4574, -2.7986,  0.1201]]) 0\n",
      "tensor([[-1.1813, -4.3527, -0.9965,  8.2676,  0.7010, -0.5558]]) 0\n",
      "tensor([[ 0.2746,  6.9322, -1.4201, -0.0260, -1.6279, -3.6133]]) 1\n",
      "tensor([[-1.5014, -2.4300,  0.6905,  3.2991, -0.6136,  1.1978]]) 1\n",
      "tensor([[-0.1097, -0.2088, -2.1893,  2.6073,  0.8500, -2.1168]]) 0\n",
      "tensor([[-4.6604, -2.0976, -1.3452, -0.6373, -0.6578,  9.6866]]) 1\n",
      "tensor([[ 0.3242,  8.7894, -0.8021, -3.3189, -0.9706, -4.4481]]) 0\n",
      "tensor([[-0.7972, -0.7447, -3.7940, -0.5259,  6.0002, -2.7995]]) 0\n",
      "tensor([[-0.0950,  5.4933,  0.8017, -4.1150, -1.5342, -1.6320]]) 1\n",
      "tensor([[-1.8465, -2.3404,  5.1597,  1.3804, -1.2582,  1.1288]]) 1\n",
      "tensor([[-1.5352, -1.5970,  0.0806,  6.5373, -0.8203, -0.4626]]) 0\n",
      "tensor([[-3.2177, -1.7393,  4.1156,  3.3690, -1.3748,  1.7661]]) 0\n",
      "tensor([[-3.2600,  0.2254, -2.2463, -0.3089,  3.4657,  0.5538]]) 0\n",
      "tensor([[-2.0280, -0.9624, -0.5575, -1.3927, -0.6741,  4.5783]]) 0\n",
      "tensor([[ 0.1024,  4.4108, -1.4636, -1.8197, -0.5044, -3.3304]]) 1\n",
      "tensor([[-1.5985, -0.2680, -1.3724,  0.4361,  3.2945, -1.2828]]) 1\n",
      "tensor([[-1.5031, -1.5102,  9.5153,  2.1584, -2.8182, -0.3338]]) 0\n",
      "tensor([[-0.4393, -0.5491,  3.8665,  0.6414, -0.5089, -0.6318]]) 1\n",
      "tensor([[-1.1089, -2.9275,  2.2850,  6.9719, -2.5889, -0.0746]]) 0\n",
      "tensor([[ 1.2169,  1.1779,  3.9454,  1.2200, -1.7866, -4.0822]]) 0\n",
      "tensor([[-0.7811, -2.1394,  0.0301,  6.0153, -1.2098, -0.8929]]) 1\n",
      "tensor([[-1.4058,  1.0999,  1.6434,  1.0300, -2.4394,  0.4623]]) 0\n",
      "tensor([[-0.9203, -0.4298,  8.3703, -0.3581, -2.0919, -0.0132]]) 1\n",
      "tensor([[-0.0599,  1.7629,  1.2285, -1.6453,  0.8257, -2.0864]]) 0\n",
      "tensor([[-0.5561, -1.1162, -3.0847,  0.8152,  5.3760, -3.4051]]) 0\n",
      "tensor([[ 3.1710, -0.9771, -1.6910, -0.5882, -1.3759, -1.4620]]) 0\n",
      "tensor([[ 0.9478,  0.8653, -3.5707, -0.5352,  6.0713, -6.5656]]) 0\n",
      "tensor([[ 0.3126,  8.5042, -2.1261, -2.1297, -1.0237, -3.5115]]) 1\n",
      "tensor([[-2.0765,  0.7449, -3.0789, -1.4952,  6.8575, -3.0265]]) 1\n",
      "tensor([[-0.3190,  7.7831, -0.1178, -3.0766, -1.6984, -2.9676]]) 1\n",
      "tensor([[-2.6903, -0.9439,  0.0979,  0.1244, -1.3340,  5.4048]]) 1\n",
      "tensor([[-0.9093, -2.1042, -2.0641,  5.8231,  0.9550, -2.6661]]) 1\n",
      "tensor([[-0.7648, -1.8687, -1.8610,  5.2489, -0.8105, -0.0802]]) 0\n",
      "tensor([[ 0.7360,  6.4137, -2.5584, -0.8853, -1.6574, -3.1733]]) 0\n",
      "tensor([[ 0.1752,  3.5026,  6.1915, -1.2022, -2.9752, -2.1110]]) 0\n",
      "tensor([[ 0.4119,  3.8483,  0.1691, -0.2855, -1.0798, -2.8933]]) 0\n",
      "tensor([[-0.8795, -2.3878,  0.9364,  6.5585, -0.6613, -2.1329]]) 1\n",
      "tensor([[-1.6180, -2.4801, -1.2217,  8.4329, -1.0696, -0.6309]]) 1\n",
      "tensor([[-0.3303, -2.8594, -2.5013,  7.8138, -1.0177, -1.0243]]) 0\n",
      "tensor([[ 0.3185,  5.4197, -1.9275, -1.3792,  0.4503, -4.4271]]) 0\n",
      "tensor([[ 2.5586,  1.0203, -1.9370, -2.3765, -0.3565, -2.4071]]) 1\n",
      "tensor([[-2.5989, -2.7734, -1.3633,  1.2098,  5.0394,  0.0577]]) 0\n",
      "tensor([[-0.0186,  4.8537,  2.5614, -0.7710, -2.9031, -0.5540]]) 0\n",
      "tensor([[-1.4114, -3.3256,  1.1428,  6.9192, -0.6678,  0.0453]]) 1\n",
      "tensor([[-3.0884, -2.2447,  3.9006,  0.7781, -0.8666,  3.6861]]) 1\n",
      "tensor([[-1.0669, -1.2627,  5.7513,  1.5589, -0.7442, -0.9775]]) 0\n",
      "tensor([[-4.6928, -1.2765, -2.7503, -0.3779,  0.8054,  6.3225]]) 1\n",
      "tensor([[-5.4403, -2.8823, -2.2227, -0.1723, -0.2024,  9.9351]]) 0\n",
      "tensor([[ 0.2897,  3.2672,  0.8876, -0.3404, -1.1297, -2.2044]]) 1\n",
      "tensor([[-2.3999,  1.4984,  6.8795,  0.5747, -1.8784,  0.5574]]) 0\n",
      "tensor([[-2.8100, -0.4421,  3.1604,  0.6543,  1.9815, -1.8953]]) 0\n",
      "tensor([[-0.1163, -1.1001, -0.9095,  3.7111, -1.1685,  0.4327]]) 1\n",
      "tensor([[-3.2061, -1.0276, -1.6996, -1.6627, -0.3036,  5.7865]]) 1\n",
      "tensor([[-0.8074,  3.8975, -0.8661,  3.4611, -0.8770, -4.0737]]) 1\n",
      "tensor([[ 2.0060,  0.6646, -2.5963, -2.1252,  0.6643, -2.9129]]) 0\n",
      "tensor([[-1.6705,  0.8906, -4.7933, -0.2819,  7.0670, -5.1355]]) 1\n",
      "tensor([[-1.3832, -0.7980,  3.1200,  6.4383, -1.7495, -1.6179]]) 0\n",
      "tensor([[-2.7747, -1.6464,  3.1340, -1.7327, -0.8640,  4.2794]]) 1\n",
      "tensor([[ 3.1749, -0.5345, -1.5561,  0.9096, -0.5543, -3.1513]]) 1\n",
      "tensor([[ 4.0888, -2.7292, -3.8364,  4.4804, -0.4691, -3.6757]]) 1\n",
      "tensor([[ 1.2128,  9.8150, -2.4111, -1.0426, -1.6175, -5.0325]]) 1\n",
      "tensor([[ 0.3793,  5.4969,  0.4233, -1.1300, -1.0477, -4.3372]]) 1\n",
      "tensor([[ 2.8563,  2.8687, -3.0730,  2.1107, -1.5890, -4.6275]]) 1\n",
      "tensor([[ 0.0418,  2.9537, -0.8845, -0.4455,  0.5898, -2.3959]]) 0\n",
      "tensor([[ 3.0850,  2.6656, -0.7927, -0.3047, -2.1471, -3.6619]]) 1\n",
      "tensor([[-2.3561, -1.8138, -1.8087,  0.4413, -1.1296,  5.9548]]) 1\n",
      "tensor([[-1.5585, -1.4605,  6.1520,  1.1909, -1.0450, -0.5676]]) 1\n",
      "tensor([[-2.7953, -1.4221, -1.8934,  0.2901,  4.7378, -0.6220]]) 1\n",
      "tensor([[-1.7168,  1.0190, -4.5129, -1.4444,  7.0088, -4.1485]]) 1\n",
      "tensor([[-1.1395,  1.8565, -0.4405, -0.7179, -0.9442,  1.2158]]) 0\n",
      "tensor([[-1.3931, -1.4043, -1.2510, -0.6618,  0.0655,  3.4173]]) 1\n",
      "tensor([[-1.8973,  0.1040,  5.2077, -1.0689, -1.4491,  2.3752]]) 0\n",
      "tensor([[ 0.3291,  7.7344, -2.7813, -3.1964,  0.3556, -4.0279]]) 0\n",
      "tensor([[-2.7347, -1.2944, -1.2542,  5.9635, -0.0860,  0.3487]]) 1\n",
      "tensor([[-1.0238, -2.2452,  0.0465,  5.3468,  0.5662, -1.7221]]) 0\n",
      "tensor([[ 0.9371,  3.5780,  0.5812,  0.0938, -1.8127, -3.1875]]) 0\n",
      "tensor([[-1.8099, -0.9765,  2.7495,  3.4399, -1.5721,  0.8365]]) 1\n",
      "tensor([[-1.0201, -1.7543, -2.3244,  0.1368,  5.3225, -2.9647]]) 1\n",
      "tensor([[-2.5160, -1.8070, -1.6543,  0.3730,  5.0678, -0.2118]]) 1\n",
      "tensor([[-1.8080, -3.2634,  3.4333,  6.5305, -2.6570,  0.4800]]) 0\n",
      "tensor([[-1.7304,  0.5576,  4.2308, -0.6708, -1.3501,  1.4779]]) 0\n",
      "tensor([[ 0.2764, -0.3551,  3.6071, -1.2889, -0.7143, -0.1980]]) 0\n",
      "tensor([[-1.8673, -2.2116,  1.9880,  7.4746, -1.5366, -0.7838]]) 0\n",
      "tensor([[-2.8525, -1.1592,  0.3074, -2.2123,  0.9039,  4.5696]]) 1\n",
      "tensor([[-2.5013, -1.0923, -2.0854,  0.3772,  3.2866,  0.8331]]) 0\n",
      "tensor([[-0.2400,  0.0573, -2.8570, -2.1107,  6.0353, -4.4817]]) 1\n",
      "tensor([[-1.9758, -1.9965, -0.8492, -0.2243, -1.4046,  6.0471]]) 1\n",
      "tensor([[-0.3375, -2.3098, -2.4957,  5.7752,  0.6393, -2.0710]]) 1\n",
      "tensor([[-2.2443, -2.0697, -1.5612,  6.0721, -0.3999,  0.8637]]) 0\n",
      "tensor([[ 0.4086,  4.6951,  3.9042, -1.6758, -2.2537, -2.3535]]) 1\n",
      "tensor([[ 2.7782, -1.7186, -2.7284, -1.5618, -1.2888,  0.0435]]) 1\n",
      "tensor([[-2.0236, -3.6085, -0.3510,  6.5656,  0.8712, -0.3875]]) 0\n",
      "tensor([[ 0.0115,  8.2827, -1.3707, -2.9391, -1.1859, -3.4414]]) 1\n",
      "tensor([[-0.7774, -0.0634,  5.8050, -0.3678, -1.6363,  0.5555]]) 0\n",
      "tensor([[-2.1056, -1.6183, -1.2647, -1.3913,  4.8157, -0.4179]]) 0\n",
      "tensor([[ 0.0886,  7.1171, -2.8673, -3.1016,  1.5658, -4.0940]]) 0\n",
      "tensor([[-1.0129,  4.3539, -0.9916,  3.0716, -1.1139, -2.6306]]) 1\n",
      "tensor([[-3.3521, -0.1495, -0.6085, -0.4587,  7.8758, -3.6352]]) 0\n",
      "tensor([[-0.9460, -2.1420, -2.5805, -0.3182,  2.7719,  0.9806]]) 1\n",
      "tensor([[-3.4338, -1.8977, -2.2492,  1.7098, -0.4285,  6.2327]]) 1\n",
      "tensor([[ 3.2801, -0.4016, -1.8173, -1.0419, -0.2514, -3.0736]]) 1\n",
      "tensor([[ 4.5314, -0.6357, -2.1423, -1.9180, -1.4381, -3.5740]]) 1\n",
      "tensor([[ 0.4248,  4.8664,  2.5443, -1.9286, -3.3638, -1.6492]]) 0\n",
      "tensor([[-0.7477,  0.6141, -0.5569, -1.2445,  0.3973,  0.4662]]) 0\n",
      "tensor([[-1.0235,  0.9965, -6.2128, -2.1939,  7.3698, -4.7610]]) 1\n",
      "tensor([[-0.4646, -0.5062, -2.2453, -1.2124,  4.7508, -1.1945]]) 1\n",
      "tensor([[-0.8708,  6.4119, -0.1294,  0.2272, -2.1464, -2.2370]]) 1\n",
      "tensor([[-0.5649, -2.7197, -5.5168, -0.1096,  7.4537, -5.7807]]) 1\n",
      "tensor([[-3.5346, -0.1189, -2.3504,  0.0338, -0.3405,  4.5996]]) 0\n",
      "tensor([[-2.7086, -2.2276, -0.9585,  2.0475, -1.8548,  5.3754]]) 0\n",
      "tensor([[-0.9947, -1.3097,  0.5528,  5.6812, -0.5804, -1.3551]]) 1\n",
      "tensor([[-1.8552, -0.1143,  5.2334,  0.5981, -1.5612,  1.2361]]) 1\n",
      "tensor([[-1.8300,  1.2998,  5.4468,  1.9516, -1.6864, -0.7872]]) 1\n",
      "tensor([[ 0.7119, -3.0818, -3.8425,  6.6539, -0.6001, -1.9281]]) 1\n",
      "tensor([[ 0.4620, -0.4878, -1.1757,  4.9108, -1.6985, -0.8724]]) 0\n",
      "tensor([[-1.6200, -0.8878, -1.6157,  5.7646, -1.1357,  0.7054]]) 1\n",
      "tensor([[-0.9529,  4.5973,  0.7546, -2.5552, -1.4899, -0.9803]]) 0\n",
      "tensor([[-0.0522, -1.3463,  3.2090,  1.6870, -0.0778, -1.3705]]) 0\n",
      "tensor([[-1.6705,  0.0091, -3.5000, -1.2888,  4.7822, -0.7277]]) 0\n",
      "tensor([[-1.1404, -0.5037,  0.7289,  3.4653, -0.4894, -1.3689]]) 0\n",
      "tensor([[-1.3811,  1.0367,  0.7527, -0.3136, -1.1632,  1.3483]]) 0\n",
      "tensor([[-0.5519, -0.5085, -4.0492, -0.8055,  5.7394, -4.0070]]) 1\n",
      "tensor([[ 0.1459, -0.2657, -2.9979,  0.0346,  4.0388, -3.5025]]) 1\n",
      "tensor([[ 2.3763, -0.9952, -3.7074,  0.9975,  2.8029, -7.1517]]) 1\n",
      "tensor([[-1.1904, -1.9653,  1.7985,  7.2763, -1.0671, -1.3274]]) 0\n",
      "tensor([[-4.1710, -3.1609,  1.1036, -1.1787,  5.9144,  0.2940]]) 1\n",
      "tensor([[-2.6643, -1.8432, -1.0361,  5.7354, -1.0473,  1.8377]]) 1\n",
      "tensor([[-0.6619,  5.5627,  0.4794,  1.9780, -1.6788, -4.3033]]) 0\n",
      "tensor([[ 0.4602,  2.9760,  1.1313, -1.5432, -2.1509, -0.5441]]) 0\n",
      "tensor([[ 0.4513, -0.5247,  0.1033,  3.2018, -1.7497, -0.5120]]) 0\n",
      "tensor([[-1.7331,  5.5287,  1.3054, -2.0501, -1.8879,  0.0688]]) 0\n",
      "tensor([[-0.6655,  1.9505,  7.5295, -0.8172, -2.1961, -1.0399]]) 0\n",
      "tensor([[ 4.9143, -2.7494, -2.9526,  5.1572, -2.6416, -3.2668]]) 0\n",
      "tensor([[-1.8901, -0.2431,  1.7210,  0.5756, -0.4025,  1.0316]]) 1\n",
      "tensor([[-1.4876, -2.3813, -0.7673,  1.8259,  4.2610, -1.8079]]) 1\n",
      "tensor([[-0.7360, -1.9838,  2.3826,  8.4352, -2.0960, -2.7654]]) 1\n",
      "tensor([[-1.2126,  0.6880,  4.0599,  0.1688, -1.9193,  0.5966]]) 0\n",
      "tensor([[-0.2418,  1.7072, -0.4686,  0.1872,  1.5655, -2.1513]]) 0\n",
      "tensor([[-1.6849, -2.8117,  0.4208,  6.7420, -0.0229, -0.0663]]) 0\n",
      "tensor([[ 5.1321,  0.2368, -3.4861,  0.5804, -1.4244, -6.6721]]) 1\n",
      "tensor([[-2.3615, -2.6713, -0.1347,  2.3658,  1.5044,  0.5662]]) 0\n",
      "tensor([[ 0.2299,  6.3362, -3.2272, -3.9833,  1.6240, -4.0082]]) 0\n",
      "tensor([[-0.8295, -2.1185,  6.4501,  0.8412, -0.6551, -0.5469]]) 1\n",
      "tensor([[-1.5387, -1.0746, -1.1277,  4.3851, -0.1206, -0.3430]]) 1\n",
      "tensor([[-2.3420, -1.1086, -3.2252,  0.0491,  5.6848, -1.1351]]) 0\n",
      "tensor([[-1.8278, -1.4338,  8.7439,  2.3670, -3.3783,  0.4987]]) 1\n",
      "tensor([[ 3.1786, -0.7062, -2.0795, -1.5666, -1.3754, -1.4447]]) 0\n",
      "tensor([[-1.1692, -0.2804,  5.3705, -0.0461, -1.3996,  0.1820]]) 0\n",
      "tensor([[-2.2718, -0.1246, -4.6519,  0.5881,  7.1976, -3.8947]]) 1\n",
      "tensor([[-1.9566, -3.9117, -5.2498,  5.3089,  5.7918, -2.7671]]) 1\n",
      "tensor([[-2.8103,  1.0488, -0.8683, -0.6281,  4.2029, -0.6782]]) 1\n",
      "tensor([[-2.6976, -2.1372, -1.1907,  0.5721,  3.2080,  1.4301]]) 1\n",
      "tensor([[ 4.2880, -2.3284, -1.3350, -0.9432, -1.8226, -1.3126]]) 0\n",
      "tensor([[-1.9414, -0.5078, -2.3129, -0.2334,  7.8691, -4.1668]]) 1\n",
      "tensor([[ 0.4972,  3.8035, -4.0576, -4.0060,  4.8516, -4.9533]]) 0\n",
      "tensor([[ 2.8327, -0.3282,  5.2191, -1.0604, -3.8921, -2.1756]]) 1\n",
      "tensor([[-4.0161, -2.6300, -1.8326,  0.8863, -0.2178,  7.1084]]) 1\n",
      "tensor([[ 5.3304, -0.0868, -1.4942, -2.5999, -2.1993, -2.1034]]) 0\n",
      "tensor([[-0.5791,  1.0233,  0.6374,  2.6033, -0.5548, -2.0756]]) 0\n",
      "tensor([[-0.5858, -1.0606,  4.0901,  3.8770, -2.0711, -0.9738]]) 0\n",
      "tensor([[ 0.4254,  0.6549,  6.0638,  0.4706, -2.5196, -2.1090]]) 0\n",
      "tensor([[-1.0460, -0.8597, -0.9097,  5.4083,  0.1043, -1.6998]]) 0\n",
      "tensor([[-3.1630, -0.8977, -1.9012,  0.5522, -1.4928,  6.1359]]) 0\n",
      "tensor([[-3.3180, -1.1824, -1.0966, -0.4632, -1.3464,  7.1670]]) 0\n",
      "tensor([[-3.0760, -1.2340, -4.5344,  0.3463,  4.4965,  1.2119]]) 1\n",
      "tensor([[-1.6883,  1.1820,  2.0262,  0.3911, -0.7359,  0.6554]]) 0\n",
      "tensor([[ 3.7520, -0.3125, -2.6938, -2.8683, -1.2280, -1.3532]]) 1\n",
      "tensor([[-2.3852, -3.5422, -0.7545,  0.9821,  0.1031,  5.2920]]) 0\n",
      "tensor([[-2.5958, -0.1985, -0.5022, -1.6366, -1.1601,  6.1953]]) 0\n",
      "tensor([[-3.7996, -0.5427, -2.5430,  0.3730,  6.8271, -1.7615]]) 1\n",
      "tensor([[-1.0718,  6.9853, -3.2643, -3.6813,  1.7888, -2.9535]]) 1\n",
      "tensor([[-2.1322e+00, -2.0897e+00, -2.4022e+00,  7.3495e+00, -1.1471e+00,\n",
      "         -2.1913e-03]]) 1\n",
      "tensor([[ 0.0057, -1.1601,  3.1534,  1.6298, -0.9670, -1.1039]]) 0\n",
      "tensor([[ 0.7066, -3.0326,  5.2118,  2.6664, -1.3016, -1.1124]]) 0\n",
      "tensor([[-0.2432,  0.0672,  0.9004,  3.5277, -1.1201, -2.2115]]) 1\n",
      "tensor([[-1.3384,  1.8355,  4.2507, -2.0097, -0.9442, -0.8741]]) 0\n",
      "tensor([[ 1.9026,  3.6115,  0.9302, -0.4707, -2.6358, -3.6247]]) 0\n",
      "tensor([[ 0.2247, -0.9147,  5.4455,  2.0842, -3.1301, -0.4868]]) 1\n",
      "tensor([[-0.7626,  2.1687,  5.2321,  0.6939, -2.3183, -0.2833]]) 0\n",
      "tensor([[-2.4396,  0.0885, -3.7134, -0.5649,  6.6174, -2.6825]]) 0\n",
      "tensor([[-1.0288, -1.9928, -2.2509,  6.3204,  0.9899, -2.7453]]) 1\n",
      "tensor([[ 0.2953, -2.6542,  2.8679,  5.7520, -0.9018, -2.9349]]) 1\n",
      "tensor([[-2.6220, -1.5771, -1.5765,  0.0451, -0.4768,  5.9879]]) 1\n",
      "tensor([[-0.1924,  2.6999, -2.9254, -0.1558,  0.7191, -1.9830]]) 1\n",
      "tensor([[ 2.6782,  4.2241, -1.2959, -1.9964, -3.1508, -2.7725]]) 1\n",
      "tensor([[-1.5846,  0.0991,  0.7982,  2.5738, -1.0721,  0.2602]]) 0\n",
      "tensor([[ 0.5065,  0.9183,  4.4910, -1.8082, -2.4785, -0.1430]]) 1\n",
      "tensor([[-1.0750, -0.4960,  1.9235,  2.7475, -0.0845, -0.8060]]) 0\n",
      "tensor([[-0.9930,  0.2593,  6.7156, -1.2571, -1.6330,  0.2734]]) 0\n",
      "tensor([[-4.1951, -3.4523, -4.3496,  2.6504,  4.2330,  2.3607]]) 1\n",
      "tensor([[-1.6766, -4.7118, -1.5350, 10.7121, -1.1896, -0.2322]]) 1\n",
      "tensor([[-1.5662,  0.1172, -4.4666, -0.0543,  9.7949, -6.1430]]) 1\n",
      "tensor([[-2.5155,  1.3234,  6.2946, -0.9045, -1.1527,  0.2025]]) 1\n",
      "tensor([[-1.2800,  1.3821,  4.2475, -1.1651, -2.0075, -0.0195]]) 0\n",
      "tensor([[-0.8327,  0.1296,  0.9887,  2.1061,  1.0811, -1.9427]]) 0\n",
      "tensor([[-1.3415, -1.5652,  3.2182,  4.0146, -1.4885, -0.1850]]) 0\n",
      "tensor([[-0.9656,  2.6039, -0.6445,  6.6384, -1.7859, -4.3951]]) 0\n",
      "tensor([[-1.9044, -2.2484, -1.2879,  2.8207,  1.5312,  0.7082]]) 0\n",
      "tensor([[-1.1960,  0.2369,  3.4748,  0.1900, -1.4266,  1.2011]]) 0\n",
      "tensor([[-1.9537, -0.1748, -5.6752, -0.9734,  8.3749, -4.2118]]) 0\n",
      "tensor([[-1.3475,  0.5582, -1.5797, -1.7241,  4.2210, -3.4623]]) 0\n",
      "tensor([[ 0.7462,  6.1324, -1.5985, -0.2403, -1.4571, -3.2051]]) 0\n",
      "tensor([[ 1.2049,  5.8311, -1.3589, -3.2979,  1.5080, -5.0779]]) 1\n",
      "tensor([[-0.2646,  1.3764,  4.9806,  0.8363, -1.4594, -2.2648]]) 1\n",
      "tensor([[-1.3451, -3.7513, -2.2705,  3.9002,  4.1417, -0.7601]]) 1\n",
      "tensor([[-1.6529, -2.0115, -2.3558, -1.4175,  4.9942, -1.0916]]) 0\n",
      "tensor([[-3.1721, -1.4177, -0.8912, -1.5822,  0.8081,  4.5760]]) 1\n",
      "tensor([[-0.6985, -0.4600, -3.6105, -0.2911,  5.3080, -2.7461]]) 1\n",
      "tensor([[ 2.3985, -2.6172,  0.9781, -0.2821, -0.1632, -0.8977]]) 1\n",
      "tensor([[-2.6855,  0.3692, -1.9807, -3.1597,  3.9426,  0.3690]]) 0\n",
      "tensor([[-3.0340, -0.1721, -1.1479,  5.9720, -0.0955, -0.0578]]) 1\n",
      "tensor([[-0.8485, -1.3430, -4.1142, -0.0473,  7.2801, -4.4408]]) 1\n",
      "tensor([[-1.5206,  0.4524, -0.1572, -0.3940, -2.1006,  4.1470]]) 0\n",
      "tensor([[-1.0977,  0.4718, -1.8895, -0.8089,  4.0145, -1.9837]]) 1\n",
      "tensor([[ 0.1511,  1.9019,  5.2315, -0.3191, -2.7267, -1.9857]]) 1\n",
      "tensor([[ 0.4142,  6.9521, -2.0587, -0.6447, -0.9809, -2.9759]]) 1\n",
      "tensor([[-1.6167, -1.1923, -2.8694, -0.9624,  7.8222, -3.5144]]) 1\n",
      "tensor([[-3.7369, -1.6178,  0.4538,  0.1866, -0.3178,  5.8673]]) 1\n",
      "tensor([[-1.4254, -1.5844,  3.8756,  0.6961, -1.3185,  1.9619]]) 1\n",
      "tensor([[ 1.7468, -0.2578, -2.9647,  6.4713, -2.0541, -2.0274]]) 1\n",
      "tensor([[ 3.6047,  1.6702, -2.4458, -3.5062, -2.4015, -1.2769]]) 1\n",
      "tensor([[-0.0526,  1.4730,  6.0167, -1.2133, -1.7848, -1.4239]]) 0\n",
      "tensor([[-2.3872,  0.5704, -2.2085,  1.4078,  3.0603, -0.9628]]) 1\n",
      "tensor([[ 0.6538, -2.2268, -4.1804,  5.8434, -1.0325, -0.4535]]) 1\n",
      "tensor([[ 0.1723,  0.7021,  6.3494, -0.3547, -2.8050, -1.2562]]) 1\n",
      "tensor([[-0.9851, -0.7831, -0.6693, -0.2224, -0.7456,  3.0594]]) 1\n",
      "tensor([[-0.3333, -0.8457,  2.2661,  4.9581, -1.7599, -1.5070]]) 0\n",
      "tensor([[-0.9370,  6.7386, -0.6879, -0.7256,  0.1896, -2.3889]]) 1\n",
      "tensor([[ 0.3495,  1.1725, -2.5752, -0.4213, -1.1324,  0.8365]]) 0\n",
      "tensor([[-1.5477, -1.9479, -3.2329, -0.0338,  6.2961, -3.0804]]) 1\n",
      "tensor([[-3.5913, -0.5495, -0.0526, -0.3962, -0.6909,  6.0460]]) 0\n",
      "tensor([[-0.6851, -1.8226, -3.0132,  1.0633,  2.8937, -1.7256]]) 1\n",
      "tensor([[ 1.1352,  1.7884, -2.0973,  3.8330, -1.8288, -3.0959]]) 1\n",
      "tensor([[-1.3917,  0.8918, -1.6325, -0.8160,  1.4436, -1.1326]]) 0\n",
      "tensor([[-0.2333, -2.0666,  0.3805,  7.7326, -2.3516, -1.1840]]) 1\n",
      "tensor([[-1.2910,  4.8059,  4.5478,  2.4086, -2.6643, -3.0726]]) 0\n",
      "tensor([[-2.1758, -0.8020, -1.6296, -0.7468, -0.0915,  4.4282]]) 0\n",
      "tensor([[-0.6851,  1.4694,  5.4427,  0.1242, -2.1781, -0.4282]]) 0\n",
      "tensor([[-5.7563e-01, -3.5697e-01, -1.8168e+00,  2.3370e-03,  3.6156e+00,\n",
      "         -1.3875e+00]]) 1\n",
      "tensor([[-3.0500, -2.2300, -2.0832,  1.8165,  0.7354,  2.9567]]) 1\n",
      "tensor([[-0.3658, -1.3647, -0.7186,  4.0870, -0.9796, -0.3152]]) 1\n",
      "tensor([[-2.7415, -1.0663, -1.4678,  6.8877,  1.1226, -1.2646]]) 0\n",
      "tensor([[-1.3952, -0.9905,  0.1952,  3.8507, -0.3196, -1.2606]]) 1\n",
      "tensor([[-3.3045,  0.1705, -1.8730, -1.8004,  4.6973,  0.3112]]) 0\n",
      "tensor([[-4.8617, -4.1511, -2.5703,  1.1079, -1.0905, 10.4206]]) 0\n",
      "tensor([[-1.3705, -0.2450,  2.3000,  0.7066, -1.3304,  1.5256]]) 0\n",
      "tensor([[-0.8245,  2.0582,  6.2815, -2.6403,  0.9002, -2.5940]]) 1\n",
      "tensor([[-1.4140, -1.5262, -0.1172,  6.8494, -1.2913, -0.3244]]) 1\n",
      "tensor([[-1.3566, -2.9184, -0.8602,  7.5072, -0.6863, -0.2965]]) 0\n",
      "tensor([[ 0.9264,  6.2105, -2.5275, -0.9928, -1.0324, -3.6163]]) 1\n",
      "tensor([[ 2.3726, -0.6403,  0.9670,  0.9816, -1.1739, -3.1390]]) 0\n",
      "tensor([[-0.7468,  0.8392,  4.2017, -0.2883, -0.8710, -0.1837]]) 0\n",
      "tensor([[ 1.0146,  1.2910,  6.6179, -0.3323, -2.5541, -2.6736]]) 0\n",
      "tensor([[-3.5343, -0.3422, -5.1404, -0.3336,  8.1780, -3.5055]]) 1\n",
      "tensor([[ 0.4325,  2.6134,  9.1387, -1.3630, -4.1446, -1.9978]]) 0\n",
      "tensor([[-1.4849, -0.0904,  2.6610,  0.2983, -0.9721,  0.9147]]) 0\n",
      "tensor([[-1.0787, -0.6694, -2.4966, -0.1975,  4.7712, -1.9017]]) 1\n",
      "tensor([[-1.0485,  1.9627,  2.7817,  3.4997, -1.2126, -2.3351]]) 1\n",
      "tensor([[-3.4030, -1.9300, -2.4995, -1.7495,  1.4834,  5.9343]]) 0\n",
      "tensor([[ 2.7711,  0.9623, -0.9677, -0.1315, -1.8320, -1.8642]]) 1\n",
      "tensor([[ 0.2497,  6.7474,  0.5472, -2.3116, -1.9184, -3.9805]]) 0\n",
      "tensor([[-0.0874,  4.5839,  0.5619, -0.0305, -1.5974, -3.2595]]) 0\n",
      "tensor([[-1.5391,  0.1153,  4.2620, -1.4434, -1.0052,  1.1610]]) 1\n",
      "tensor([[-1.7011, -1.5436, -4.0853,  0.3137,  4.1216,  0.0711]]) 0\n",
      "tensor([[ 0.0217,  7.5610, -0.5514, -1.5127, -0.7201, -3.6107]]) 0\n",
      "tensor([[ 0.2276,  7.3289, -2.1652, -1.7403, -0.8711, -3.4759]]) 0\n",
      "tensor([[ 0.6191,  4.4846, -0.3568,  0.4315, -1.1017, -2.6375]]) 0\n",
      "tensor([[-2.0596,  1.4076,  7.7032,  0.6898, -2.1729, -0.3604]]) 1\n",
      "tensor([[-2.7408, -3.1283, -1.5156,  7.8511,  0.2259,  0.6451]]) 0\n",
      "tensor([[ 0.6953,  6.2201, -2.6890,  0.3103, -0.0372, -4.2232]]) 1\n",
      "tensor([[-2.8276,  0.5686, -2.9535, -0.2196,  7.3364, -2.6510]]) 0\n",
      "tensor([[ 0.6326,  8.7046, -1.7577, -3.3909, -0.1087, -4.3571]]) 0\n",
      "tensor([[-2.2231,  0.1679, -4.1178,  0.5767,  4.2972, -0.5587]]) 0\n",
      "tensor([[-1.2376, -1.3492, -1.4631, -0.0140,  4.0490, -1.2211]]) 1\n",
      "tensor([[-1.6621,  6.8694,  2.6003, -2.4491, -1.0102, -2.6880]]) 1\n",
      "tensor([[-1.5532,  4.3752, -2.6099, -0.5056,  0.5019, -2.1554]]) 1\n",
      "tensor([[-0.3790,  1.7432, -0.7372, -0.7103,  0.7978, -0.5565]]) 0\n",
      "tensor([[ 3.3095, -0.8644, -0.4402,  0.6806, -2.5982, -2.1066]]) 0\n",
      "tensor([[ 6.6965, -2.6767, -3.9654,  5.0582, -3.0097, -4.9358]]) 0\n",
      "tensor([[ 1.7321, -0.0760,  2.7140, -0.4310, -2.4001, -1.0979]]) 1\n",
      "tensor([[ 4.9674,  1.5892, -2.5569, -0.0543, -2.4185, -4.7707]]) 1\n",
      "tensor([[-0.9421, -2.2738, -0.6332,  4.9160, -0.6603, -0.1416]]) 1\n",
      "tensor([[ 0.1396,  7.7494, -1.3787, -0.2208, -1.7863, -4.6152]]) 1\n",
      "tensor([[-0.7600, -0.3272,  6.7404,  1.7224, -2.2028, -0.7507]]) 1\n",
      "tensor([[-2.2393, -1.4465,  1.7094, -1.1691,  2.0595,  1.0228]]) 0\n",
      "tensor([[-0.8556,  1.4620,  5.9149,  1.6883, -3.7718, -0.3074]]) 1\n",
      "tensor([[-2.7984, -2.3220, -0.0055,  1.5984,  0.1586,  3.8061]]) 1\n",
      "tensor([[ 0.0837,  4.0433,  0.7319, -0.4803, -0.5037, -2.2114]]) 1\n",
      "tensor([[ 0.3873,  2.5792,  5.6583, -2.1954, -1.4057, -2.1793]]) 0\n",
      "tensor([[ 0.3010,  6.2481, -1.1460, -2.3022, -0.5673, -3.2221]]) 1\n",
      "tensor([[-0.8340, -2.7382, -2.0893,  6.8183,  0.1867, -1.7906]]) 1\n",
      "tensor([[-2.2451, -1.7885,  0.8885,  7.1365, -1.5131, -0.3505]]) 1\n",
      "tensor([[-1.5978,  4.0013, -0.7933, -1.5991,  0.8767, -1.1069]]) 1\n",
      "tensor([[-2.3125, -0.7091,  1.7945,  0.6611, -0.5654,  2.0099]]) 0\n",
      "tensor([[-2.4921, -1.5556, -3.7960,  0.3071,  7.9223, -3.2306]]) 1\n",
      "tensor([[-1.8968, -1.8575,  2.0658,  6.9525, -1.6621, -0.3138]]) 1\n",
      "tensor([[-0.5290, -2.1471, -0.6490,  7.4546, -0.4559, -2.2179]]) 0\n",
      "tensor([[-1.7428,  1.2391,  8.2237,  0.2841, -3.2035, -0.5501]]) 0\n",
      "tensor([[ 0.1897,  5.5992, -1.4808, -2.9520,  1.3641, -4.2619]]) 1\n",
      "tensor([[ 4.2290,  4.4046, -1.3395, -1.9402, -3.2822, -3.9816]]) 1\n",
      "tensor([[ 1.9838,  3.6131, -1.2216, -2.1939, -0.3826, -3.1843]]) 0\n",
      "tensor([[-0.8694, -0.2342,  4.9052,  1.5941, -1.9071, -0.7943]]) 1\n",
      "tensor([[-1.7817, -0.9305,  5.4870,  1.3505, -1.7840,  1.0482]]) 0\n",
      "tensor([[-2.2874, -0.3583,  5.8073,  1.6768, -0.9306, -0.3590]]) 1\n",
      "tensor([[-1.5055, -0.5370, -0.2664,  0.2258,  1.5517,  0.0544]]) 0\n",
      "tensor([[-0.5136, -1.1077,  1.3121,  4.9920, -1.6883, -0.2885]]) 0\n",
      "tensor([[-3.0610, -1.3647, -2.2093, -1.2190, -1.2696,  7.3626]]) 0\n",
      "tensor([[-1.6991, -3.1538,  1.1984,  4.8769, -0.0432,  0.2106]]) 1\n",
      "tensor([[-3.1930, -1.1984, -3.1366, -0.7162, -0.5853,  6.6691]]) 1\n",
      "tensor([[-1.2433,  2.9072,  4.4470, -0.2026, -2.0387, -1.1539]]) 0\n",
      "tensor([[-0.6753, -1.1672,  0.7460,  3.1033, -1.2953,  0.1311]]) 1\n",
      "tensor([[-1.6081,  2.2594, -1.4150, -1.9059, -1.5804,  2.4853]]) 1\n",
      "tensor([[-1.5442,  1.3885, -2.1555,  1.2905, -0.3599,  0.5039]]) 0\n",
      "tensor([[-0.1530, -2.6420, -2.3259,  4.8285,  0.4800, -0.9889]]) 1\n",
      "tensor([[-2.4077, -2.0341, -2.2382,  0.1309,  3.0990,  1.7959]]) 1\n",
      "tensor([[ 0.7507, -0.7885,  2.3293,  0.6911, -0.7398, -0.8571]]) 0\n",
      "tensor([[ 4.1572,  0.1027, -1.3231,  0.3762, -1.3415, -4.4895]]) 0\n",
      "tensor([[ 0.0715,  3.3409, -0.9639, -0.3572, -1.8678, -0.5187]]) 1\n",
      "tensor([[-2.6572,  1.0389,  2.0260, -2.8506,  1.5631,  1.2841]]) 0\n",
      "tensor([[-0.1924, -0.2261,  2.7755,  2.6644, -1.5146, -1.7139]]) 0\n",
      "tensor([[ 0.4966,  6.8624, -1.9225, -1.9929,  0.9966, -5.9020]]) 0\n",
      "tensor([[-0.9511,  1.3053, -3.3427, -0.8182,  3.1909, -1.3284]]) 0\n",
      "tensor([[-1.3335, -1.0514,  2.8268,  4.8702, -0.2368, -2.2456]]) 0\n",
      "tensor([[-0.9103,  0.5680, -0.6579,  1.3096,  0.0047, -1.3179]]) 0\n",
      "tensor([[-0.0123, -0.5821,  8.2498,  2.7351, -3.1126, -1.6464]]) 1\n",
      "tensor([[-0.1218, -1.4442, -1.5919,  2.9639,  0.1142, -0.1119]]) 1\n",
      "tensor([[ 3.9377, -0.4082, -2.9217, -0.0296, -2.1794, -3.4944]]) 1\n",
      "tensor([[-0.3137,  4.0890, -1.7056, -1.8877,  3.7332, -5.9084]]) 0\n",
      "tensor([[-3.2003, -0.9703,  0.9186, -0.9366,  5.3005, -1.1753]]) 1\n",
      "tensor([[-0.4944,  0.1779, -1.1733,  1.2052, -2.4668,  2.0457]]) 0\n",
      "tensor([[-1.0276,  0.0137, -0.7285, -0.8296, -0.1146,  1.1908]]) 0\n",
      "tensor([[-2.6225,  1.6614, -2.3985,  1.5729, -0.1690,  1.7222]]) 0\n",
      "tensor([[ 4.1452, -1.8291, -4.7145, -0.5846, -2.0992, -0.6281]]) 0\n",
      "tensor([[-3.5342,  0.0901, -0.7251,  0.8830,  5.2584, -1.9184]]) 1\n",
      "tensor([[-2.1840,  0.0881, -0.0767, -0.0924,  3.6559, -1.8717]]) 0\n",
      "tensor([[-0.4007, -1.2811,  6.3422,  0.4628, -2.2377,  0.3101]]) 1\n",
      "tensor([[-1.8756, -3.4675, -0.1184,  9.8782, -1.2372, -0.6436]]) 0\n",
      "tensor([[-2.6667, -1.5619, -0.0981, -0.4285, -1.7714,  6.9370]]) 0\n",
      "tensor([[-0.9336, -3.6199, -1.3281,  7.7597, -0.6321, -0.3565]]) 1\n",
      "tensor([[-1.0347,  1.0471,  5.8820,  0.9040, -0.5677, -2.6725]]) 1\n",
      "tensor([[ 1.3561, -2.0234, -4.1368,  0.2286,  0.2441, -0.4828]]) 0\n",
      "tensor([[-1.8215, -0.7504,  6.4735,  0.8639, -0.7792, -0.7274]]) 0\n",
      "tensor([[-2.5121,  0.0193,  1.5649,  1.7412, -0.0872,  0.2271]]) 1\n",
      "tensor([[-0.0322,  0.1994, -0.7436,  3.9706, -2.2209, -0.1169]]) 1\n",
      "tensor([[-0.5646,  8.9090, -2.4803, -3.4368,  2.6677, -4.7785]]) 0\n",
      "tensor([[-0.4724,  3.8784, -0.1820, -2.1238, -0.0648, -1.4370]]) 0\n",
      "tensor([[-0.4054,  7.3544, -4.6900, -1.9836,  0.0677, -1.6915]]) 0\n",
      "tensor([[-0.2064,  3.4166,  2.0538,  0.7792, -3.8735, -0.5607]]) 0\n",
      "tensor([[-2.3303, -1.6299, -1.6833, -0.2566,  5.0230, -1.5546]]) 0\n",
      "tensor([[-0.4899, -0.3175, -1.6144,  1.4609,  1.3234, -1.1576]]) 0\n",
      "tensor([[-1.1190,  0.1079,  4.9940,  1.3328, -1.9827,  0.3597]]) 1\n",
      "tensor([[ 0.7495,  7.7684,  1.0452, -2.0709, -2.9837, -3.2544]]) 1\n",
      "tensor([[-3.7319, -0.5940,  2.4616, -0.3759, -2.3224,  6.5587]]) 1\n",
      "tensor([[ 0.3033, -0.5697,  7.7420, -0.0677, -2.9187, -1.0166]]) 1\n",
      "tensor([[-2.0172, -1.3805, -1.7295, -0.5444,  5.2991, -1.2611]]) 1\n",
      "tensor([[-0.9084, -4.4042, -2.8372, 10.3214,  0.1652, -1.0665]]) 1\n",
      "tensor([[-2.1917, -0.0129, -3.6070,  0.9870,  3.7840,  0.5680]]) 1\n",
      "tensor([[-1.1130,  1.1460, -3.4744, -1.7010,  6.3567, -4.2589]]) 0\n",
      "tensor([[-1.5605, -3.2974, -0.6628,  8.5980, -0.3348, -1.0225]]) 1\n",
      "tensor([[ 0.0461,  0.7151, -0.2554, -0.5765,  0.6552, -2.2709]]) 0\n",
      "tensor([[-2.7379, -0.5856, -0.6200,  2.0062,  1.7588, -0.3406]]) 0\n",
      "tensor([[-1.3655, -0.0309,  7.0485,  0.8674, -1.7741, -0.7771]]) 1\n",
      "tensor([[-1.6911, -0.5535,  5.8010, -0.2567, -1.2072,  0.6985]]) 1\n",
      "tensor([[ 0.7220,  6.4811, -2.7065, -2.5100, -0.8434, -2.3676]]) 1\n",
      "tensor([[ 3.1634,  0.7902, -1.5781, -1.9350, -0.4258, -2.2864]]) 0\n",
      "tensor([[-0.8610,  5.1351,  5.2446, -1.4726, -2.9373, -2.1153]]) 0\n",
      "tensor([[-0.3808, -0.7336,  2.2811,  5.9470, -1.5649, -2.2755]]) 0\n",
      "tensor([[-0.8789,  2.6128, -0.4336,  1.9881, -1.8937, -0.7774]]) 1\n",
      "tensor([[-2.4130,  4.3342, -1.5513,  0.8986,  0.7056, -0.3742]]) 0\n",
      "tensor([[-0.8907, -0.0221,  4.4832,  0.4288, -1.9786, -0.0835]]) 1\n",
      "tensor([[ 0.6065,  0.3559,  7.6828,  0.4687, -3.4631, -1.5901]]) 1\n",
      "tensor([[-1.2924, -1.3966,  2.4318,  5.4502, -0.7992, -1.0951]]) 1\n",
      "tensor([[-4.4168, -4.1497, -2.7491, -0.5404, -0.7356, 10.7510]]) 0\n",
      "tensor([[-0.4981, -1.5772,  1.9299,  4.1147, -2.1614, -0.1444]]) 0\n",
      "tensor([[ 1.0516, 10.4996, -2.3139, -1.2492, -1.4169, -6.5324]]) 1\n",
      "tensor([[-1.9832, -4.5066, -0.0783,  6.5106, -0.8629,  2.4580]]) 0\n",
      "tensor([[-3.1474,  0.4946, -3.1752, -0.1535,  4.5358, -0.9033]]) 1\n",
      "tensor([[-1.7194, -3.6375, -1.7635,  6.8991,  0.4040,  0.9154]]) 1\n",
      "tensor([[-1.7256, -1.6805,  0.4707,  4.3887, -1.1047,  1.2807]]) 1\n",
      "tensor([[-1.9218, -1.8251, -3.2544, -0.6153,  5.8873, -1.2893]]) 1\n",
      "tensor([[-2.6460, -2.0172, -3.1769, -1.2623,  5.7241,  0.7484]]) 1\n",
      "tensor([[-1.3689, -0.9577, -2.6991,  0.4367,  3.6847, -0.4288]]) 1\n",
      "tensor([[ 2.7853, -0.2014, -1.0915, -1.1706, -1.4990, -1.1644]]) 1\n",
      "tensor([[-4.3741, -1.2522, -1.1212, -0.8300,  0.7525,  6.4902]]) 1\n",
      "tensor([[ 0.9093,  4.5425, -3.8364, -0.0298,  1.5057, -5.3453]]) 0\n",
      "tensor([[-2.3052, -2.0416,  2.7972,  1.1536,  0.2489,  1.5449]]) 0\n",
      "tensor([[-1.5093,  7.3727,  0.7724, -0.9933, -1.2814, -2.7773]]) 1\n",
      "tensor([[-0.7284, -2.3640, -3.4805,  6.7409,  0.3766, -0.9478]]) 1\n",
      "tensor([[ 1.2193,  0.9543,  4.8049,  0.2378, -3.4570, -1.9525]]) 1\n",
      "tensor([[-3.6697, -0.3841, -1.1121, -0.3441, -0.1593,  3.9875]]) 1\n",
      "tensor([[-0.5873, -1.5241, -2.3764,  6.3343, -0.1298, -2.0965]]) 1\n",
      "tensor([[-1.0404, -2.3204, -1.2420,  5.8424, -0.0451, -0.1587]]) 1\n",
      "tensor([[-2.9417, -2.1226,  0.6678,  0.3684, -1.3143,  6.3639]]) 1\n",
      "tensor([[-1.3010,  1.6547, -0.1337,  2.4856, -2.2569,  0.3218]]) 0\n",
      "tensor([[-1.8176, -2.6074, -2.1183,  1.8005,  4.1602, -0.7279]]) 1\n",
      "tensor([[-0.4910,  1.9646, -2.7563, -0.2466,  4.1748, -3.8884]]) 0\n",
      "tensor([[-1.4298, -0.7647,  4.3289,  0.5365, -2.5372,  2.2672]]) 0\n",
      "tensor([[-1.4923, -3.1569, -2.3278,  3.4582,  4.4204, -1.5014]]) 0\n",
      "tensor([[-0.7104, -1.1390,  1.3989,  1.8720,  0.2110, -0.4475]]) 1\n",
      "tensor([[-3.3024,  2.1872, -4.9851, -0.9061,  6.0610, -1.8240]]) 1\n",
      "tensor([[-2.6719, -0.2790,  2.9955, -2.1334,  4.5853, -2.9635]]) 1\n",
      "tensor([[-1.1571,  0.7335,  3.0338,  0.6257,  0.1291, -2.1420]]) 0\n",
      "tensor([[-2.6365,  0.3006,  0.6701,  2.6115,  0.9241, -0.8687]]) 0\n",
      "tensor([[-3.4935, -1.8826,  1.6725, -1.2738, -0.7579,  5.7931]]) 1\n",
      "tensor([[-2.6676, -2.3965, -4.5279, -0.9520,  8.2371, -0.3989]]) 1\n",
      "tensor([[-1.3648, -2.3282,  1.7536,  6.5807, -1.2960, -0.4223]]) 1\n",
      "tensor([[-1.8488, -0.9521, -1.6560, -0.8648,  4.0961,  0.0044]]) 0\n",
      "tensor([[-0.3100, -2.8416, -1.0540,  7.2642, -0.8208, -1.1474]]) 1\n",
      "tensor([[-0.4359,  0.8082,  8.9805,  0.2295, -2.4414, -1.3224]]) 0\n",
      "tensor([[-2.3158,  0.1306,  7.7237,  2.1133, -2.4878, -0.1980]]) 1\n",
      "tensor([[-1.1950, -0.9382, -3.0174,  1.6422,  4.4870, -1.7442]]) 1\n",
      "tensor([[ 0.5362, -0.8457, 10.0485, -1.0349, -2.5040, -2.0793]]) 1\n",
      "tensor([[ 0.0521,  5.1576, -1.4161, -2.1142, -1.8556, -1.0435]]) 1\n",
      "tensor([[-2.1612,  2.3145, -2.4972, -2.1008,  4.6319, -1.4041]]) 1\n",
      "tensor([[-3.8812,  1.2406, -1.2692, -1.6552,  2.4652,  2.4038]]) 0\n",
      "tensor([[-1.3821, -1.2852, -2.0032,  3.3943,  0.3976,  0.5207]]) 0\n",
      "tensor([[-2.9979, -1.8028, -5.4492, -0.8982,  8.4279, -2.9382]]) 1\n",
      "tensor([[ 1.9963,  2.1263, -2.6281,  0.2031, -1.1282, -3.1237]]) 1\n",
      "tensor([[-2.0553, -2.6249,  0.8288,  2.3260, -1.3183,  2.5838]]) 1\n",
      "tensor([[ 3.4408,  4.0248,  0.7376, -3.0233, -1.0434, -4.8266]]) 0\n",
      "tensor([[ 2.9973,  1.4649, -2.4394, -2.8163, -0.5062, -2.8421]]) 0\n",
      "tensor([[-0.5305, -3.0343, -2.2676,  6.9269,  0.5243, -1.4774]]) 1\n",
      "tensor([[-1.9222,  1.6632,  2.4317,  2.2298, -0.5036, -0.9121]]) 0\n",
      "tensor([[ 0.3151,  5.5497, -3.8163, -3.6547,  2.1559, -4.7286]]) 1\n",
      "tensor([[-1.2822,  4.1973, -1.6185,  1.7726, -0.5174, -2.7027]]) 0\n",
      "tensor([[-2.7515, -0.9077,  0.7208, -0.4081,  1.1739,  2.4632]]) 0\n",
      "tensor([[ 0.0622,  0.3429, -1.3948, -1.5962, -1.2660,  0.8486]]) 0\n",
      "tensor([[ 0.6276, -1.2963,  0.7930,  0.4483, -0.6774, -0.7378]]) 0\n",
      "tensor([[ 0.2237,  0.2438,  0.5821,  1.7834,  0.2413, -3.1399]]) 0\n",
      "tensor([[-1.4400,  1.5075,  6.1197, -0.7705, -2.2205,  0.6245]]) 0\n",
      "tensor([[-0.4710,  0.0893,  1.1734,  4.4182, -0.8060, -2.3422]]) 1\n",
      "tensor([[ 0.3155,  2.0912,  1.4897,  4.4887, -3.0810, -3.5625]]) 0\n",
      "tensor([[-4.8508,  1.4045, -1.1838, -1.1650,  6.2550, -0.9626]]) 0\n",
      "tensor([[-4.2049, -2.3198, -2.4377,  2.0012,  1.9031,  4.4762]]) 0\n",
      "tensor([[ 0.2406, -0.1867,  2.8033, -0.8190, -0.1898, -1.2988]]) 1\n",
      "tensor([[ 0.0810, -3.0514, -3.2817, -0.5645,  4.6048, -1.6230]]) 0\n",
      "tensor([[-3.0084, -2.2559, -1.6729,  0.6099,  2.9971,  1.2218]]) 1\n",
      "tensor([[-0.8945, -0.4961, -0.9832,  4.1856, -0.6336, -0.8040]]) 1\n",
      "tensor([[ 1.2111,  5.2838, -0.5746, -0.3985, -0.4945, -4.5289]]) 1\n",
      "tensor([[-0.9441,  6.3644,  0.7821, -2.9970, -0.1663, -2.1625]]) 0\n",
      "tensor([[ 0.0140, -0.4162,  4.8109,  0.9767, -2.2610, -1.1135]]) 1\n",
      "tensor([[ 0.1859,  0.5156,  6.8083,  0.8965, -1.9766, -3.3759]]) 0\n",
      "tensor([[-0.8088, -0.5153, -0.8796, -1.0047,  4.8530, -2.3271]]) 0\n",
      "tensor([[-2.3233,  0.4204, -0.9882, -2.2708,  2.7632,  0.8960]]) 1\n",
      "tensor([[-1.3615,  0.6699,  6.7651,  0.3800, -2.2892, -0.3658]]) 0\n",
      "tensor([[-1.8133,  1.0058,  1.8193, -1.7945, -3.0833,  4.3950]]) 0\n",
      "tensor([[-0.4225,  2.5524,  4.5780,  0.2665, -3.0776, -0.4838]]) 1\n",
      "tensor([[ 0.0173,  8.2995,  0.0308, -1.0058, -1.4146, -4.3482]]) 1\n",
      "tensor([[-2.1954,  0.1795, -1.5406,  0.0685, -0.6277,  3.6762]]) 0\n",
      "tensor([[-0.7001,  1.8114,  1.9907,  0.0175, -2.4095,  1.0598]]) 0\n",
      "tensor([[ 1.5036, -0.4130,  2.6340,  0.1511, -2.0916, -1.5043]]) 0\n",
      "tensor([[-2.4577, -1.5718, -0.5048,  4.8901, -0.5303,  1.0880]]) 0\n",
      "tensor([[-0.4513, -0.2334,  0.5134, -0.4948, -1.0534,  1.7097]]) 0\n",
      "tensor([[ 0.2356, -1.3397, -0.8136,  2.7753, -0.1616, -0.7931]]) 0\n",
      "tensor([[ 4.2017,  0.7690, -1.4710,  0.6966, -2.8166, -2.5906]]) 1\n",
      "tensor([[-2.3310, -0.2822, -1.8149,  1.0717,  1.9139,  1.1907]]) 0\n",
      "tensor([[-1.4711, -1.8318,  1.5110,  8.5243, -0.7502, -2.4469]]) 1\n",
      "tensor([[-4.8469, -1.0220, -0.6089, -1.4973,  0.7461,  5.4387]]) 1\n",
      "tensor([[-1.8327, -0.5416, -4.2423,  1.4050,  3.2102, -0.5094]]) 1\n",
      "tensor([[-1.0818, -1.9019,  3.3456,  5.0560, -1.9853, -0.9876]]) 0\n",
      "tensor([[-1.8908, -2.2510, -1.9338,  4.9205, -0.3554,  1.5274]]) 1\n",
      "tensor([[-0.9718, -2.2726, -0.9520,  7.9171, -1.3147, -1.0408]]) 1\n",
      "tensor([[ 4.2611,  0.5092, -3.7852,  2.8815, -2.1438, -4.8151]]) 1\n",
      "tensor([[-0.2026,  3.2376,  3.3235,  1.0528, -3.3965, -2.5103]]) 1\n",
      "tensor([[ 0.5594, 11.3590, -0.5015, -3.0120, -0.8301, -5.2215]]) 0\n",
      "tensor([[ 0.7148,  0.0052,  2.1031,  0.6420, -1.0077, -1.3338]]) 0\n",
      "tensor([[-1.8125, -1.8043, -3.6633,  0.7391,  6.8674, -3.2586]]) 1\n",
      "tensor([[ 1.4844,  6.0967, -0.4025, -1.0553, -1.9923, -4.2080]]) 1\n",
      "tensor([[-1.2802, -0.3222,  3.3793, -0.7070,  0.7226, -0.1252]]) 1\n",
      "tensor([[ 0.9960,  4.4929,  0.7820, -0.2753, -1.1029, -4.1669]]) 0\n",
      "tensor([[-3.1123, -2.0885, -1.0222, -0.3428, -0.7313,  6.9173]]) 0\n",
      "tensor([[-0.0178, -1.1615,  2.2776,  7.1453, -2.8307, -1.7670]]) 0\n",
      "tensor([[-0.5473,  4.6675, -0.0938, -1.8765, -0.8106, -0.9675]]) 0\n",
      "tensor([[ 1.3356, -4.0853, -2.6787,  6.3285, -0.5880, -1.6865]]) 0\n",
      "tensor([[ 4.6307, -0.0616, -3.4971, -0.6440, -0.5139, -7.5065]]) 1\n",
      "tensor([[-2.8233, -0.0529, -3.5432,  0.6147, -0.4525,  4.9733]]) 0\n",
      "tensor([[-0.4899, -2.7366, -1.1893,  5.2907,  0.1164, -0.5623]]) 0\n",
      "tensor([[ 0.8486,  5.3945,  1.6029, -2.1224, -0.3080, -5.2995]]) 0\n",
      "tensor([[-0.1571,  8.1084,  0.2455, -1.1147, -0.4215, -3.8654]]) 1\n",
      "tensor([[ 0.5265, -1.4752,  1.9090,  4.5271, -1.6384, -1.9411]]) 1\n",
      "tensor([[-1.2188, -2.2126,  0.2353,  7.2710, -1.4419, -0.2512]]) 1\n",
      "tensor([[-2.1141, -2.3244, -0.3449, -1.2190,  5.4336, -0.3991]]) 0\n",
      "tensor([[-0.8624,  1.0334,  4.6938, -0.1118, -2.7146,  0.3388]]) 0\n",
      "tensor([[-1.9878, -0.7751, -0.7266,  0.0992,  1.2524,  1.8302]]) 0\n",
      "tensor([[-2.1163, -2.4393, -0.8997, -0.1105,  4.3882, -0.1050]]) 1\n",
      "tensor([[ 2.2992,  3.9344, -0.3423, -1.3296, -1.1808, -4.9279]]) 1\n",
      "tensor([[-1.0810, -1.9482, -2.1853, -0.6569,  5.3432, -2.8783]]) 1\n",
      "tensor([[-0.5363, -0.6741, -1.0093,  4.6742, -0.8617, -0.0685]]) 1\n",
      "tensor([[-0.8808, -1.5106,  0.3697,  3.7458, -0.0138, -1.1120]]) 0\n",
      "tensor([[ 0.9229,  6.2192, -0.8384, -2.7675, -0.2887, -3.9878]]) 0\n",
      "tensor([[-0.6552, -0.6829, -3.1325,  0.0972,  5.5069, -3.7467]]) 0\n",
      "tensor([[ 0.3569,  3.0629,  7.2627, -1.6663, -3.7558, -2.3655]]) 0\n",
      "tensor([[ 0.0923,  1.7377,  5.1639,  0.4494, -1.2167, -3.7849]]) 0\n",
      "tensor([[ 0.4987, -0.9371,  1.0342,  2.6515, -1.1427, -1.0131]]) 0\n",
      "tensor([[-2.2472,  1.1866, -1.4813, -2.3473,  5.4031, -1.0986]]) 1\n",
      "tensor([[ 3.4253, -0.5232, -1.9336, -0.1181, -1.6322, -1.3692]]) 0\n",
      "tensor([[ 0.4190,  0.4152,  0.6816,  2.4423, -1.2133, -1.5717]]) 0\n",
      "tensor([[-0.3056,  3.3574, -1.2430, -1.1218,  0.0222, -2.1423]]) 1\n",
      "tensor([[-1.3013, -4.4101, -2.4460, 12.3228, -1.3230, -0.9871]]) 1\n",
      "tensor([[-1.4946,  0.8367,  7.6791, -0.5701, -1.2768, -1.1497]]) 0\n",
      "tensor([[-4.0055,  0.0122, -2.7591, -0.5870,  0.2482,  5.5357]]) 1\n",
      "tensor([[-1.2606e+00, -2.2422e-01,  5.4286e+00,  2.1410e-03, -1.9009e+00,\n",
      "          1.3213e+00]]) 1\n",
      "tensor([[-5.0069, -1.8177, -0.9294, -1.1513,  0.1224,  7.8574]]) 1\n",
      "tensor([[-2.0837, -1.2063,  1.4448,  9.7343, -2.1947, -1.4705]]) 1\n",
      "tensor([[-2.0935, -1.5804, -2.8813,  1.2045,  4.5766, -0.9083]]) 1\n",
      "tensor([[-3.6193, -2.2535, -5.5449,  0.3686,  8.8446, -0.5453]]) 1\n",
      "tensor([[-0.5829, -2.5138,  1.8085,  6.5146, -1.3679, -1.1166]]) 1\n",
      "tensor([[-2.4927, -1.0195,  6.1586,  0.2932, -1.6221,  2.3719]]) 0\n",
      "tensor([[ 1.7070,  5.8590, -1.0500, -2.5488, -1.3986, -3.8422]]) 0\n",
      "tensor([[-1.0169, -3.0032, -3.3420,  6.5596, -0.3279, -0.1839]]) 0\n",
      "tensor([[ 1.1840,  3.0273,  8.5940, -0.8683, -4.7970, -3.3373]]) 0\n",
      "tensor([[-1.2974,  0.6026,  1.2151, -2.2579, -1.1383,  2.7889]]) 0\n",
      "tensor([[-2.7134,  0.2556,  0.7987, -2.7998,  3.0529,  1.4115]]) 0\n",
      "tensor([[-1.6619,  1.3327, 10.6568, -1.3563, -2.8371,  0.7506]]) 1\n",
      "tensor([[-1.0491, -1.7947, -1.6081,  1.7254,  4.8763, -2.3076]]) 0\n",
      "tensor([[-0.2245, -0.1871, -1.6767,  3.9911, -0.7655, -1.0492]]) 1\n",
      "tensor([[-0.3577,  2.2397,  6.7665, -0.5634, -2.4486, -2.1392]]) 0\n",
      "tensor([[-0.1543, -0.1486,  4.3721,  0.1737, -1.1634,  0.3577]]) 1\n",
      "tensor([[-1.3570,  5.3653,  2.0973, -1.3940, -1.5162, -2.0878]]) 1\n",
      "tensor([[-0.3511,  0.0440,  5.9791, -0.2512, -1.1566, -1.2977]]) 1\n",
      "tensor([[-0.2205, -1.4739, -0.5189,  4.7807, -1.4286, -0.4229]]) 0\n",
      "tensor([[ 2.1244,  1.3715, -0.5911, -1.0848, -2.3218, -1.1814]]) 1\n",
      "tensor([[-1.5935, -1.3421, -1.3296,  1.7369,  4.6198, -2.0907]]) 1\n",
      "tensor([[-1.1465,  0.1031,  6.3720, -0.0730, -1.9996, -0.6527]]) 0\n",
      "tensor([[-2.2132, -1.6978, -1.4464,  5.7075, -0.0444,  0.5975]]) 1\n",
      "tensor([[ 0.5235, -0.2595,  5.0369,  0.6610, -2.7077, -1.0898]]) 1\n",
      "tensor([[-2.0934,  6.5923,  1.1446, -2.3385, -0.7650, -1.0737]]) 1\n",
      "tensor([[ 1.1921,  4.3466,  3.5082, -0.7749, -2.8364, -3.5965]]) 0\n",
      "tensor([[-0.1893, -0.3012, -1.7558,  4.6460,  0.0964, -1.9431]]) 1\n",
      "tensor([[-2.0085, -3.7631, -0.7692,  7.6757,  0.1573,  0.5292]]) 1\n",
      "tensor([[-1.6596, -1.4882,  0.0550,  7.7959,  0.7559, -2.9046]]) 1\n",
      "tensor([[-2.7315e+00,  6.2477e-02, -1.3578e+00, -2.2055e-03,  4.2107e+00,\n",
      "         -8.0373e-01]]) 0\n",
      "tensor([[-0.0252, -0.8199, -0.3618,  2.7323, -0.7404, -1.0828]]) 0\n",
      "tensor([[-1.3989, -0.7004,  3.8721,  0.2062, -0.5713, -0.4817]]) 0\n",
      "tensor([[-4.0112, -1.9668, -2.9225,  0.6377,  6.2269,  0.7373]]) 0\n",
      "tensor([[-0.6667,  4.9037, -0.7602, -2.0379, -0.2147, -1.6382]]) 1\n",
      "tensor([[-0.9369,  1.1406, -3.1814, -0.6072,  4.2478, -2.6942]]) 1\n",
      "tensor([[-0.3446,  0.8160,  5.3979,  0.5998, -1.1133, -1.9794]]) 0\n",
      "tensor([[-0.0295,  0.4972,  3.2421,  2.1556, -2.1295, -0.6980]]) 0\n",
      "tensor([[-0.6876,  2.2705, -2.8462, -0.8136,  4.6611, -3.7677]]) 1\n",
      "tensor([[ 3.9328, -1.3830, -1.6204, -1.6263, -0.2891, -0.7915]]) 1\n",
      "tensor([[ 2.1226, -0.4504,  0.2423,  3.0421, -3.1526, -1.9127]]) 0\n",
      "tensor([[-1.2286, -1.0640,  7.2040,  2.8391, -3.0956, -0.7544]]) 0\n",
      "tensor([[ 2.4684, -1.7461, -1.5720,  2.2860,  0.1252, -3.4189]]) 1\n",
      "tensor([[ 0.1619, -0.2770, -2.0059, -0.4732,  0.7752, -0.1093]]) 0\n",
      "tensor([[ 4.8943, -1.7149, -1.7927, -0.9657, -1.3817, -3.1103]]) 0\n",
      "tensor([[-1.9315, -0.0805, -0.8875, -0.2100, -0.3629,  2.5300]]) 0\n",
      "tensor([[-1.1560,  1.2320,  5.8469,  1.4392, -2.1220, -1.1421]]) 0\n",
      "tensor([[ 1.8899,  8.9464, -0.5621, -1.6608, -2.8449, -5.3731]]) 0\n",
      "tensor([[-0.2290,  3.5909,  0.1403,  3.7312, -1.8281, -3.7901]]) 1\n",
      "tensor([[-4.1094, -0.1556,  0.7565, -1.2223,  1.6181,  4.0962]]) 0\n",
      "tensor([[ 0.4812,  6.4833, -1.3286, -2.6614, -0.0444, -3.0665]]) 0\n",
      "tensor([[-1.6147, -1.4459,  6.5990,  3.1348, -1.4167, -1.0204]]) 1\n",
      "tensor([[-3.8032e-03,  4.5041e-01,  9.1763e+00,  1.8389e-01, -2.2350e+00,\n",
      "         -2.3745e+00]]) 1\n",
      "tensor([[-3.3837, -2.1497, -1.8041, -0.5229, -1.1370,  8.5001]]) 1\n",
      "tensor([[-0.9725, -0.9884, -2.9642, -0.3781,  3.8939, -2.3647]]) 1\n",
      "tensor([[-1.3681, -1.7495, -1.9773, -0.5279, -0.4183,  4.5121]]) 1\n",
      "tensor([[-1.7723,  0.1484, -2.4100, -0.2988, -1.4055,  4.5748]]) 1\n",
      "tensor([[ 0.1657, -0.4389,  4.3054,  2.3461, -1.3833, -2.7143]]) 1\n",
      "tensor([[-2.9005, -1.2470, -0.2844, -0.4771,  0.0325,  5.1606]]) 0\n",
      "tensor([[ 0.0132,  8.2460,  0.2015, -1.0313, -1.5665, -4.5209]]) 1\n",
      "tensor([[-2.5397, -0.0657, -1.4115,  0.1072, -0.9594,  3.5448]]) 1\n",
      "tensor([[-0.6881, -0.8250,  3.4975, -0.6704, -0.4687,  0.5140]]) 0\n",
      "tensor([[-1.6063, -1.1064, -1.8254, -0.1781, -1.1044,  4.2803]]) 1\n",
      "tensor([[ 0.2365,  1.8225,  3.7636, -1.5380, -1.3680, -1.5116]]) 0\n",
      "tensor([[-2.4445, -0.9602,  8.1889, -0.6881, -2.3900,  2.2505]]) 1\n",
      "tensor([[ 3.5916,  0.0100, -1.2924, -0.2593, -2.6377, -2.0455]]) 1\n",
      "tensor([[-1.1730, -2.1837,  3.3064,  5.5590, -1.2718, -1.1804]]) 0\n",
      "tensor([[-0.1287, -0.5541, -1.7051, -0.6725,  3.2871, -2.7079]]) 0\n",
      "tensor([[-1.2492, -0.6220, -3.0162,  1.3385, -1.4652,  3.5776]]) 0\n",
      "tensor([[-1.5465,  1.7611,  1.2834, -0.5136,  0.8753, -0.6087]]) 0\n",
      "tensor([[-2.1285, -1.0279, -0.5149,  1.3503,  1.3172,  1.0086]]) 0\n",
      "tensor([[-2.6763,  0.7187,  5.5304,  0.0177, -3.1419,  2.3345]]) 1\n",
      "tensor([[ 3.5693, -0.9802, -3.9625, -0.9565,  0.0675, -3.1366]]) 0\n",
      "tensor([[-2.6701,  0.6885,  2.7799, -0.8228, -2.8564,  4.4799]]) 0\n",
      "tensor([[-2.1009, -2.4036,  0.1523,  8.0303, -0.8755, -0.7838]]) 1\n",
      "tensor([[-1.9511, -0.9652, -1.2270, -0.0749,  3.5398, -0.6275]]) 0\n",
      "tensor([[ 3.1996, -2.1638, -2.8472,  3.2805, -0.8096, -3.4023]]) 1\n",
      "tensor([[-0.6845,  5.2837, -3.0943, -1.7827, -0.3593, -2.0269]]) 1\n",
      "tensor([[-1.4692,  3.2032, -0.3512,  3.4727, -1.1045, -2.1085]]) 1\n",
      "tensor([[-2.6483,  0.4560, -3.9378,  0.1146,  6.0958, -2.8801]]) 0\n",
      "tensor([[-0.0242, -2.4098, -2.0774,  3.0785,  2.3062, -2.4271]]) 1\n",
      "tensor([[ 0.5717, -0.7486, -3.9469,  6.8289,  0.6328, -3.8908]]) 1\n",
      "tensor([[-3.2721, -0.5461, -0.7589, -0.7030, -1.3698,  6.2015]]) 1\n",
      "tensor([[-0.0985, -1.1742, -2.4113,  0.5202,  5.1983, -5.3095]]) 1\n",
      "tensor([[-0.8674,  1.3708,  5.7862, -1.0298, -1.6992, -0.1248]]) 1\n",
      "tensor([[-0.4940, -1.3977, -1.6258, -0.4755,  3.1039, -0.7773]]) 0\n",
      "tensor([[ 3.8227, -0.3536, -2.9104, -0.8749, -1.8905, -1.6116]]) 1\n",
      "tensor([[ 0.6937, -0.3492,  2.7410, -0.8858,  0.8144, -2.8056]]) 0\n",
      "tensor([[-2.8675,  6.6009,  2.6920, -2.9549,  0.7588, -2.4773]]) 0\n",
      "tensor([[ 0.3844, -1.5172,  0.2846,  4.4758, -1.9319, -1.8048]]) 1\n",
      "tensor([[ 0.7530,  4.3082,  0.6054, -2.4232, -1.3170, -2.1599]]) 0\n",
      "tensor([[-2.5856, -0.6452,  0.3825,  0.7945, -2.5220,  4.6055]]) 0\n",
      "tensor([[ 0.8197,  6.4831, -0.0981, -2.4750, -1.8675, -3.1539]]) 1\n",
      "tensor([[-0.8766, -0.6675, -0.3424,  5.4909, -1.5959, -1.1955]]) 1\n",
      "tensor([[-1.5059,  0.0424,  2.1219,  3.6976, -2.1254,  0.4762]]) 0\n",
      "tensor([[-1.8317, -0.8946, -2.1356, -0.0540, -3.0582,  4.9880]]) 0\n",
      "tensor([[-0.5686, -3.1405, -2.2749,  7.4654,  0.0377, -1.6822]]) 0\n",
      "tensor([[-0.9917,  5.1729,  0.6015, -1.7924, -0.3601, -0.8257]]) 1\n",
      "tensor([[-0.1435, -4.4613, -1.3647,  2.0869, -1.6511,  4.0643]]) 1\n",
      "tensor([[-0.6586,  3.4021,  7.5390, -2.2760, -1.7580, -2.3955]]) 1\n",
      "tensor([[-3.0398, -1.7529, -0.3632,  3.8215,  2.3667, -0.6217]]) 0\n",
      "tensor([[-0.0894, -1.7389, -0.9441,  4.7469, -0.4816, -1.4537]]) 1\n",
      "tensor([[-0.6093,  1.0028,  5.6296,  1.1690, -1.5644, -2.3403]]) 0\n",
      "tensor([[-4.5175, -0.0966, -2.0287, -1.2552, -2.4492,  9.0732]]) 0\n",
      "tensor([[-0.0946, -1.0978,  4.7522,  5.9183, -3.6759, -2.3181]]) 1\n",
      "tensor([[-1.7964, -0.4202, -2.6223,  0.9022,  4.0678, -1.5094]]) 1\n",
      "tensor([[-0.4724, -0.7469, -0.7892, -0.8080,  0.3783,  1.3367]]) 1\n",
      "tensor([[ 3.9687, -1.9725, -2.5711, -0.4426,  1.0643, -4.0313]]) 1\n",
      "tensor([[-1.4411,  8.4869,  0.4640, -2.5162,  0.5167, -3.8636]]) 1\n",
      "tensor([[ 4.2399,  0.5362, -3.2983, -0.1740, -2.0180, -3.5770]]) 1\n",
      "tensor([[-1.0137,  4.8810,  0.8877, -0.7357, -1.8031, -0.8917]]) 0\n",
      "tensor([[-1.4590,  0.4752, -1.5589, -0.9132,  1.2268,  1.1389]]) 0\n",
      "tensor([[-1.4330, -1.6993,  1.0744, -0.6061, -0.8223,  4.6935]]) 0\n",
      "tensor([[ 4.7051, -1.2531,  0.9868,  0.0432, -0.9097, -4.6458]]) 1\n",
      "tensor([[-1.3042, -1.8644,  1.1616,  6.3259, -1.8025, -0.3857]]) 0\n",
      "tensor([[-0.6553, -0.0472, -2.7918,  1.1286, -3.2398,  3.6150]]) 1\n",
      "tensor([[-2.6750, -1.8349, -3.8209,  0.6871,  4.8633,  0.9971]]) 1\n",
      "tensor([[ 2.0774,  1.4403,  0.1577, -1.8511, -1.7481, -2.2938]]) 0\n",
      "tensor([[ 0.3395,  2.5454,  7.9085, -1.1878, -3.5280, -2.0858]]) 1\n",
      "tensor([[-4.6625, -1.7908, -1.0210,  0.0195,  0.0800,  7.4327]]) 1\n",
      "tensor([[-2.8785, -2.2395, -1.9075, -0.8316,  1.3215,  3.7633]]) 0\n",
      "tensor([[-2.3964,  0.3155, -4.3784, -1.1967,  7.5413, -2.7368]]) 1\n",
      "tensor([[-0.1741,  2.6837,  4.0976, -0.5057, -0.7396, -2.4551]]) 0\n",
      "tensor([[-2.0711,  0.0618, -2.1591,  5.5197, -0.6280,  0.0909]]) 0\n",
      "tensor([[ 0.9594,  3.6192, -1.7696, -1.5930, -0.7943, -2.9059]]) 0\n",
      "tensor([[-0.6231,  1.4903, -0.9014,  2.7160, -1.5198, -0.7205]]) 0\n",
      "tensor([[-2.7085, -0.5158, -3.5829, -1.5224,  5.4417, -0.8955]]) 1\n",
      "tensor([[ 3.8279, -0.3037, -1.6443, -0.9711, -0.7705, -3.0950]]) 1\n",
      "tensor([[-2.6256, -1.5339, -0.3736,  5.9631,  1.3517, -0.3283]]) 0\n",
      "tensor([[-2.1789, -3.3060, -5.0399,  0.1620,  5.5196,  1.5313]]) 1\n",
      "tensor([[ 4.3459, -0.8604, -1.8331, -0.6900, -1.7301, -3.0192]]) 1\n",
      "tensor([[-2.5606,  0.1464, -1.7286, -1.3862,  4.0964,  0.5404]]) 1\n",
      "tensor([[-1.4267,  4.3945, -0.9302,  0.6950, -0.2538, -1.6119]]) 1\n",
      "tensor([[-1.0752,  6.5298,  0.4478, -0.1791, -1.4999, -1.9216]]) 1\n",
      "tensor([[-1.2086,  5.1341, -1.5613, -1.9920,  1.1911, -2.5100]]) 1\n",
      "tensor([[-1.1496, -2.7628,  0.0502,  7.8285, -0.0902, -1.2566]]) 1\n",
      "tensor([[-1.3357,  1.5259, -1.8961,  1.4051, -0.3587, -0.1191]]) 0\n",
      "tensor([[ 2.4128, -1.1485,  3.2030,  0.6686,  0.1114, -4.8433]]) 0\n",
      "tensor([[-1.8463, -0.7179,  4.2411,  4.5830, -1.8650, -0.6281]]) 1\n",
      "tensor([[ 0.7273,  0.0334,  5.2920, -0.4049, -1.9821, -1.8431]]) 1\n",
      "tensor([[-1.8145,  1.8322,  0.4530, -0.2690, -0.9796,  1.0220]]) 0\n",
      "tensor([[-1.5193, -2.5747,  0.8704,  6.8509, -0.8843, -0.0363]]) 1\n",
      "tensor([[ 0.6077,  2.6273,  3.1307,  0.7646, -1.8967, -4.2794]]) 1\n",
      "tensor([[-0.4310,  0.1478, -3.0158,  6.1246,  0.7164, -3.0199]]) 1\n",
      "tensor([[ 0.1731,  1.0257, -1.2186,  3.3860, -1.1527, -0.7929]]) 0\n",
      "tensor([[-0.7572, -1.9675,  1.3837,  7.7097, -1.7201, -1.1904]]) 1\n",
      "tensor([[ 3.9696,  0.0698, -0.4800, -0.0507, -1.6981, -2.2416]]) 1\n",
      "tensor([[-3.0937,  0.9690, -1.2235,  1.1036,  3.8914, -2.5128]]) 0\n",
      "tensor([[-0.7883,  1.3931,  5.4730, -0.7767, -2.0853, -0.1845]]) 1\n",
      "tensor([[ 0.5854,  2.0010,  1.2158,  0.2141, -1.9487, -2.5161]]) 0\n",
      "tensor([[-1.6669, -0.9044,  2.0203,  0.2839, -0.9722,  2.5145]]) 0\n",
      "tensor([[-0.4813, -0.7888, -3.2161,  1.0330,  2.2449, -1.1586]]) 0\n",
      "tensor([[-1.2051,  0.8583,  1.8526,  4.8688, -1.2935, -1.6680]]) 1\n",
      "tensor([[ 0.6870,  6.3318, -0.1773, -2.2241, -1.9366, -2.3478]]) 1\n",
      "tensor([[-0.2862, -3.4283, -1.5378,  1.6816, -0.8111,  3.6386]]) 1\n",
      "tensor([[-3.1687, -2.0577, -2.2584,  0.0315,  1.1932,  7.3989]]) 1\n",
      "tensor([[-3.7068, -1.4885, -3.3229,  1.1150,  0.2171,  5.5906]]) 1\n",
      "tensor([[ 0.2530, -3.5274, -2.0367,  6.7654, -1.4259, -0.6603]]) 0\n",
      "tensor([[-0.5872, -0.7068,  8.8060,  0.6520, -2.3810, -1.3846]]) 0\n",
      "tensor([[ 3.2945, -2.9779,  2.9798,  3.2382, -1.6632, -5.2580]]) 1\n",
      "tensor([[-1.4124, -2.4500, -1.1373,  6.7875,  0.6716, -1.4278]]) 0\n",
      "tensor([[-0.4063, -2.4766, -3.8339,  1.9700,  4.7489, -2.2533]]) 0\n",
      "tensor([[-0.0135,  1.8732,  4.9584,  1.3148, -3.6132, -1.5007]]) 0\n",
      "tensor([[-1.3650, -2.0324, -3.0918,  1.9968,  5.4017, -2.8826]]) 1\n",
      "tensor([[-1.1907,  0.0150, -2.9901, -1.1788,  4.3385, -1.8039]]) 1\n",
      "tensor([[-2.8707,  0.4634, -1.4657, -0.8683, -0.3873,  4.6625]]) 0\n",
      "tensor([[-1.0086, -3.3169, -1.9296,  6.5073, -0.0071,  0.5479]]) 1\n",
      "tensor([[-3.0058, -1.8326, -1.1114,  1.2410,  0.7364,  3.8469]]) 1\n",
      "tensor([[ 3.6635, -1.8119, -0.8183, -0.8444,  0.2136, -3.0033]]) 1\n",
      "tensor([[-2.3204,  0.0791,  4.0673, -0.6592, -0.0946,  0.8822]]) 1\n",
      "tensor([[ 1.2396,  2.6811, -1.8268,  1.5689, -1.5498, -2.4976]]) 0\n",
      "tensor([[-1.5948,  0.0721,  0.5797,  3.5793, -1.1470, -0.4719]]) 0\n",
      "tensor([[-1.3324,  3.5784,  4.7954, -1.4474, -0.5611, -1.8930]]) 0\n",
      "tensor([[-2.7140, -0.3825, -3.6861, -1.7074,  4.9929, -0.0631]]) 0\n",
      "tensor([[-1.3851, -2.5551,  0.5792,  6.7694, -0.7254, -1.2682]]) 1\n",
      "tensor([[-0.4872,  6.2965,  0.6650, -1.6170, -0.8788, -2.4868]]) 0\n",
      "tensor([[-2.5507, -1.7135, -1.4313,  0.4897, -0.0167,  4.4197]]) 1\n",
      "tensor([[ 1.8387e+00,  4.8427e+00,  1.3490e-03, -1.3169e+00, -2.4650e+00,\n",
      "         -2.5433e+00]]) 0\n",
      "tensor([[-1.4543, -2.1797, -0.2558,  1.9436, -1.9251,  3.1475]]) 0\n",
      "tensor([[-0.7979,  7.0804, -0.9819, -3.4379, -0.2681, -1.9108]]) 0\n",
      "tensor([[ 3.9832,  0.2033, -2.2500, -1.9146, -1.6555, -2.5493]]) 1\n",
      "tensor([[-0.1403,  0.6467,  0.4345,  3.3944, -3.0055, -0.6440]]) 0\n",
      "tensor([[-0.0469, -2.5166, -1.8773,  9.0458, -2.5929, -0.4396]]) 1\n",
      "tensor([[-0.7604,  5.4842, -1.9719, -2.5173,  0.4470, -2.4796]]) 0\n",
      "tensor([[-0.3328, -1.1106,  0.7618,  5.7560, -2.0160, -1.3945]]) 1\n",
      "tensor([[-1.1297, -0.5299,  0.2249,  3.7361, -2.4119,  1.3262]]) 0\n",
      "tensor([[-1.8722,  2.0028,  1.0705, -1.7374,  2.8528, -2.4686]]) 0\n",
      "tensor([[ 3.5249, -1.4512, -2.8708,  0.5476, -0.5481, -4.8479]]) 1\n",
      "tensor([[-0.6248, -0.3617, -1.8111, -0.8212,  3.3897, -1.6944]]) 1\n",
      "tensor([[-2.5809, -0.5641, -1.1988, -0.1710, -0.7054,  4.3026]]) 0\n",
      "tensor([[-3.2786,  0.2283,  5.3956,  0.0523, -0.5415,  1.9185]]) 1\n",
      "tensor([[-2.2000, -1.4268, -2.0622, -0.4957,  5.7446, -0.6462]]) 1\n",
      "tensor([[-2.4323, -2.1520, -1.3292,  1.0566, -0.9272,  4.4259]]) 1\n",
      "tensor([[-1.8212, -0.1574, -3.2545, -1.6337,  5.0329, -0.9382]]) 1\n",
      "tensor([[-0.8232, -0.2939, -2.5311, -1.2977,  1.0286,  0.7317]]) 0\n",
      "tensor([[-3.2541, -1.8764, -1.6226, -0.0402, -1.4319,  7.6710]]) 1\n",
      "tensor([[ 0.0779,  0.1901,  1.3683,  3.2983, -1.6930, -1.5023]]) 0\n",
      "tensor([[-2.8603, -2.0759, -0.4455, -0.2077, -0.6850,  7.1860]]) 1\n",
      "tensor([[-0.2718, -1.9946,  3.2122, -0.1272, -0.0218, -1.5347]]) 1\n",
      "tensor([[-0.1966,  4.2118, -1.5116, -1.4974,  0.5828, -2.5342]]) 0\n",
      "tensor([[ 4.9817, -0.0305,  0.0619,  1.3642, -2.4716, -4.8000]]) 1\n",
      "tensor([[-0.6029,  0.7851,  6.7076,  0.6137, -1.9473, -1.9228]]) 0\n",
      "tensor([[-3.9526, -4.5073, -3.3602,  1.7091,  8.2202, -0.5504]]) 0\n",
      "tensor([[-1.3747,  0.7585, -0.9317, -1.3992,  4.5968, -2.1209]]) 0\n",
      "tensor([[-1.8610, -1.8231, -0.8718,  1.0473, -0.2532,  2.9969]]) 0\n",
      "tensor([[-1.8293, -2.8396, -2.1941,  1.0038,  4.4973, -0.2994]]) 0\n",
      "tensor([[-2.9381, -0.0662, -1.2580, -0.8422, -2.5066,  7.5327]]) 1\n",
      "tensor([[-1.4344, -1.3517, -0.2940,  3.2746,  0.4920, -0.4696]]) 0\n",
      "tensor([[-1.5543,  1.4836,  8.3199, -0.7427, -1.9471, -1.1473]]) 1\n",
      "tensor([[ 2.8436,  1.7791, -3.1867, -2.1619,  2.5560, -6.5650]]) 0\n",
      "tensor([[-1.6003,  0.0818,  1.7810,  1.7353, -0.6278,  0.6878]]) 0\n",
      "tensor([[ 0.8560,  8.2601, -0.7833, -4.0267, -0.6724, -3.8657]]) 0\n",
      "tensor([[-0.4864, 10.9237, -0.2841, -1.7281, -1.5938, -4.7322]]) 0\n",
      "tensor([[ 1.9086,  1.4168, -2.3323,  0.0899, -1.0503, -2.1777]]) 0\n",
      "tensor([[-2.1970, -2.7833,  0.4705,  9.7036, -1.0288, -0.5391]]) 1\n",
      "tensor([[-3.3937, -2.2392,  1.0057,  0.5348,  0.2583,  4.6645]]) 1\n",
      "tensor([[-3.0144, -1.7959,  1.4349,  0.0727, -1.9523,  5.2501]]) 1\n",
      "tensor([[ 0.7545,  7.4984, -0.7684, -3.2856, -0.5142, -3.8671]]) 1\n",
      "tensor([[ 1.0589,  2.7658, -1.9732,  1.5239, -0.5108, -3.3754]]) 1\n",
      "tensor([[-2.4360, -0.6596,  1.5074,  2.6244, -2.0995,  2.3498]]) 0\n",
      "tensor([[-1.0790,  1.8568,  1.5438, -0.2045, -0.7725, -0.6617]]) 0\n",
      "tensor([[-2.4794, -2.5917, -4.8926,  1.7804,  8.1975, -3.6759]]) 0\n",
      "tensor([[-2.3755, -1.6656,  0.9540, -1.0402,  0.8521,  2.5078]]) 0\n",
      "tensor([[-0.7675,  3.4899, -2.4200, -1.5479,  0.2479, -0.2543]]) 1\n",
      "tensor([[-2.2449,  0.5560,  5.8838, -0.4096, -3.1474,  1.9897]]) 1\n",
      "tensor([[ 0.7765, -0.3236,  0.5017,  5.0597, -1.1735, -3.6453]]) 1\n",
      "tensor([[-0.5877, -0.0976, -1.1983, -1.2234,  3.3790, -1.8554]]) 1\n",
      "tensor([[-3.2600, -1.9069, -1.2908,  2.1468, -1.3862,  5.6912]]) 0\n",
      "tensor([[-4.0168, -2.8175, -1.8762,  1.0386, -1.1586,  7.7902]]) 1\n",
      "tensor([[-1.3585,  3.4711, -2.1669, -1.5691,  3.0359, -1.7563]]) 0\n",
      "tensor([[-0.3858, -2.4656, -0.6346,  8.2410, -1.0666, -1.9814]]) 0\n",
      "tensor([[-0.3688,  2.4244, -1.6217, -0.1233,  2.2596, -3.2890]]) 0\n",
      "tensor([[ 0.7763, -2.0455,  0.9354,  0.1662,  1.0023, -3.0442]]) 1\n",
      "tensor([[ 3.3525, -0.6964, -0.3772, -0.9166, -0.5433, -2.8698]]) 0\n",
      "tensor([[ 0.7469,  0.2763, -1.8087, -1.9186,  2.7537, -3.8060]]) 0\n",
      "tensor([[-2.0342, -1.0094, -0.4124, -0.6058,  0.0523,  4.1767]]) 1\n",
      "tensor([[-2.9740,  0.1405,  0.9539,  4.3112,  0.8620, -1.3946]]) 1\n",
      "tensor([[ 3.6243,  1.9846, -0.2376, -1.4591, -2.8480, -2.7462]]) 1\n",
      "tensor([[-0.2054,  7.4097, -0.1936, -1.8776, -1.1052, -2.6317]]) 1\n",
      "tensor([[-0.6923,  6.0505, -1.5961,  1.8837, -2.3108, -2.9420]]) 1\n",
      "tensor([[ 2.5456, -0.4480, -1.0774,  0.5958, -1.0295, -1.3496]]) 0\n",
      "tensor([[-1.9660, -0.0553, -1.0104,  0.0434, -1.3359,  4.7759]]) 1\n",
      "tensor([[ 4.1444,  3.2040, -4.4077, -0.7206, -2.4333, -4.0389]]) 1\n",
      "tensor([[-1.3079,  0.3609, -2.9400,  0.4159,  4.1756, -2.5488]]) 0\n",
      "tensor([[-3.7613, -1.8500, -1.8871, -0.7301,  0.5495,  6.6154]]) 1\n",
      "tensor([[-0.2594,  5.5759,  0.1979,  1.3258, -2.7190, -2.2196]]) 1\n",
      "tensor([[ 0.3173,  5.0204, -1.7419, -2.6864, -0.3589, -2.2903]]) 0\n",
      "tensor([[ 0.0811,  1.7477,  6.7239, -0.4887, -2.6030, -2.0323]]) 0\n",
      "tensor([[-2.8303, -1.6821, -4.8636, -0.0957,  7.3560, -1.1808]]) 1\n",
      "tensor([[-0.1230,  1.9410,  3.2043, -0.6430, -0.3769, -3.5169]]) 0\n",
      "tensor([[-0.9446,  0.2377,  4.9025,  0.3969, -1.2048, -0.0711]]) 0\n",
      "tensor([[-1.1378,  0.1171,  5.2002,  1.9761, -2.3069, -0.2007]]) 1\n",
      "tensor([[-0.4065,  1.3682,  0.5060, -0.7850, -0.1304, -0.9631]]) 0\n",
      "tensor([[-0.3677, -2.1146, -0.5144,  4.7817,  0.0684, -0.8749]]) 0\n",
      "tensor([[-1.5909, -0.8132,  2.2885,  1.7509, -1.0468,  1.1311]]) 0\n",
      "tensor([[ 4.5966,  0.3989, -0.8034, -0.0993, -3.0355, -3.1092]]) 1\n",
      "tensor([[-1.0136,  4.7065,  0.2367, -0.5068, -0.3125, -2.1837]]) 0\n",
      "tensor([[ 0.1638, -1.7959, -3.4077, -1.7479, -0.6957,  3.4744]]) 0\n",
      "tensor([[-0.8199,  0.9387, -2.9012, -1.1793,  2.9605, -1.5134]]) 0\n",
      "tensor([[ 0.4406,  1.1513,  0.3312, -1.1093,  0.2135, -0.9210]]) 0\n",
      "tensor([[ 0.1123,  0.2714,  3.5263, -1.3350, -0.9533, -0.9027]]) 1\n",
      "tensor([[-1.6453, -1.5352, -1.2330,  0.4528, -2.9417,  5.9582]]) 1\n",
      "tensor([[-0.9179, -0.0777,  6.0930,  2.9975, -2.2196, -1.8790]]) 1\n",
      "tensor([[-1.2128,  3.1366,  0.2812, -1.5948, -0.1490,  0.9070]]) 0\n",
      "tensor([[-0.7935, -3.6744, -0.2186,  7.5260, -0.4308, -0.3649]]) 1\n",
      "tensor([[ 0.0431, -0.5458,  4.7666, -0.0736, -1.8113, -0.4347]]) 1\n",
      "tensor([[-0.7157, -1.0600, -0.7776,  3.8061, -1.0700,  0.0760]]) 1\n",
      "tensor([[-1.6591, -2.8504, -0.8029,  2.0183,  4.4981, -1.6250]]) 0\n",
      "tensor([[ 0.1439,  4.5830, -1.6919, -1.0586, -0.6818, -2.2673]]) 1\n",
      "tensor([[-4.3318, -2.1676, -2.4780,  0.6813,  6.9101, -0.8311]]) 1\n",
      "tensor([[ 0.2007, -1.8292, -2.6638,  0.6677,  6.4377, -3.9070]]) 0\n",
      "tensor([[-1.8903,  0.3429, -3.0114, -2.6544,  6.4827, -2.7104]]) 1\n",
      "tensor([[-0.9530, -0.5441,  1.2731,  7.1867, -2.4391, -1.8428]]) 0\n",
      "tensor([[-1.7454,  6.2145, -1.2010, -2.8654,  1.3949, -2.5345]]) 1\n",
      "tensor([[-0.0611,  1.1170,  3.5190,  0.1644, -1.3479, -2.3552]]) 1\n",
      "tensor([[-2.0287, -2.1125, -0.9374,  1.3124,  0.5347,  2.9725]]) 0\n",
      "tensor([[-0.1364,  7.7342, -0.2302, -2.6919, -0.0484, -4.7888]]) 0\n",
      "tensor([[-1.1814,  1.4412,  4.3885, -1.3092, -2.3617,  0.9671]]) 1\n",
      "tensor([[-2.2317, -0.6765,  2.0071,  2.1341, -1.6529,  1.6268]]) 0\n",
      "tensor([[-2.9566,  0.1566, -2.6791, -0.0912,  5.6646, -2.1126]]) 0\n",
      "tensor([[-1.5507, -3.1039, -3.0446,  1.8006,  5.0082, -0.0675]]) 0\n",
      "tensor([[ 1.3953, -2.1468, -3.1538, -0.3158, -0.8945,  1.7030]]) 0\n",
      "tensor([[-2.3925, -0.6108,  5.3479, -0.4598, -0.9531,  2.3266]]) 1\n",
      "tensor([[-2.4613,  5.0824,  6.7132, -0.9957, -2.3923, -0.6212]]) 1\n",
      "tensor([[-2.9269, -1.8464, -1.4886,  2.5523,  5.4595, -2.5094]]) 0\n",
      "tensor([[ 4.2299, -0.2207, -2.4749, -0.8320, -1.0377, -2.9808]]) 1\n",
      "tensor([[-2.3956, -1.9821, -5.5477,  0.9633,  9.8607, -4.9121]]) 1\n",
      "tensor([[ 2.5069, -0.1751, -0.5417, -0.5987, -0.5108, -2.7995]]) 1\n",
      "tensor([[-2.9039, -2.2640, -1.0579,  1.3628,  1.8246,  3.0830]]) 0\n",
      "tensor([[-2.0434, -2.9461,  0.3096,  6.6308,  0.8470, -0.6280]]) 1\n",
      "tensor([[ 4.0347,  0.6853,  0.0546, -1.2397, -0.6171, -4.7984]]) 1\n",
      "tensor([[-2.0093, -0.3995, -1.3336, -0.4584, -2.3433,  5.1194]]) 1\n",
      "tensor([[-1.0487,  2.5126,  8.2263, -0.7834, -2.6574, -1.3661]]) 1\n",
      "tensor([[-0.6401,  5.1765,  6.5883, -0.4047, -2.7520, -3.4760]]) 1\n",
      "tensor([[-0.3555,  4.1855,  0.3254, -1.9580, -0.4948, -1.0813]]) 0\n",
      "tensor([[-1.8225, -1.2382, -1.2847, -0.4166,  0.3678,  3.7694]]) 0\n",
      "tensor([[-0.8927, -0.4164, -1.7913, -1.0519,  0.9906,  0.5912]]) 0\n",
      "tensor([[-1.1276,  0.1613,  3.9627,  3.1732, -1.6127, -1.5599]]) 0\n",
      "tensor([[-1.8717,  7.3090,  3.5692, -2.6367, -1.1302, -2.5600]]) 0\n",
      "tensor([[-1.2238, -0.4590,  5.8583, -0.8053, -0.7403,  0.6601]]) 1\n",
      "tensor([[ 1.9693, -3.1218, -1.8316,  4.9497, -0.6468, -1.3350]]) 1\n",
      "tensor([[-2.8667,  1.9233, -3.1246, -0.8289,  6.7599, -3.9557]]) 0\n",
      "tensor([[-1.3421, -2.7080, -0.5345,  7.3977,  0.1648, -1.0281]]) 1\n",
      "tensor([[-1.9217,  2.4995,  6.8764, -2.3314, -0.9062, -0.3673]]) 1\n",
      "tensor([[-2.7511, -1.8313, -3.6743,  1.9011,  2.2111,  2.1755]]) 0\n",
      "tensor([[-4.0404, -0.9124, -2.8336, -0.0166, -0.7082,  6.4033]]) 1\n",
      "tensor([[-2.0274, -1.9334, -0.9247,  0.6634, -0.3784,  4.3887]]) 1\n",
      "tensor([[ 1.8210,  2.8573,  2.2533, -2.1952, -0.8546, -4.4848]]) 0\n",
      "tensor([[-1.5904, -1.5185, -0.6638,  0.5212, -1.6859,  4.2667]]) 0\n",
      "tensor([[-2.5210,  9.1501,  0.2327, -1.7943, -1.7082, -1.0005]]) 1\n",
      "tensor([[ 0.8817,  3.7032, -0.3193, -1.9622, -1.1276, -2.3871]]) 0\n",
      "tensor([[-1.2067,  2.3370,  7.2541,  1.6086, -4.2499, -1.7211]]) 0\n",
      "tensor([[-2.1385, -2.1739, -0.5181,  5.6449, -0.7556,  1.0012]]) 0\n",
      "tensor([[-2.0596,  1.4076,  7.7032,  0.6898, -2.1729, -0.3604]]) 1\n",
      "tensor([[-2.1364,  4.4791, -1.3177,  1.5495, -1.6317, -0.3913]]) 0\n",
      "tensor([[ 0.4130, -1.8016, -3.5645,  0.6198,  2.0184, -2.5222]]) 0\n",
      "tensor([[-1.2883, -4.6334, -3.2386, 11.7632,  0.3047, -1.8207]]) 1\n",
      "tensor([[-3.3477, -1.2003, -1.7483,  0.7185,  2.7356,  1.0746]]) 1\n",
      "tensor([[ 2.6024,  0.3195, -0.3991, -0.0040, -2.3835, -0.5940]]) 1\n",
      "tensor([[-2.6490, -3.1117, -1.3828,  1.0380, -0.0281,  5.6380]]) 1\n",
      "tensor([[-3.4944, -1.7797, -2.7842,  1.5412,  5.0561, -1.1924]]) 1\n",
      "tensor([[-2.7697, -0.7317, -0.3105, -0.4357,  4.2603, -0.3847]]) 1\n",
      "tensor([[ 0.1051,  4.9986,  0.5976, -1.6610, -1.8680, -3.1117]]) 1\n",
      "tensor([[-1.7850, -1.9430, -4.4501,  1.3841,  7.6508, -3.4914]]) 0\n",
      "tensor([[-1.1555, -2.9305, -2.3436,  1.2540,  5.1202, -2.0008]]) 1\n",
      "tensor([[-2.0246, -2.0136,  0.4589,  4.4428, -1.6972,  2.8166]]) 0\n",
      "tensor([[-1.3965, -0.3352, -1.8253, -0.1987,  4.0306, -1.4368]]) 0\n",
      "tensor([[-1.8944, -1.6207, -3.3173,  0.2737,  2.7315,  0.2767]]) 0\n",
      "tensor([[-1.0894, -0.8240, -2.6663, -1.4784,  2.0803,  1.0794]]) 0\n",
      "tensor([[-1.1301, -1.3894, -1.1804,  0.7304,  1.9397, -0.0528]]) 0\n",
      "tensor([[-2.5520, -0.6435,  3.0421,  1.8970,  0.5664, -0.6469]]) 0\n",
      "tensor([[ 5.9282, -0.6252, -3.2938, -1.3379, -1.1079, -6.5030]]) 1\n",
      "tensor([[ 0.6414, -0.7227, -0.5833,  2.2280,  1.0365, -3.6924]]) 0\n",
      "tensor([[-0.7457,  8.2993,  1.9178, -2.3631, -1.2222, -4.3837]]) 1\n",
      "tensor([[-1.9348,  2.4949,  4.9570, -1.4031, -2.3077,  1.2717]]) 1\n",
      "tensor([[ 4.2388, -0.8690, -2.6777, -0.9219, -0.3461, -3.8947]]) 1\n",
      "tensor([[-0.0617,  1.8768,  7.3421, -2.0303, -0.6968, -2.5738]]) 1\n",
      "tensor([[-1.0677, -1.4184, -1.2492,  6.7055, -0.6897, -0.6640]]) 0\n",
      "tensor([[-1.7342, -0.0082,  3.2725, -0.4515,  0.7630, -1.1996]]) 1\n",
      "tensor([[ 1.1331,  3.2042, -0.5790, -1.9807, -0.7975, -3.1228]]) 0\n",
      "tensor([[-3.0327, -1.7219, -1.8776,  1.0473, -0.3875,  5.6500]]) 0\n",
      "tensor([[-2.8171, -4.6312, -0.6159,  1.7363,  3.9703,  2.5194]]) 0\n",
      "tensor([[ 0.7443,  2.5872,  3.8004, -0.1882, -2.6011, -2.4375]]) 0\n",
      "tensor([[ 1.4791,  5.3784,  0.5938, -0.0185, -1.9528, -4.5514]]) 0\n",
      "tensor([[-1.0931, -0.3198,  2.9065, -0.6920, -0.0381,  0.1297]]) 0\n",
      "tensor([[-3.0658, -0.6976,  0.8038, -0.1449, -1.3696,  5.9333]]) 0\n",
      "tensor([[ 0.9185, -0.4988,  0.2592,  2.1432, -0.7281, -2.9612]]) 1\n",
      "tensor([[-3.5524, -2.1255,  1.1843,  0.4957, -0.9561,  5.8523]]) 0\n",
      "tensor([[-0.3097,  0.0178,  6.7392,  1.1078, -3.0199, -0.2767]]) 1\n",
      "tensor([[-0.9269,  0.4418, -0.6003,  4.5590, -1.6819, -0.8355]]) 0\n",
      "tensor([[-1.1686e+00, -1.9268e+00,  4.4522e-03,  6.8136e+00,  4.6898e-03,\n",
      "         -1.7103e+00]]) 0\n",
      "tensor([[-2.2528, -2.0674, -1.3799,  1.0864, -1.8104,  4.2090]]) 1\n",
      "tensor([[-0.0540,  3.9113,  3.6500, -1.7300, -1.8613, -3.0369]]) 1\n",
      "tensor([[-2.8079, -1.0283,  9.4203,  0.5959, -3.0169,  1.9132]]) 1\n",
      "tensor([[ 0.0309, -1.2966, -0.5187,  2.8430,  0.5457, -2.2293]]) 1\n",
      "tensor([[-1.4664, -1.2196, -1.8270,  5.0270,  1.5930, -1.3818]]) 0\n",
      "tensor([[ 0.3097,  4.1520,  4.7539, -2.2723, -2.5487, -1.7727]]) 0\n",
      "tensor([[-2.5464, -2.5868, -0.5323,  5.7849, -0.6916,  1.2573]]) 1\n",
      "tensor([[ 1.4512,  0.7484,  5.1526, -1.0391, -0.5375, -3.0848]]) 1\n",
      "tensor([[ 3.6156,  0.0949,  2.3773, -2.1549, -0.9981, -3.9607]]) 1\n",
      "tensor([[-1.1101, -3.6722, -3.3340,  9.2241,  0.0653, -0.4866]]) 1\n",
      "tensor([[-1.8860, -1.9932, -3.6269,  1.0364,  6.0747, -3.3359]]) 0\n",
      "tensor([[ 2.0619, -2.8824, -0.4928,  5.2340, -1.6437, -1.9760]]) 0\n",
      "tensor([[-0.0668, -2.1054, -2.3835,  6.6823, -0.3001, -1.5286]]) 1\n",
      "tensor([[-1.0339, -0.5246,  3.6188,  3.9046, -1.4225, -0.8715]]) 0\n",
      "tensor([[-4.2794, -0.9962, -0.1750, -0.4413, -0.1644,  6.8597]]) 0\n",
      "tensor([[ 0.2116,  1.1261, -1.9342, -0.7699, -0.3744, -0.8991]]) 0\n",
      "tensor([[-0.8456, -1.3610, -0.9197, -0.4720,  1.8481,  0.1530]]) 0\n",
      "tensor([[-4.5290, -2.8955, -3.1956,  1.0605,  0.2431,  6.1810]]) 0\n",
      "tensor([[-0.4515,  9.9101, -1.5678, -3.9279, -0.2642, -4.0103]]) 1\n",
      "tensor([[-1.4971,  7.0799, -0.9259, -2.9066,  0.3112, -3.0026]]) 1\n",
      "tensor([[ 2.9409, -1.2544, -1.7254,  4.0685, -1.9493, -3.0805]]) 0\n",
      "tensor([[-0.1592, -1.3101, -1.7841,  6.2588, -1.1409, -0.7261]]) 0\n",
      "tensor([[-0.0216, -0.5600,  2.6706,  4.2697, -1.1902, -2.6384]]) 0\n",
      "tensor([[-3.0491, -4.2011, -2.3871, 10.4664,  0.3937,  1.0808]]) 0\n",
      "tensor([[-1.3831, -4.2827, -3.6619, 10.8970, -1.4617,  0.7513]]) 1\n",
      "tensor([[-2.5244, -2.4955, -1.2535,  5.7288,  0.8016, -0.0107]]) 0\n",
      "tensor([[-0.5056,  0.7274,  6.7522, -0.9966, -1.1593, -1.3218]]) 0\n",
      "tensor([[-1.7094,  1.2031,  8.2234, -1.6545, -1.5633, -0.5821]]) 1\n",
      "tensor([[-0.9081,  1.0938,  7.8874, -1.2544, -1.4757, -1.7071]]) 0\n",
      "tensor([[-2.7042, -0.8214, -1.0695,  2.9422, -0.3270,  1.5476]]) 1\n",
      "tensor([[-4.2393, -2.9252,  0.3159, -0.7936, -1.9747, 10.6427]]) 1\n",
      "tensor([[-3.6749, -1.5479, -0.6449, -1.3006, -1.2809,  8.3587]]) 0\n",
      "tensor([[-1.5353,  1.7757,  5.3096, -0.2027, -1.3428, -0.6168]]) 0\n",
      "tensor([[-1.4966,  2.4788, -1.3162,  1.3722, -0.1869, -0.7849]]) 0\n",
      "tensor([[-1.0836,  0.1347,  6.3296, -0.6409, -0.0492, -0.6884]]) 0\n",
      "tensor([[ 0.5848,  3.9578, -1.4536,  1.5954, -3.0086, -2.1768]]) 0\n",
      "tensor([[-1.0313,  7.3759,  0.0934, -1.1990, -0.4050, -3.4911]]) 0\n",
      "tensor([[ 0.4813, -1.9190,  1.0792,  5.3041, -1.3524, -1.5429]]) 1\n",
      "tensor([[-2.1111, -1.9287, -4.4042,  2.3323,  7.9351, -4.1778]]) 1\n",
      "tensor([[-0.1933, -1.1347,  3.1271,  5.3968, -1.3438, -2.9649]]) 1\n",
      "tensor([[-1.2169,  1.7509,  3.4485, -0.7104, -2.7777,  0.9777]]) 0\n",
      "tensor([[-1.3713, -1.0500,  0.3053,  1.7285, -0.1011,  0.7608]]) 0\n",
      "tensor([[ 0.6190, -0.0126,  0.7314,  0.4610,  0.7231, -2.1838]]) 0\n",
      "tensor([[-1.4215,  1.8842, 11.3837, -2.3430, -1.7881, -1.5776]]) 1\n",
      "tensor([[-0.2532,  2.0477,  9.7783, -2.4254, -2.2416, -2.0179]]) 0\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 1\n",
      "tensor([[-0.9748, -0.3207, -0.4798,  6.3847, -2.1769, -0.1936]]) 0\n",
      "tensor([[-1.3934, -2.0167,  1.0988,  5.9809, -1.5696,  0.2996]]) 1\n",
      "tensor([[-1.9599,  0.1171, -2.9235,  0.2147,  6.4929, -4.0552]]) 1\n",
      "tensor([[ 0.4407,  8.4211, -1.2190, -1.8488, -1.2270, -4.6715]]) 1\n",
      "tensor([[-2.0190,  1.4149,  8.0958,  2.1545, -3.2795, -1.4568]]) 1\n",
      "tensor([[-1.2056, -0.2571,  9.2996,  1.9043, -2.5399, -2.2165]]) 1\n",
      "tensor([[-0.8455, -0.9631,  5.2542, -0.3754, -2.2199,  1.9715]]) 1\n",
      "tensor([[-0.5957,  5.8393,  6.9273, -2.4037, -3.1715, -1.4698]]) 0\n",
      "tensor([[-1.9254,  2.1644,  1.2021,  1.8804, -0.7787, -1.2943]]) 0\n",
      "tensor([[-0.9619,  8.8613, -2.6247, -3.8071, -0.7774, -2.9011]]) 0\n",
      "tensor([[-1.4309,  1.2591, -2.4739,  0.2872,  0.2485,  1.5001]]) 0\n",
      "tensor([[-0.3041,  9.1441,  2.2304, -2.2486, -1.0380, -5.6181]]) 0\n",
      "tensor([[ 1.9688,  0.4272,  1.7365, -0.4241, -2.4002, -0.7695]]) 1\n",
      "tensor([[ 0.1224,  5.8744,  0.3885, -2.7381,  0.3951, -3.5044]]) 1\n",
      "tensor([[-0.9478, -1.2799,  3.5511,  6.4641, -1.6485, -2.5552]]) 1\n",
      "tensor([[-1.1076, -2.8174, -0.9310,  7.8478, -0.6751, -1.0061]]) 1\n",
      "tensor([[-1.8670, -3.6369,  3.6091,  8.7488, -1.6599, -1.4846]]) 0\n",
      "tensor([[-1.1108,  3.3873,  4.4336, -1.3743, -0.9919, -2.0118]]) 0\n",
      "tensor([[-0.4154,  0.4839,  5.9139, -1.3406, -1.4166, -0.5670]]) 1\n",
      "tensor([[-1.3175,  0.4280,  3.1857,  4.4123, -2.3177, -1.1643]]) 1\n",
      "tensor([[-2.4987,  0.5566, -1.8802, -0.7916,  4.3422, -0.6120]]) 1\n",
      "tensor([[ 5.7238, -1.4324, -2.2766,  0.0348, -0.9936, -6.9562]]) 1\n",
      "tensor([[-0.7355,  5.2959,  0.5798, -1.7333, -0.9193, -2.5274]]) 0\n",
      "tensor([[-1.3784,  4.2160, -1.3411, -1.8150, -0.3578, -0.6527]]) 1\n",
      "tensor([[-1.1597, -2.2873, -0.5560,  7.2086, -0.7447, -0.1108]]) 1\n",
      "tensor([[-2.7724, -1.0843, -4.9712,  0.3450,  6.3935, -1.7532]]) 0\n",
      "tensor([[-0.5604, -2.2222, -1.4943,  6.0298, -0.2595, -0.7717]]) 0\n",
      "tensor([[-0.2887,  0.0774,  0.4915,  4.5560, -1.9696, -1.6927]]) 1\n",
      "tensor([[-1.2927,  0.1417, -0.7211, -0.2798, -1.6345,  3.5614]]) 1\n",
      "tensor([[-4.1454, -2.0932,  1.6049, -0.6479,  3.0410,  4.0592]]) 1\n",
      "tensor([[-2.7512, -1.0862,  3.6244, -1.3240,  1.4180,  2.5994]]) 1\n",
      "tensor([[-0.7437,  0.9845,  5.7948,  0.2987, -1.6099, -0.9763]]) 1\n",
      "tensor([[ 0.2046, -2.2822, -1.7904,  7.9621, -1.1313, -3.2004]]) 0\n",
      "tensor([[ 0.1611, -1.9687, -2.7225,  6.8804, -1.0194, -1.3354]]) 0\n",
      "tensor([[-0.5393, -1.4344,  2.6140,  3.7492, -2.5663,  0.0474]]) 0\n",
      "tensor([[-0.3430,  4.1760,  6.1285, -2.0005, -1.7431, -3.4431]]) 1\n",
      "tensor([[-0.1169, -1.4948,  1.0093,  4.5647, -0.7111, -1.8676]]) 0\n",
      "tensor([[-4.2358, -0.6618,  0.6152, -0.7451, -1.0465,  5.7156]]) 0\n",
      "tensor([[-4.9948, -2.2598, -5.7922,  0.7084,  5.2337,  2.8962]]) 0\n",
      "tensor([[ 2.2707,  0.9662,  5.9868,  0.0657, -3.6482, -3.5172]]) 0\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 1\n",
      "tensor([[-0.0894,  3.3736, -1.0448, -2.1530, -1.6353,  1.9044]]) 0\n",
      "tensor([[-2.3975, -1.1267, -1.9538, -0.1474,  6.3848, -1.2955]]) 1\n",
      "tensor([[-2.8567, -3.1413, -1.2216,  0.7176,  4.9345,  0.7986]]) 0\n",
      "tensor([[-1.0371, -2.0321, -2.4422,  1.0734,  4.7474, -1.2582]]) 1\n",
      "tensor([[-0.0976,  2.8232, -0.4321, -1.3068, -1.3504, -1.8388]]) 0\n",
      "tensor([[-2.0378e+00,  1.2712e-03,  6.0831e+00, -3.3160e-01, -1.8528e+00,\n",
      "          1.3658e+00]]) 0\n",
      "tensor([[ 0.9340,  6.5431, -4.1918, -3.2777,  2.5395, -5.3795]]) 1\n",
      "tensor([[ 0.3964,  1.5356,  3.7435,  1.6423, -2.6423, -3.3516]]) 1\n",
      "tensor([[-0.5843, -1.7639,  0.4124,  6.4825, -0.5847, -2.3739]]) 0\n",
      "tensor([[-2.3807, -0.6529, -2.9436,  1.7535, -1.2286,  3.6726]]) 0\n",
      "tensor([[-2.7694, -1.0199, -4.1346,  1.1181,  6.6478, -2.3695]]) 1\n",
      "tensor([[-2.5703, -1.1548, -2.3327,  2.0202,  5.7018, -3.1603]]) 0\n",
      "tensor([[ 0.7928,  7.1036,  0.3679, -0.9658, -0.2319, -6.5638]]) 1\n",
      "tensor([[-0.1248,  5.6032,  5.5227, -0.7188, -2.3930, -5.2812]]) 0\n",
      "tensor([[-1.4866,  3.8468,  1.4917,  0.0267, -2.6643, -0.8581]]) 0\n",
      "tensor([[-1.6835, -1.2968, -1.9385, -0.4036, -1.0279,  5.0625]]) 0\n",
      "tensor([[-2.0584, -1.7959,  4.2495,  4.8243, -0.2229, -0.5845]]) 1\n",
      "tensor([[-3.1759,  0.5469, -2.6168, -1.3421,  7.0760, -3.1101]]) 0\n",
      "tensor([[ 3.7708,  0.3123, -3.8498,  0.5450,  1.1065, -6.4015]]) 0\n",
      "tensor([[-0.8147, -1.6591, -0.8030,  4.5792, -0.1474, -0.8692]]) 1\n",
      "tensor([[-3.1498,  0.6871,  7.0513, -0.4137, -1.1373,  1.0417]]) 0\n",
      "tensor([[-3.4381, -2.2574, -0.0072,  1.0038, -0.5273,  5.4586]]) 0\n",
      "tensor([[-1.6258,  0.6253,  1.0784,  4.2711, -1.9093, -1.0321]]) 1\n",
      "tensor([[-0.9753, -1.4691,  0.4154,  0.6382, -0.6333,  2.1352]]) 0\n",
      "tensor([[-2.0260, -0.1401, -0.7307, -1.3105, -2.1021,  5.7748]]) 1\n",
      "tensor([[-4.4923, -3.2657, -0.3399,  0.9667, -1.1827,  8.1633]]) 1\n",
      "tensor([[-3.0278, -2.8471, -1.7653,  0.8677, -1.4255,  6.3285]]) 0\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 1\n",
      "tensor([[ 0.9671, -1.2165, -1.3454,  4.5047, -0.5582, -1.3546]]) 1\n",
      "tensor([[-2.5558, -0.2294, -1.3050, -0.4805,  5.1247, -1.2476]]) 1\n",
      "tensor([[ 4.4569, -0.1844, -1.3268,  2.4528, -2.8661, -3.9186]]) 0\n",
      "tensor([[-3.0527, -0.6289, -0.7191,  0.7132,  7.0586, -3.6702]]) 0\n",
      "tensor([[ 1.9578,  6.6284,  4.1731, -2.1478, -2.5292, -6.0117]]) 1\n",
      "tensor([[ 3.6168, -1.3542,  2.4844,  1.5055, -0.8168, -6.2655]]) 1\n",
      "tensor([[-1.7631, -0.8598, -0.1478, -1.1522, -3.2618,  5.4186]]) 1\n",
      "tensor([[ 4.0426,  0.4457,  1.3768,  1.0106, -2.7370, -3.7388]]) 0\n",
      "tensor([[ 0.0405,  4.3568, -1.7918, -2.2925,  0.4273, -2.2561]]) 1\n",
      "tensor([[-1.1449, -1.6623,  2.2179,  4.8442, -0.3835, -0.3869]]) 0\n",
      "tensor([[-1.7143,  1.5386,  3.7912,  0.6141, -0.7579,  0.0101]]) 0\n",
      "tensor([[-2.5043, -1.3479,  0.0514,  0.5989, -2.4298,  5.7992]]) 0\n",
      "tensor([[ 4.2502, -3.0498, -0.6979,  7.3114, -3.2492, -2.9201]]) 0\n",
      "tensor([[ 0.7693,  3.6639, -2.7710,  1.5507,  0.2919, -4.2612]]) 0\n",
      "tensor([[-1.0031, -3.8791,  0.8756,  6.4910,  0.2739, -1.1822]]) 0\n",
      "tensor([[ 1.7280,  5.4651, -0.1350, -2.4354, -0.7400, -4.4879]]) 1\n",
      "tensor([[-1.6402,  1.5339,  7.5680,  0.1584, -2.5185, -0.7667]]) 0\n",
      "tensor([[-1.9044, -1.2040,  0.6279,  5.4018, -0.4088, -0.5807]]) 0\n",
      "tensor([[ 1.6898,  6.4197, -2.5136, -2.7077, -1.1692, -4.9288]]) 0\n",
      "tensor([[-0.5669,  6.3712, -0.8338, -2.0677,  0.1657, -3.1798]]) 1\n",
      "tensor([[-1.6190, -0.3310,  7.8005,  0.1931, -1.6184, -0.6724]]) 0\n",
      "tensor([[-0.0872,  2.6467, -1.4198, -1.0069, -0.5355, -0.0785]]) 0\n",
      "tensor([[-0.2134, -0.7831,  6.8842,  1.7352, -1.3324, -2.0527]]) 1\n",
      "tensor([[ 0.8679, -0.2899,  1.7868,  6.1178, -1.2939, -5.0018]]) 0\n",
      "tensor([[ 0.5788, -0.1756,  6.2584, -0.9673, -2.1382, -0.8253]]) 1\n",
      "tensor([[ 5.7665, -2.2838, -3.1814,  2.9060, -0.6225, -5.1590]]) 1\n",
      "tensor([[-1.1560, -0.7953,  2.4683,  6.9693, -1.3667, -3.1221]]) 0\n",
      "tensor([[-4.2137, -1.5683, -1.2522, -1.3215,  8.1208, -1.1899]]) 1\n",
      "tensor([[-1.4966, -1.2087,  0.2828,  0.1991,  2.8499, -1.4699]]) 0\n",
      "tensor([[ 6.1160,  0.1682, -1.5745,  1.0145, -1.5623, -6.6232]]) 1\n",
      "tensor([[ 3.2012, -0.1221, -0.3343,  0.9588, -1.8194, -2.7121]]) 0\n",
      "tensor([[ 3.6375, -0.5986, -2.0459, -0.1724,  0.5856, -3.3728]]) 0\n",
      "tensor([[-0.1076,  7.3447, -1.9765, -2.6798,  0.9895, -4.8182]]) 1\n",
      "tensor([[-1.1545,  0.6793, 10.6093, -0.1000, -3.0742, -0.4575]]) 1\n",
      "tensor([[-1.6864,  0.1701,  4.7009,  3.4588, -1.7018, -1.7133]]) 1\n",
      "tensor([[-0.8383,  2.4359,  3.8046,  1.4827, -2.7952, -0.7353]]) 1\n",
      "tensor([[ 2.4860, -0.7934, -2.9026, -0.0800, -1.1993, -0.5704]]) 0\n",
      "tensor([[ 3.8715, -0.8202, -3.6330, -0.9980, -1.5788, -1.1223]]) 1\n",
      "tensor([[-2.0670, -3.3604,  0.8901,  8.4873, -0.2427, -1.5426]]) 1\n",
      "tensor([[-1.0977, -1.1963,  4.5246,  3.1730, -1.8531, -0.6162]]) 1\n",
      "tensor([[-2.0677, -1.2216, -3.1430, -1.3377,  4.7895,  1.1801]]) 0\n",
      "tensor([[-0.7550,  4.2992,  2.4283, -0.0503, -1.1662, -2.6324]]) 0\n",
      "tensor([[ 0.7091,  3.9442,  0.9629, -2.5955, -1.8717, -1.3074]]) 0\n",
      "tensor([[-0.5909,  4.0668, -0.3998, -3.7106,  2.1954, -3.9378]]) 0\n",
      "tensor([[-0.7402,  2.7120, -4.0450, -0.5223,  3.7254, -4.6167]]) 0\n",
      "tensor([[-0.1608, -1.3634,  4.6782,  3.5799, -2.5834, -2.0383]]) 0\n",
      "tensor([[-0.1063,  2.7282,  5.7064, -0.8905, -2.5524, -1.5676]]) 1\n",
      "tensor([[ 1.7674, -3.1299, -0.2960, -0.8021, -2.6979,  2.6694]]) 1\n",
      "tensor([[-1.0964,  2.5455,  2.5607,  2.5141, -2.5678, -0.8001]]) 0\n",
      "tensor([[-0.2274,  1.5145,  3.6357,  1.5549, -1.4516, -3.5175]]) 1\n",
      "tensor([[-1.7396,  0.5462,  4.8684,  2.3623, -1.7076, -1.4714]]) 0\n",
      "tensor([[-0.1362, -0.3991, -0.0654,  4.9068, -1.8923, -1.7105]]) 1\n",
      "tensor([[ 0.6643,  8.1103, -0.8286, -4.3081, -1.4751, -2.8967]]) 1\n",
      "tensor([[-0.1835,  0.2792,  2.0506,  5.0375, -1.7232, -2.7944]]) 1\n",
      "tensor([[ 0.0854, -1.1601, -1.1435,  4.2467, -1.8676, -0.1801]]) 0\n",
      "tensor([[-2.7225, -0.1741,  5.7844,  1.4773, -1.5721,  0.3906]]) 0\n",
      "tensor([[ 0.2933, -2.1812, -0.0316,  0.7407,  1.4820, -1.0725]]) 1\n",
      "tensor([[-1.7725, -0.0169,  4.7089,  0.3343, -2.5385,  1.9880]]) 1\n",
      "tensor([[-0.9807,  0.3446,  7.8967, -1.1994, -0.8943, -0.6610]]) 1\n",
      "tensor([[-1.7642, -0.9361, -1.6172, -2.9510,  1.3493,  4.2709]]) 1\n",
      "tensor([[ 0.1232, -0.0441,  6.4365, -0.0218, -1.4009, -2.1776]]) 1\n",
      "tensor([[-2.7273, -2.2189, -1.5240,  1.0723, -1.1976,  5.8354]]) 0\n",
      "tensor([[-4.5529, -0.0771, -1.3429, -1.8795,  3.1359,  3.6683]]) 0\n",
      "tensor([[-1.3208, -1.3520, -0.1862,  0.1051, -1.4060,  3.6176]]) 1\n",
      "tensor([[-1.7840, -2.4658, -2.1320, -0.4741,  2.3361,  2.9792]]) 1\n",
      "tensor([[ 3.7611, -1.5470,  0.0567, -0.9630, -1.2496, -1.4607]]) 1\n",
      "tensor([[ 0.9126,  5.4060,  2.5431, -2.3138, -1.9377, -4.3775]]) 0\n",
      "tensor([[-3.3915,  0.2251,  2.8294,  1.9120, -0.7789,  1.7253]]) 0\n",
      "tensor([[ 3.7993,  0.0605, -1.8058,  0.9696, -1.1880, -4.5869]]) 1\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 0\n",
      "tensor([[-0.6299, -0.3252, -1.6945, -1.0934,  5.1106, -2.4780]]) 0\n",
      "tensor([[-1.2213, -1.0647,  1.8973,  4.5234, -2.3315, -0.3977]]) 1\n",
      "tensor([[-2.7471, -0.8576, -0.9721,  4.4876, -1.3442,  2.0073]]) 0\n",
      "tensor([[-0.7099, -4.2544, -2.2921,  9.5588, -0.9253,  0.1310]]) 0\n",
      "tensor([[-2.0006, -0.7311,  2.4014,  2.5486,  0.5647, -0.4410]]) 0\n",
      "tensor([[ 0.5979,  1.5937,  0.6994,  0.4528,  0.0371, -3.8139]]) 0\n",
      "tensor([[-4.6979, -2.4660,  0.0670,  1.8069, -1.0065,  5.3736]]) 1\n",
      "tensor([[ 1.0331, -0.9601, -2.2456,  0.6698, -0.8951,  0.6543]]) 0\n",
      "tensor([[-0.7176,  1.0040,  0.0724,  5.0516, -0.8408, -2.5873]]) 1\n",
      "tensor([[-0.7345, -1.8674, -3.2571, -0.3714, -1.4308,  4.6506]]) 0\n",
      "tensor([[-1.1346, -1.8879,  3.7198,  3.4127, -1.5041,  0.2326]]) 0\n",
      "tensor([[ 0.9351,  6.7511, -2.6705,  0.0576, -0.1416, -4.9606]]) 0\n",
      "tensor([[-1.2998,  9.3523, -1.8422, -2.8782,  2.3157, -5.2711]]) 1\n",
      "tensor([[-0.8627,  2.7733,  1.1718,  0.3589,  0.1387, -1.6092]]) 1\n",
      "tensor([[-0.5470, 11.5536, -0.9302, -4.7679, -1.8629, -3.1184]]) 1\n",
      "tensor([[-0.5286,  8.2465,  2.7670, -2.8262, -1.7426, -4.0630]]) 0\n",
      "tensor([[-3.3900, -3.8093, -3.6362,  2.3198,  5.6336,  0.5702]]) 1\n",
      "tensor([[-0.8387, -3.9377,  1.8226,  0.9314,  0.9287,  0.4789]]) 1\n",
      "tensor([[ 3.5717, -0.9691, -0.7961, -0.9202, -1.6839, -2.0869]]) 1\n",
      "tensor([[-2.2949, -0.1474, -0.8331, -0.8991,  4.4350, -2.2124]]) 0\n",
      "tensor([[-1.8108,  2.4468, -0.3187,  2.2821,  0.2864, -2.4958]]) 0\n",
      "tensor([[-1.6538, -2.6055,  0.5907,  1.6913, -1.5919,  4.0716]]) 1\n",
      "tensor([[-2.2075, -0.8934,  0.7689,  0.8549, -1.0099,  2.7571]]) 1\n",
      "tensor([[-4.2874, -1.7044, -4.2235,  0.2185,  4.1881,  2.4691]]) 1\n",
      "tensor([[-2.4083, -0.3876, -4.2082, -1.4895,  5.3264,  0.6718]]) 0\n",
      "tensor([[-2.5833, -1.6997, -4.6781, -0.6992,  6.9244, -0.6640]]) 0\n",
      "tensor([[-0.6641,  1.2673,  9.6801, -1.2818, -2.4626, -0.7859]]) 1\n",
      "tensor([[-2.4841, -0.6288,  0.7013,  7.1708, -0.8402, -1.6226]]) 0\n",
      "tensor([[ 0.6940,  3.7345, -2.2536, -1.8422, -1.5771, -0.5275]]) 1\n",
      "tensor([[ 3.9345, -0.5155, -2.1600, -0.8863,  0.8449, -5.6421]]) 1\n",
      "tensor([[ 2.5469,  5.8127, -2.6772, -2.4862, -0.1067, -5.3881]]) 0\n",
      "tensor([[ 3.7232, -0.0241, -2.2365, -1.5886, -1.1873, -1.6187]]) 1\n",
      "tensor([[-2.7776, -3.3652, -0.4939,  1.3250, -0.4102,  5.9561]]) 1\n",
      "tensor([[-0.6550,  3.5521,  3.5452,  0.8728, -3.3323, -1.3376]]) 0\n",
      "tensor([[-3.2345, -3.2536,  0.8783,  3.9608, -0.4250,  3.3737]]) 0\n",
      "tensor([[-0.9934, -4.2110,  0.4062,  5.0852, -0.8547,  1.1436]]) 1\n",
      "tensor([[-0.7192, -0.9962,  0.7576,  1.5245, -1.0138,  0.7072]]) 0\n",
      "tensor([[-1.6473, -1.4951, -0.6512,  1.9151,  0.0802,  1.9118]]) 0\n",
      "tensor([[-2.8236,  6.7800,  2.5854, -3.1219,  0.7476, -2.4950]]) 1\n",
      "tensor([[-2.3131, -3.1543, -3.1445,  7.2177,  1.6820, -0.5423]]) 1\n",
      "tensor([[-4.9913, -0.2967, -2.2684, -2.0753,  0.0140,  8.6708]]) 1\n",
      "tensor([[-3.8049, -1.2765, -0.5998, -0.4959, -2.0988,  7.6863]]) 0\n",
      "tensor([[-1.1793, -3.9615, -0.2527,  5.5482, -0.8359,  0.7586]]) 1\n",
      "tensor([[ 0.1351,  0.6333, 10.1186, -0.9152, -3.8427, -1.6142]]) 1\n",
      "tensor([[-0.6098,  0.0416,  9.3617,  1.7022, -2.9848, -2.4355]]) 1\n",
      "tensor([[-1.3991,  0.5208,  6.7502,  0.8597, -1.7091, -0.9143]]) 1\n",
      "tensor([[-0.7945,  1.0334, 10.5691, -1.5791, -1.7214, -1.0545]]) 0\n",
      "tensor([[ 0.5504, -4.7383, -4.8489,  9.8336, -0.8003, -0.8143]]) 0\n",
      "tensor([[-3.6401, -2.1684,  0.9182,  0.5871, -1.3669,  7.3560]]) 1\n",
      "tensor([[-0.3224,  1.4622,  0.3733, -1.3053,  0.3659, -0.6239]]) 0\n",
      "tensor([[ 0.4090,  4.5958, -0.2157, -1.8691, -1.1582, -2.6261]]) 0\n",
      "tensor([[-1.2602,  1.2950,  0.4460,  3.5053, -1.1116, -2.0274]]) 0\n",
      "tensor([[-0.4533,  2.8957, -0.4075,  5.3617, -1.9591, -4.3691]]) 1\n",
      "tensor([[-0.8976,  3.8174, -1.0534,  3.7135, -1.0588, -3.3609]]) 1\n",
      "tensor([[-0.5869,  0.4354,  7.7342, -2.3482, -1.7570, -0.7221]]) 1\n",
      "tensor([[-8.9213e-01, -3.0289e+00, -1.3670e+00,  7.4653e+00, -1.5359e+00,\n",
      "          7.0971e-03]]) 1\n",
      "tensor([[-0.6666, -2.3925, -1.9790,  7.6557, -0.5440, -1.9617]]) 1\n",
      "tensor([[-1.1250, -2.5784, -1.8480,  6.7321, -0.6956,  0.0484]]) 0\n",
      "tensor([[ 0.8792,  6.6901,  0.1973, -1.2889, -0.0797, -6.1640]]) 1\n",
      "tensor([[-0.1581,  2.9769, -4.3628, -2.7781,  4.1314, -5.2509]]) 1\n",
      "tensor([[-0.6309,  5.4673, -2.8825, -4.1253, -0.6876,  0.9721]]) 1\n",
      "tensor([[-1.8544, -2.3868, -4.4219,  1.0853,  6.6290, -1.9259]]) 0\n",
      "tensor([[-1.7542, -2.8161, -4.5327,  0.5534,  8.0568, -3.2577]]) 0\n",
      "tensor([[-1.4978, -4.4378, -6.0842,  4.0163,  7.6694, -3.6091]]) 1\n",
      "tensor([[-0.8599, -0.8951, -0.9586, -1.6179,  3.0571,  0.0108]]) 0\n",
      "tensor([[-1.7009,  2.1223,  4.1976, -0.7891, -1.1562, -0.3329]]) 1\n",
      "tensor([[-2.1845,  0.8990,  2.7142, -1.1646, -0.9587,  2.2961]]) 0\n",
      "tensor([[ 0.5107,  4.7711,  3.6169, -2.2373, -2.5037, -3.0206]]) 0\n",
      "tensor([[-1.7519, -0.9036,  7.3335,  1.4689, -1.9677,  0.5776]]) 0\n",
      "tensor([[-2.0125, -1.0627,  4.1874,  4.6470, -0.9383, -0.8388]]) 0\n",
      "tensor([[ 0.7152, -0.0524, -2.0446, -1.6955,  0.5715, -0.1635]]) 1\n",
      "tensor([[-2.2842, -0.3740, -4.0118,  1.3625,  4.6047, -1.0730]]) 1\n",
      "tensor([[-1.0387, -0.1455, -4.1630,  0.7894,  4.3078, -1.5795]]) 0\n",
      "tensor([[-2.6099, -0.1053,  8.0752,  0.1240, -1.3853,  0.9437]]) 1\n",
      "tensor([[-1.2436,  1.3937, -3.6533, -1.8988,  3.9784, -2.1242]]) 0\n",
      "tensor([[-0.3926, -0.5157,  4.6492,  0.7041, -0.6543, -1.4411]]) 0\n",
      "tensor([[-0.3357,  0.9052, -0.4156,  1.9976, -2.8881,  0.3060]]) 1\n",
      "tensor([[-1.5666,  5.2674, -1.1581, -0.5164,  0.3513, -1.4106]]) 1\n",
      "tensor([[ 0.4502,  6.5681, -2.2997, -0.5265, -0.6839, -3.9168]]) 1\n",
      "tensor([[ 0.2138,  0.1359,  7.9421, -0.2260, -2.6455, -0.8996]]) 1\n",
      "tensor([[ 0.1548,  9.3968, -2.7380, -2.6141, -0.0969, -3.9639]]) 0\n",
      "tensor([[-3.9484, -1.1315, -1.8388, -0.1339,  1.7557,  4.3919]]) 0\n",
      "tensor([[ 1.4272,  7.5793, -3.0787, -2.9173, -0.2446, -4.9997]]) 1\n",
      "tensor([[ 4.5115, -0.0115,  0.7507, -0.2401, -2.3269, -4.6068]]) 1\n",
      "tensor([[ 1.6389,  0.1176,  0.9612, -1.6879, -0.8591, -0.7961]]) 1\n",
      "tensor([[-0.9849, -1.4731,  1.0958,  6.9980, -1.3890, -2.2572]]) 0\n",
      "tensor([[-1.3889, -1.8688,  5.0470,  2.8144, -1.1612, -0.7164]]) 0\n",
      "tensor([[-0.5036, -0.9424,  6.1050, -1.5323, -0.9088,  0.7538]]) 1\n",
      "tensor([[-0.8595,  5.2659, -1.3870, -0.4835, -0.3624, -1.9815]]) 1\n",
      "tensor([[ 1.7158,  7.9137, -1.6425, -1.9902, -1.9917, -4.5385]]) 1\n",
      "tensor([[ 0.3024, -0.2988,  0.5369,  2.3968,  1.0963, -3.0219]]) 0\n",
      "tensor([[-1.4114, -1.6934,  0.8958,  0.2521, -1.0607,  3.9103]]) 1\n",
      "tensor([[-2.8891, -1.8518, -1.5778, -0.4167,  0.3977,  5.3597]]) 1\n",
      "tensor([[-0.8873, -1.3832,  5.1698,  0.4710, -1.5715,  0.2164]]) 0\n",
      "tensor([[-2.0063, -3.3348, -0.7273,  7.9822,  0.2897, -0.9864]]) 1\n",
      "tensor([[ 0.6079,  1.4952,  1.2311, -2.5116, -0.4601, -2.0772]]) 0\n",
      "tensor([[-1.3130, -2.3029, -0.2041,  6.0793, -0.1496, -0.2949]]) 0\n",
      "tensor([[-0.7699,  7.8415, -0.9907, -1.6847, -1.5538, -1.4478]]) 1\n",
      "tensor([[ 0.2235,  0.6219,  3.3100, -1.2595, -0.8835, -1.0666]]) 0\n",
      "tensor([[ 0.0820,  6.4353, -0.4875, -0.6369, -3.3793, -2.1919]]) 1\n",
      "tensor([[-1.0152, -2.8663,  0.9069,  6.8839, -0.7167, -1.2715]]) 1\n",
      "tensor([[-0.7068, -1.6663,  1.3166,  2.2798,  0.0065, -0.0030]]) 0\n",
      "tensor([[-2.6029,  1.2960, -3.6400, -2.5648,  6.0352, -1.1083]]) 0\n",
      "tensor([[-1.7557,  3.0882, -2.1577, -2.2787,  3.1112, -1.0011]]) 0\n",
      "tensor([[-1.7361, -0.7840,  0.9693,  7.1152, -2.5351, -0.2261]]) 1\n",
      "tensor([[-0.7838, -2.2807, -1.1739,  7.0597, -0.1425, -1.4627]]) 1\n",
      "tensor([[ 0.3874,  7.4346,  0.3573, -3.2006, -0.3542, -4.0358]]) 1\n",
      "tensor([[-2.2782, -1.0459, -0.2625,  6.0708, -2.2078,  1.0665]]) 1\n",
      "tensor([[-3.0432, -1.6623, -4.5607,  1.0184,  0.5059,  5.3134]]) 1\n",
      "tensor([[-2.9824, -2.1715, -0.7616,  0.0787,  4.3287,  1.8032]]) 0\n",
      "tensor([[ 4.3194,  0.1581, -1.6766, -0.8511, -4.4694, -0.6616]]) 1\n",
      "tensor([[-1.5897, -1.5438,  1.6777,  2.1271,  0.3469,  0.0841]]) 0\n",
      "tensor([[ 1.0664,  9.2059, -1.2504, -0.9184, -1.1194, -5.8272]]) 0\n",
      "tensor([[-0.7836,  1.5577, -2.8349, -2.4098,  4.8855, -3.4778]]) 1\n",
      "tensor([[ 4.8983e-03,  7.9140e+00, -1.3235e+00, -3.7780e+00, -1.2090e-01,\n",
      "         -3.3773e+00]]) 1\n",
      "tensor([[-2.9681, -1.0038, -5.1302, -1.0620,  9.5309, -3.2194]]) 1\n",
      "tensor([[-0.8800,  3.1902,  0.8520, -0.2202, -0.9462, -0.4591]]) 0\n",
      "tensor([[-1.6632, -1.8052, -0.7496,  0.1466, -0.8746,  4.3357]]) 1\n",
      "tensor([[ 1.1242, -0.9770, -0.0684,  5.1694, -2.0610, -1.5480]]) 0\n",
      "tensor([[ 0.0264,  1.5544,  4.2567,  0.5260, -3.6876, -0.0460]]) 1\n",
      "tensor([[-3.6103, -2.2729, -0.7807, -0.8617, -0.3927,  7.1985]]) 1\n",
      "tensor([[-2.6563, -1.2112, -0.2602, -0.3572,  3.0405,  0.9682]]) 1\n",
      "tensor([[ 0.9585,  0.8155,  4.6668,  0.0506, -2.2952, -1.2533]]) 1\n",
      "tensor([[-1.2802, -1.0674,  4.2523,  0.2877, -0.2757,  0.0661]]) 0\n",
      "tensor([[ 0.1811, -0.6065,  4.3157,  4.5933, -3.5099, -2.0221]]) 0\n",
      "tensor([[-2.1422, -0.8386,  3.3154,  4.9157, -1.7176, -0.2101]]) 1\n",
      "tensor([[-1.2865, -3.5853, -4.3978,  8.3892,  1.1361, -0.3651]]) 1\n",
      "tensor([[-4.6134, -1.2913, -0.8195, -0.4645, -1.9778,  8.7279]]) 0\n",
      "tensor([[-3.5598, -2.1620, -0.9516,  0.5181, -1.3285,  7.1489]]) 1\n",
      "tensor([[-2.1110, -0.0800,  1.8386,  0.9321, -3.4557,  4.0323]]) 0\n",
      "tensor([[-2.3591, -1.4627, -0.5973, -1.3136,  1.0638,  3.5830]]) 0\n",
      "tensor([[-0.2462,  3.0873, -3.5135, -1.7348, -1.6911,  1.4760]]) 1\n",
      "tensor([[-1.7395,  0.9976, -4.2153, -2.4538,  7.2170, -4.5510]]) 1\n",
      "tensor([[ 0.6652, -2.5784, -3.5285,  6.4659,  0.3503, -2.8793]]) 0\n",
      "tensor([[-1.7627, -1.1516, -3.5254, -0.6052,  4.7054, -1.0426]]) 0\n",
      "tensor([[-2.5982, -1.9990, -2.2578,  0.1149,  4.5378,  1.1724]]) 1\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 0\n",
      "tensor([[ 3.8920,  3.0865, -2.1258, -2.7601, -1.4849, -4.7666]]) 0\n",
      "tensor([[ 4.7587,  0.9864, -1.9799, -2.5454, -3.6009, -3.2008]]) 1\n",
      "tensor([[ 2.5257, -0.9151, -2.0893,  3.4722, -2.0782, -2.3810]]) 0\n",
      "tensor([[-1.2101, -2.4128,  4.3647,  0.5414, -2.0691,  3.3114]]) 1\n",
      "tensor([[-0.4940, -1.3977, -1.6258, -0.4755,  3.1039, -0.7773]]) 0\n",
      "tensor([[-1.7018,  3.6217,  7.0634, -1.0980, -2.0390, -1.6453]]) 0\n",
      "tensor([[-0.5888,  7.4903,  1.5827, -1.6247, -0.1858, -4.9334]]) 0\n",
      "tensor([[ 0.9702,  7.9761, -2.6561, -1.5034, -1.6124, -3.4592]]) 0\n",
      "tensor([[-2.4496, -0.8519, -0.3322, -3.6212,  6.2262, -1.1164]]) 0\n",
      "tensor([[-2.3919, -1.2509, -0.7239,  0.1121,  0.4217,  3.1441]]) 1\n",
      "tensor([[-1.8012,  3.3563, -2.1401, -0.2705,  1.9457, -1.9920]]) 0\n",
      "tensor([[-3.7433,  0.3231, -1.7182, -0.4970,  1.3590,  3.8464]]) 0\n",
      "tensor([[-2.2965, -1.6113, -1.1321, -0.9817,  1.1333,  3.7726]]) 1\n",
      "tensor([[-1.6912,  1.3343,  1.7095,  1.7851, -1.3806,  0.0906]]) 0\n",
      "tensor([[-0.3175,  3.6883,  0.0899, -0.9759, -0.3778, -1.5711]]) 0\n",
      "tensor([[ 3.8938, -0.9887, -1.8348,  0.5168, -2.2104, -2.8308]]) 0\n",
      "tensor([[-3.0253, -0.8014, -2.7846,  1.9401,  2.6358,  1.3114]]) 0\n",
      "tensor([[ 0.3403,  2.6185,  0.9494, -1.6674, -1.2883, -0.7880]]) 0\n",
      "tensor([[ 0.0195,  2.1361, -0.2935,  3.0130, -0.7536, -2.7981]]) 0\n",
      "tensor([[-2.8472,  0.8957, -0.2555, -1.0598, -0.8032,  4.0381]]) 1\n",
      "tensor([[-1.9287, -1.5176,  2.1883,  0.4025,  0.8236,  1.3443]]) 0\n",
      "tensor([[-0.6656,  0.6181, -4.8215, -0.5075,  4.6955, -2.4997]]) 0\n",
      "tensor([[ 0.3846,  0.9584, -2.3811, -0.9394,  2.8822, -2.7497]]) 0\n",
      "tensor([[-2.5212,  1.1180, -2.9207, -0.9989,  3.8053, -0.2398]]) 1\n",
      "tensor([[-1.5316,  1.3065,  3.7503,  0.6006, -1.2313, -0.8366]]) 0\n",
      "tensor([[-3.1836, -1.4607, -2.2018,  1.0630, -0.3924,  4.7080]]) 0\n",
      "tensor([[-0.2456,  6.7302, -1.3289, -2.1834, -1.5073, -1.6504]]) 1\n",
      "tensor([[-2.8446, -2.9629, -3.8761,  2.8384,  0.3055,  4.3264]]) 1\n",
      "tensor([[-0.0660, -2.2079, -0.8460,  6.5175,  0.2716, -2.6072]]) 0\n",
      "tensor([[-2.0934, -3.0856, -4.8548,  2.4309,  7.5120, -2.5679]]) 0\n",
      "tensor([[-1.4978,  0.2329,  5.4034, -0.4265, -2.8418,  2.0898]]) 1\n",
      "tensor([[-1.6278,  2.5072, -1.8316, -1.4648,  2.7460, -1.1874]]) 0\n",
      "tensor([[-1.4958, -0.1832, 12.2397, -1.6911, -2.9095, -0.3985]]) 1\n",
      "tensor([[-3.7116, -2.9517, -1.8902,  1.5349, -0.2392,  6.5925]]) 1\n",
      "tensor([[-2.3372, -2.4091, -1.0411,  3.3139, -0.3098,  3.5863]]) 1\n",
      "tensor([[-1.7348, -2.3367, -1.5716,  5.2566,  0.3296,  0.2899]]) 0\n",
      "tensor([[-0.4813, -2.1097, -2.1010,  6.1781, -1.2167,  0.3441]]) 1\n",
      "tensor([[-2.4559, -2.2653, -1.6029, -0.2962, -1.1349,  6.9391]]) 1\n",
      "tensor([[-2.2050, -3.4637,  0.8577,  9.1906, -0.6689,  0.1430]]) 0\n",
      "tensor([[-2.8582, -1.1578, -6.6824,  0.4682, 10.5397, -6.8723]]) 0\n",
      "tensor([[-1.5348,  0.0621, -2.7647, -1.3630,  6.2281, -3.0288]]) 0\n",
      "tensor([[-4.2243, -0.4303, -1.3196, -0.3618,  3.2700,  2.8216]]) 0\n",
      "tensor([[-0.8016, -0.2885, -1.8702, -0.0218,  2.8981, -1.1413]]) 0\n",
      "tensor([[ 0.4044,  0.2803, -1.2530, -1.0379, -0.3999,  0.7846]]) 1\n",
      "tensor([[-1.7770, -1.8433,  0.0736, -0.4449, -0.6577,  4.5594]]) 1\n",
      "tensor([[-1.0746,  2.7699, -1.4681, -0.4327,  0.2637, -1.4598]]) 0\n",
      "tensor([[ 0.5318,  7.1971, -0.6231, -1.4108, -0.3888, -4.8715]]) 0\n",
      "tensor([[-1.3858, -0.3925, -0.1638,  1.9723, -0.5287,  0.1342]]) 0\n",
      "tensor([[-0.8936, -3.1295,  0.9304,  7.1422, -0.6763, -0.5096]]) 1\n",
      "tensor([[-4.0921, -2.8115, -2.7808,  1.2514,  2.0632,  4.1703]]) 0\n",
      "tensor([[-0.9719,  3.2800, -0.2979,  1.1731, -1.3301, -0.6518]]) 0\n",
      "tensor([[-1.4433, -2.9223, -2.3202,  1.8285,  0.1911,  2.9755]]) 0\n",
      "tensor([[-0.5580, -0.1560,  4.5707,  1.0613, -1.1636, -0.9471]]) 1\n",
      "tensor([[-0.8639,  5.3205, -0.7394, -1.9004, -0.9385, -1.3642]]) 1\n",
      "tensor([[-1.4272e+00, -1.1974e+00, -3.7486e+00, -3.6862e-03,  5.6046e+00,\n",
      "         -2.0452e+00]]) 1\n",
      "tensor([[ 0.9320,  4.5529,  0.3472,  0.0716, -0.8012, -4.2335]]) 0\n",
      "tensor([[ 0.9284,  5.6715, -0.7043, -3.6940, -0.7658, -2.5935]]) 1\n",
      "tensor([[ 1.1084,  6.9137, -1.5009, -2.2801, -0.9284, -3.3619]]) 1\n",
      "tensor([[-1.4631, -0.2158, -0.9108,  1.7260, -1.1218,  1.4939]]) 0\n",
      "tensor([[ 3.6522,  0.2685, -1.7075, -0.1797, -1.7165, -2.6320]]) 1\n",
      "tensor([[-0.8811,  6.0222, -2.8097, -1.2228, -0.2834, -1.9483]]) 0\n",
      "tensor([[-3.7387, -1.1564, -0.3242,  0.5280, -1.4530,  6.1538]]) 1\n",
      "tensor([[-1.5975,  0.5711,  2.0755,  2.1055, -1.3618,  0.2473]]) 0\n",
      "tensor([[ 1.3860,  2.1812, -0.7855, -2.8087,  0.8950, -2.6847]]) 0\n",
      "tensor([[-0.1558, -0.6167, -2.4139,  1.7540, -1.8461,  1.7140]]) 0\n",
      "tensor([[-1.1349,  1.0079,  0.3630,  0.7453, -0.2887,  0.4276]]) 0\n",
      "tensor([[-0.8584,  3.9852, -1.3401, -1.3120, -2.2706,  0.8622]]) 0\n",
      "tensor([[-0.9590, -2.7824, -0.2628,  5.3145, -0.2930, -0.2223]]) 0\n",
      "tensor([[-2.2751, -0.7555, -0.8204,  5.1146, -0.1102,  0.2098]]) 0\n",
      "tensor([[-1.1863,  3.3880,  3.7782,  0.6799, -1.3580, -2.1981]]) 0\n",
      "tensor([[-1.0028, -3.5503, -1.8645,  6.9470, -0.6642,  0.5789]]) 1\n",
      "tensor([[-2.6821, -1.5361,  3.2229, -0.1196, -0.5380,  4.0822]]) 1\n",
      "tensor([[-3.9998, -2.1820, -0.4089, -0.6765, -1.1206,  7.8217]]) 1\n",
      "tensor([[-3.8167,  0.1017, -2.9377, -0.4572, -1.8854,  7.5548]]) 0\n",
      "tensor([[-1.4772, -2.0592, -2.0734,  0.7649, -0.7048,  4.2334]]) 0\n",
      "tensor([[-2.4002, -1.5650,  0.0079,  6.5709, -1.5167,  0.3853]]) 0\n",
      "tensor([[ 0.1089, -1.8823,  0.5966,  6.4297, -1.4406, -1.6287]]) 1\n",
      "tensor([[-1.8378, -1.1320,  0.6878, -0.8009, -1.6359,  5.6506]]) 0\n",
      "tensor([[ 0.4060, -0.3625,  5.3993,  1.2931, -1.5805, -2.2764]]) 0\n",
      "tensor([[-1.9096, -0.1052,  3.3635,  2.6269, -0.4420, -0.4904]]) 0\n",
      "tensor([[-1.3507, -1.1345, -0.0532,  6.2289, -1.3585, -0.9968]]) 1\n",
      "tensor([[-2.3273, -0.6596, -4.2030, -1.1527,  6.9328, -3.2551]]) 1\n",
      "tensor([[-2.8019, -2.3337, -2.0562,  5.4793,  0.9160,  0.8365]]) 1\n",
      "tensor([[-3.1385, -1.6008, -2.6615, -2.1305,  6.7312,  0.0949]]) 0\n",
      "tensor([[-0.9869,  0.3123, -2.3855,  0.1840, -0.9521,  0.9809]]) 0\n",
      "tensor([[ 4.9556, -0.2539, -3.4350, -0.5233,  1.0528, -5.6660]]) 0\n",
      "tensor([[ 3.8951, -1.6360, -0.9192,  0.3103, -1.1093, -3.6187]]) 1\n",
      "tensor([[-0.3802,  7.1305, -2.6743, -2.5593,  0.1683, -3.1725]]) 1\n",
      "tensor([[-0.0962, -2.9509, -0.7632,  7.8589, -1.4396, -1.2025]]) 1\n",
      "tensor([[-2.7490, -0.3805, -3.0068, -1.3652,  6.0017, -0.7835]]) 1\n",
      "tensor([[-2.3010,  0.3212,  0.5115, -1.7591, -3.0768,  7.0984]]) 1\n",
      "tensor([[ 0.8651,  0.0374,  2.3765, -1.1338,  0.7630, -3.0168]]) 1\n",
      "tensor([[ 1.7418,  5.1643, -3.0371, -0.5591, -3.3587, -1.7913]]) 1\n",
      "tensor([[-2.3208,  7.3608, -1.1839, -1.4775,  0.3744, -2.4482]]) 1\n",
      "tensor([[-1.5376,  1.0358, -2.8255, -1.7985,  6.0570, -3.0218]]) 1\n",
      "tensor([[-1.3266, -0.8123,  6.2230,  1.2623, -2.3167,  0.6522]]) 0\n",
      "tensor([[-0.3895,  1.9668,  0.6123, -0.4194, -1.4235, -0.8334]]) 0\n",
      "tensor([[-3.9982e+00, -3.5250e+00, -2.4864e+00,  2.8540e+00, -1.0543e-03,\n",
      "          6.0274e+00]]) 1\n",
      "tensor([[-3.6438, -2.7698, -2.1450,  0.5896, -0.0135,  6.8671]]) 0\n",
      "tensor([[-2.1545, -1.3688, -3.8478,  0.6197,  5.2999, -1.9360]]) 1\n",
      "tensor([[ 3.0441, -1.5026, -3.1853,  0.9240,  0.5437, -3.2479]]) 1\n",
      "tensor([[-0.9690, -2.0331, -1.9604,  6.7555, -1.2734, -0.4525]]) 0\n",
      "tensor([[ 4.4649, -0.2049, -1.7826, -0.1905, -2.1304, -3.4659]]) 1\n",
      "tensor([[ 0.4948,  0.5548,  1.6309,  2.7962, -2.0126, -2.1800]]) 1\n",
      "tensor([[-1.3441, -1.7076, -3.8849,  0.1332,  3.5985,  0.0935]]) 0\n",
      "tensor([[ 0.6187, -2.0908, -1.3380,  5.9507, -0.8835, -1.7577]]) 1\n",
      "tensor([[-2.1667, -3.0650,  3.6556,  6.0314, -0.7729, -0.5936]]) 0\n",
      "tensor([[-1.0894, -1.3911,  1.1994,  7.5191, -1.3861, -2.2877]]) 1\n",
      "tensor([[-1.7614, -2.3300, -0.2103,  9.0982, -0.5099, -1.3235]]) 1\n",
      "tensor([[-1.6121, -2.4944,  2.0601,  7.6120, -1.5855, -0.6656]]) 0\n",
      "tensor([[-3.5040,  0.6532, -0.5010, -0.5256, -0.8182,  4.3636]]) 0\n",
      "tensor([[-0.9305, -1.3077,  0.4077,  6.4115, -1.8142, -0.1892]]) 0\n",
      "tensor([[ 2.9007,  1.3847, -1.5538, -0.9839, -2.4078, -2.9738]]) 0\n",
      "tensor([[ 3.1533,  0.4936, -1.1615, -1.3080, -0.4096, -3.2546]]) 1\n",
      "tensor([[-2.1288, -2.8275, -2.2902,  6.1780,  0.4092,  0.2249]]) 0\n",
      "tensor([[ 1.6991,  2.0905,  0.6200, -1.4095, -0.8637, -2.2888]]) 0\n",
      "tensor([[-2.9294, -2.2325, -1.4839,  1.4332, -0.9372,  5.8956]]) 1\n",
      "tensor([[-0.8343, -3.1394, -1.9323,  9.7495, -1.1903, -0.4984]]) 1\n",
      "tensor([[-0.4583, -2.1994, -0.9115,  6.3637, -0.5606, -1.3541]]) 0\n",
      "tensor([[-0.6909,  0.1162, -1.5454,  2.6613,  1.7524, -2.5203]]) 0\n",
      "tensor([[-3.7297,  3.2901, -2.1498, -3.4803, -1.1159,  5.9514]]) 1\n",
      "tensor([[-1.3869, -0.7335, -0.9201,  1.0786, -1.4904,  2.7734]]) 1\n",
      "tensor([[-0.4789,  0.2404,  0.0613, -0.2792, -1.8074,  2.1263]]) 0\n",
      "tensor([[-0.7014,  6.3306, -0.1175, -2.7726, -2.6754,  0.6697]]) 1\n",
      "tensor([[-1.4145,  3.1722,  0.2545,  0.0204,  0.1507, -1.8050]]) 0\n",
      "tensor([[-2.9587, -3.2763,  1.9136,  1.0322,  1.8711,  0.6419]]) 0\n",
      "tensor([[-1.9367, -0.5690,  0.7659, -0.6787,  1.0977,  1.6063]]) 0\n",
      "tensor([[-1.3904, -1.0701,  0.7976,  3.0376, -1.0073,  0.7729]]) 0\n",
      "tensor([[-1.2694,  0.6002,  0.8349, -0.3027, -0.4133,  1.1049]]) 0\n",
      "tensor([[-3.1176,  0.7238, -3.6581, -1.6758,  7.2481, -2.5683]]) 1\n",
      "tensor([[ 0.3128,  6.1359,  0.5887, -1.5952, -1.4059, -3.4981]]) 0\n",
      "tensor([[-1.3985, -0.0156, -1.9711, -1.3340, -0.7763,  4.0674]]) 0\n",
      "tensor([[-0.9199, -1.5382, -1.9883,  1.4579,  2.0357, -0.3060]]) 0\n",
      "tensor([[-0.2678,  5.6550, -1.2121,  0.2990, -1.2218, -3.3323]]) 0\n",
      "tensor([[-1.0964,  2.7815,  0.6299, -0.4806,  2.8113, -4.2094]]) 1\n",
      "tensor([[-4.4424, -1.4526, -2.4309,  0.3899, -1.9920,  8.9152]]) 0\n",
      "tensor([[-4.1599, -1.1320, -6.1621,  0.4056,  5.9905,  0.5965]]) 1\n",
      "tensor([[-2.5613,  0.3190, -2.9293, -1.0326,  4.0655, -0.9924]]) 0\n",
      "tensor([[-4.0808, -0.8627, -1.8553,  1.1960, -1.5848,  6.4721]]) 0\n",
      "tensor([[-1.9195, -2.5701,  0.0928,  5.8296,  0.3170, -0.3225]]) 0\n",
      "tensor([[ 5.1443,  2.4810,  0.0938, -1.3791, -2.3681, -5.3053]]) 1\n",
      "tensor([[-2.0218,  0.5534, -1.5468, -2.3172,  4.0008, -0.7559]]) 1\n",
      "tensor([[-1.4108, -1.6425, -0.3219,  5.8512, -1.7030,  0.3985]]) 0\n",
      "tensor([[-1.4713, -1.3047,  0.6450,  6.6663, -1.5098, -0.6284]]) 0\n",
      "tensor([[-0.8269,  0.9937, -3.1503, -0.6525,  6.2448, -4.8528]]) 0\n",
      "tensor([[ 1.3280,  3.2722, -1.3981,  0.6705, -1.5078, -3.0649]]) 0\n",
      "tensor([[-1.1040, -0.1128, -0.6106, -1.2503,  3.8070, -0.9429]]) 1\n",
      "tensor([[-0.5480,  3.9816, -1.9011, -0.8744, -0.7393, -1.0337]]) 1\n",
      "tensor([[-4.2464, -1.7030, -1.1015,  1.1153, -1.5420,  7.2389]]) 0\n",
      "tensor([[-3.2053, -1.9747, -7.3165,  1.7408, 10.5343, -4.1675]]) 1\n",
      "tensor([[ 0.2977,  3.1990, -0.6185,  1.2118, -1.1250, -2.7614]]) 1\n",
      "tensor([[-0.4343, -0.1537, -2.3774, -2.4457,  4.1225, -2.3921]]) 0\n",
      "tensor([[-2.0133, -2.2305,  0.7052,  5.0266, -1.1328,  1.2115]]) 1\n",
      "tensor([[-0.8743,  2.7998,  2.5990,  1.9740, -1.1088, -2.3998]]) 0\n",
      "tensor([[-1.2187,  2.5756,  2.6231,  0.9405, -0.9049, -2.2258]]) 1\n",
      "tensor([[-2.1356, -0.4090,  4.5453, -0.9831, -1.0766,  2.6446]]) 1\n",
      "tensor([[-1.8370, -1.3099, -2.9502,  1.4359, -1.8454,  5.5165]]) 1\n",
      "tensor([[-1.7187, -3.7818, -4.3135,  6.1189,  0.7538,  2.6067]]) 1\n",
      "tensor([[-3.1181, -1.6101,  0.1884,  2.9103, -1.2216,  3.5656]]) 0\n",
      "tensor([[ 0.4548,  7.1520, -1.0302, -1.7970, -1.5716, -3.3950]]) 1\n",
      "tensor([[-1.9412, -2.9229,  0.2288,  8.8620, -1.2252, -0.3133]]) 0\n",
      "tensor([[-3.8515, -1.3438, -0.7702,  0.4295, -2.7290,  7.6375]]) 0\n",
      "tensor([[ 2.7753,  2.3633, -4.9217, -2.5787,  2.0851, -6.9964]]) 0\n",
      "tensor([[-0.3277, -0.7992,  1.0629,  2.6721, -1.4344, -0.2987]]) 0\n",
      "tensor([[ 0.1673, -2.6455, -2.0620,  0.7050,  3.6059, -2.4534]]) 0\n",
      "tensor([[-1.6231,  3.8748, -1.4937, -1.2815,  1.6384, -1.5079]]) 0\n",
      "tensor([[-4.1443, -0.5622, -1.2057, -0.4464,  5.1295,  1.0759]]) 1\n",
      "tensor([[-2.7907, -4.0257, -1.5137,  1.9020, -1.8365,  7.6219]]) 0\n",
      "tensor([[-3.1511, -0.2004, -0.7830, -0.4405, -1.0217,  5.5746]]) 1\n",
      "tensor([[-4.3851, -0.6373, -1.2615, -0.2938,  6.8926, -1.3352]]) 0\n",
      "tensor([[ 0.1891, -0.0236, -4.1946,  0.8398,  5.1569, -4.1061]]) 0\n",
      "tensor([[-1.8203, -1.3326, -0.6326,  5.8936, -2.1303,  0.3918]]) 1\n",
      "tensor([[-1.8470,  0.5550, -1.3147, -1.0268, -2.2972,  3.8623]]) 1\n",
      "tensor([[-0.8830, -2.9602, -0.5230,  5.0459,  0.1041,  0.1820]]) 1\n",
      "tensor([[-2.7945, -0.8833, -2.2050,  0.8514,  0.0327,  3.8380]]) 1\n",
      "tensor([[-1.1898,  0.9952, -2.4540,  1.1006,  4.2284, -2.6412]]) 1\n",
      "tensor([[ 3.1716,  6.6374, -1.6371, -1.7851, -3.5004, -5.3994]]) 0\n",
      "tensor([[-0.1530, -0.0897, -0.0785,  0.2311,  0.0784,  0.0352]]) 1\n",
      "tensor([[ 3.0611, -0.5660, -2.3026, -0.2224, -0.4651, -3.5559]]) 1\n",
      "tensor([[-0.3901,  8.7073, -1.4132, -0.9574, -0.3814, -5.1740]]) 0\n",
      "tensor([[-2.7785,  0.3998, -2.9619, -1.4657,  4.6133, -1.6156]]) 1\n",
      "tensor([[-2.3620, -0.2164,  0.2341,  5.1058, -2.6165,  1.0284]]) 1\n",
      "tensor([[-3.2173,  0.0810, -1.7758, -1.4019,  6.9216, -1.8260]]) 0\n",
      "tensor([[-3.2366, -2.4511, -2.9732,  1.9990,  4.6278, -0.0340]]) 0\n",
      "tensor([[-0.0844,  6.7220, -0.8004, -1.9489, -0.6472, -3.5266]]) 1\n",
      "tensor([[-0.0089,  7.1527, -1.8197, -2.7800,  1.0487, -5.4204]]) 1\n",
      "tensor([[-0.9670, -0.5885,  1.0500,  5.8289, -2.4299, -0.8828]]) 1\n",
      "tensor([[-2.2293,  0.8075, -2.2556, -0.4587,  0.3006,  2.6070]]) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(t_data.num_nodes):\n",
    "    \n",
    "    if t_data.train_mask[i]:\n",
    "        pred = query_zero_hop(t_model, t_data.x[i]).detach().cpu()\n",
    "        print(pred, 1)\n",
    "    elif t_data.test_mask[i]:\n",
    "        pred = query_zero_hop(t_model, t_data.x[i]).detach().cpu()\n",
    "        print(pred, 0)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b48b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327, 6], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea5c5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
